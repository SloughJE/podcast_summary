{"podcast_details": {"podcast_details": {"links": [{"href": "https://feeds.megaphone.fm/MLN2155636147?post_type=episodes", "rel": "self", "type": "application/rss+xml"}, {"rel": "alternate", "type": "text/html", "href": "https://twimlai.com"}], "title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "title_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)"}, "link": "https://twimlai.com", "language": "en", "rights": "All rights reserved", "rights_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "All rights reserved"}, "subtitle": "", "subtitle_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": ""}, "image": {"href": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress"}, "itunes_explicit": null, "itunes_type": "episodic", "authors": [{"name": "TWIML", "email": "team@twimlai.com"}], "author": "TWIML", "author_detail": {"name": "TWIML", "email": "team@twimlai.com"}, "summary": "Machine learning and artificial intelligence are dramatically changing the way businesses operate and people live. The TWIML AI Podcast brings the top minds and ideas from the world of ML and AI to a broad and influential community of ML/AI researchers, data scientists, engineers and tech-savvy business and IT leaders. Hosted by Sam Charrington, a sought after industry analyst, speaker, commentator and thought leader. Technologies covered include machine learning, artificial intelligence, deep learning, natural language processing, neural networks, analytics, computer science, data science and more.", "summary_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "Machine learning and artificial intelligence are dramatically changing the way businesses operate and people live. The TWIML AI Podcast brings the top minds and ideas from the world of ML and AI to a broad and influential community of ML/AI researchers, data scientists, engineers and tech-savvy business and IT leaders. Hosted by Sam Charrington, a sought after industry analyst, speaker, commentator and thought leader. Technologies covered include machine learning, artificial intelligence, deep learning, natural language processing, neural networks, analytics, computer science, data science and more."}, "content": "<p>Machine learning and artificial intelligence are dramatically changing the way businesses operate and people live. The TWIML AI Podcast brings the top minds and ideas from the world of ML and AI to a broad and influential community of ML/AI researchers, data scientists, engineers and tech-savvy business and IT leaders. Hosted by Sam Charrington, a sought after industry analyst, speaker, commentator and thought leader. Technologies covered include machine learning, artificial intelligence, deep learning, natural language processing, neural networks, analytics, computer science, data science and more.</p>", "content_detail": {"type": "text/html", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "<p>Machine learning and artificial intelligence are dramatically changing the way businesses operate and people live. The TWIML AI Podcast brings the top minds and ideas from the world of ML and AI to a broad and influential community of ML/AI researchers, data scientists, engineers and tech-savvy business and IT leaders. Hosted by Sam Charrington, a sought after industry analyst, speaker, commentator and thought leader. Technologies covered include machine learning, artificial intelligence, deep learning, natural language processing, neural networks, analytics, computer science, data science and more.</p>"}, "publisher_detail": {"name": "TWIML", "email": "team@twimlai.com"}, "tags": [{"term": "Technology", "scheme": "http://www.itunes.com/", "label": null}, {"term": "News", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Tech News", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Science", "scheme": "http://www.itunes.com/", "label": null}], "itunes_new-feed-url": "https://feeds.megaphone.fm/MLN2155636147"}, "first_episode": {"title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643", "title_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643"}, "links": [{"rel": "alternate", "type": "text/html", "href": "https://twimlai.com/podcast/twimlai/inverse-reinforcement-learning-without-rl/"}, {"length": "0", "type": "audio/mpeg", "href": "https://chrt.fm/track/4D4ED/traffic.megaphone.fm/MLN3531803742.mp3?updated=1692641394", "rel": "enclosure"}], "link": "https://twimlai.com/podcast/twimlai/inverse-reinforcement-learning-without-rl/", "summary": "Today we\u2019re joined by Gokul Swamy, a Ph.D. Student at the Robotics Institute at Carnegie Mellon University. In the final conversation of our ICML 2023 series, we sat down with Gokul to discuss his accepted papers at the event, leading off with \u201cInverse Reinforcement Learning without Reinforcement Learning.\u201d In this paper, Gokul explores the challenges and benefits of inverse reinforcement learning, and the potential and advantages it holds for various applications. Next up, we explore the \u201cComplementing a Policy with a Different Observation Space\u201d paper which applies causal inference techniques to accurately estimate sampling balance and make decisions based on limited observed features. Finally, we touched on \u201cLearning Shared Safety Constraints from Multi-task Demonstrations\u201d which centers on learning safety constraints from demonstrations using the inverse reinforcement learning approach.\n\nThe complete show notes for this episode can be found at twimlai.com/go/643.", "summary_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "Today we\u2019re joined by Gokul Swamy, a Ph.D. Student at the Robotics Institute at Carnegie Mellon University. In the final conversation of our ICML 2023 series, we sat down with Gokul to discuss his accepted papers at the event, leading off with \u201cInverse Reinforcement Learning without Reinforcement Learning.\u201d In this paper, Gokul explores the challenges and benefits of inverse reinforcement learning, and the potential and advantages it holds for various applications. Next up, we explore the \u201cComplementing a Policy with a Different Observation Space\u201d paper which applies causal inference techniques to accurately estimate sampling balance and make decisions based on limited observed features. Finally, we touched on \u201cLearning Shared Safety Constraints from Multi-task Demonstrations\u201d which centers on learning safety constraints from demonstrations using the inverse reinforcement learning approach.\n\nThe complete show notes for this episode can be found at twimlai.com/go/643."}, "published": "Mon, 21 Aug 2023 17:59:05 -0000", "published_parsed": [2023, 8, 21, 17, 59, 5, 0, 233, 0], "itunes_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy", "itunes_episodetype": "full", "itunes_episode": "643", "authors": [{"name": "Sam Charrington"}], "author": "Sam Charrington", "author_detail": {"name": "Sam Charrington"}, "image": {"href": "https://megaphone.imgix.net/podcasts/16a2d432-3f52-11ee-a6e6-e7b527e8ba03/image/d66dcb.jpg?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress"}, "subtitle": "", "subtitle_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": ""}, "content": [{"type": "text/html", "language": null, "base": "https://feeds.megaphone.fm/MLN2155636147", "value": "<p>Today we\u2019re joined by Gokul Swamy, a Ph.D. Student at the Robotics Institute at Carnegie Mellon University. In the final conversation of our ICML 2023 series, we sat down with Gokul to discuss his accepted papers at the event, leading off with \u201cInverse Reinforcement Learning without Reinforcement Learning.\u201d In this paper, Gokul explores the challenges and benefits of inverse reinforcement learning, and the potential and advantages it holds for various applications. Next up, we explore the \u201cComplementing a Policy with a Different Observation Space\u201d paper which applies causal inference techniques to accurately estimate sampling balance and make decisions based on limited observed features. Finally, we touched on \u201cLearning Shared Safety Constraints from Multi-task Demonstrations\u201d which centers on learning safety constraints from demonstrations using the inverse reinforcement learning approach.</p><p><br /></p><p>The complete show notes for this episode can be found at <a href=\"https://feeds.megaphone.fm/twimlai.com/go/643\">twimlai.com/go/643</a>.</p>"}], "itunes_duration": "2035", "itunes_explicit": null, "id": "16a2d432-3f52-11ee-a6e6-e7b527e8ba03", "guidislink": false}, "transcribed_data": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI podcast. I'm your host, Sam Charrington, and today I'm joined by Gokul Swamy. Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University. Wherever you're listening to today's show, make sure you're subscribed and be sure to like, rate, and review the show. Gokul, welcome to the podcast. Yeah. Thank you so much for having me. Hey, I'm looking forward to digging into our conversation. We'll be talking about your research into the fields of efficient interactive learning and making good decisions without observable boundaries, with a particular emphasis on a few of the papers that you are presenting at this year's ICML conference. Before we dig into that though, I'd love to have you share a little bit about your background and how you came to the field. Yeah, good question. So I just wrapped up my third year at CMU. I spent a bunch of time there working on different things that are about sort of algorithms for this field called imitation learning, which is broadly speaking about how you can try and learn to make good decisions from data. Before that, I spent a few years at Berkeley where I was a master's and undergrad student, and there I was working on methods for human-robot interaction. Even before that, I grew up in San Diego, where I tried to just maximize the amount of time I spent on the beach. Sounds worthwhile. Yeah. Tell us a little bit about your research interests and the focus of your work. Yeah. So I think over the last few years, what I've been really trying to think about is how we can try and learn to make sequence of decisions well from observing data, some sort of expert demonstrated during the same thing. So perhaps the most intuitive example of this is something like self-driving cars. So you put some person in a car, we strap the car up with sensors, we get them to drive all around Pittsburgh, and then we want to learn a program, let's the car do the same thing. And I spent a lot of time thinking about what are the right sorts of algorithms for that problem. And I think there's two parts of that that I'm most interested in. The first is how we do this efficiently. And I mean efficiently in various senses, both in terms of the amount of data you need to use, the amount of compute you need to use, stuff like that. The other thing I'm really interested in is if we're trying to get a car to do the same thing as a person, they don't have the same observation space in some sense, right? They don't see exactly the same way, they don't have the exact same sensors. And at that point, there's all these sorts of things that show up that affect a relationship you care about that you don't really observe. So let's say you're trying to predict from some variable X to some variable Y, and there's some unobserved variable U that affects both. This is usually called an unobserved confounder. And here I'm trying to predict from some sort of observations, some sort of how much I want to the steering wheel, how much I want to press the gas pedal, stuff like that. For example, your self-driving car might not be able to pick up on the fact that a hand gesture from somebody in the car across from them actually means something. And when you kind of have this sort of partial observability, I think it's a really interesting question of how do you still learn well? So I've been thinking about both in the sort of idealized setting, how do you do it efficiently, and in the sort of more real world setting where you don't perhaps get the same access to the same pieces of information, how do you make decisions well there? Awesome. And how does that tie into the research that you're presenting at the conference this year? Yeah, so I think we have a few papers at the conference, which I'm very thankful for. And they sort of touch on different parts of those topics. So we have one paper at the main conference, which is really focused on the question of how do we do imitation learning efficiently. So that paper, I think, is really cool because it's a paper where the theory is very elegant. It's not a very sort of complicated idea mathematically, but it really, really works in practice. So we're quite excited about that. Is this the inverse RL paper? Yeah, yeah. So I can maybe talk a little bit about that if that would be good. Yeah, let's dig into it. Sure. So just before you do the title of the paper is inverse reinforcement learning without reinforcement learning, which is an intriguing title. Thank you. Yeah. So broadly speaking, right, in science is we have sort of like forward problems and inverse problems, right? You know, the forward problem is usually, okay, I have some kind of objective function and I'm going to optimize it to get something. And the inverse problem is, okay, given behavior of the optimal thing, what was the thing I was trying to optimize in the first place? And in the sort of inverse reinforcement learning setting, right, the forward problem is reinforcement learning. So I give you a word function. I want you to find the optimal policy under this word function. The inverse problem is, okay, I gave you data from the optimal policy and I want you to figure out what was the word function that was being optimized here. So if you think about the driving example, right, I might want to try to extract a function that tells you, okay, how much does this person care about, you know, staying away from the cars that are in front of them? How much do they care about, you know, adhering to the speed limits, things like that? And I think it's a very natural question to ask, like, well, can't I just write down a function that does that? And it's actually really quite hard when you try to do it, right? Because I can tell you that, okay, I definitely care about observing the speed limit. I care about staying away from people that are in front of me. But exactly how much I care about those two things relative to each other is very hard to write down. So I think it makes a lot of sense to try and learn from data. Yeah. Yeah. It sounds difficult enough to do manually when you're only thinking about one of those relationships. But when you're talking about highly dimensional set of features, it sounds very, very difficult. For sure. Yeah. Yeah. And especially if you really want to have good human-like behavior in a variety of settings, right? Most of the people are just paying attention to one thing when they're driving, right? All these things are balancing in their head. So you really do need to try and actually learn from data how to do this. And so talk about the motivation behind trying to apply inverse RL without RL. Yeah. And maybe was that a motivation or was that a result? So I think the motivation here is really like how we can make it inverse reinforcement learning more computationally efficient. So there's sort of like pros and cons, I think, to the sort of inverse reinforcement learning approach to things. I think the pro is basically that you don't suffer from something called compounding errors. So the simplest approach you could think of trying to do for imitation learning, right, is basically purely offline supervised learning approach. So what I do is I get a set of expert states, get a set of expert actions, I just regress between them, and I just roll out my car. The issue is that, of course, at some point, you're going to make a mistake, and you're going to end up in a situation in which you didn't have any data before. And then you're not going to know what to do, and you're just going to keep making mistakes over and over again. So if you kind of look at the early self-driving work in the late 80s, this is sort of what they were trying to do. And the cars drove a little bit, but as soon as you tried to do something hard with them, they didn't work. The benefit of the inverse reinforcement learning approaches is because you're actually rolling out the learner's policy, seeing how things go, you're able to see, OK, when I turn left, this is actually where I ended up. You're not just looking at states from the expert state distribution. So if you think of it as this set of maybe in the distribution shift setting, basically what interaction gives us is it lets us get samples from the test distribution, because we can actually drive and see where we end up. So this is really nice, because it lets you see where you're going to end up, so you're not going to make mistakes you don't expect. But the challenge, of course, then, is that you have to repeatedly interact with the simulator over and over again. And you have to solve these hard reinforcement learning problems. So conceptually, the way inverse reinforcement learning works is basically it's almost like again, but in the space of trajectories. So your generator here is like a policy that's kind of coupled with world models or dynamics kind of give you trajectories. And your discriminator here basically is saying, OK, I want to look at the difference between expert and learner trajectories. And then you do a policy update that is, OK, let me take this learned discriminator and use it as a reward function and do reinforcement learning with that. So the issue then is that you need to repeatedly solve a hard reinforcement learning problem at every single step of this procedure. So that means you're going to spend a huge number of samples. We already know that reinforcement learning is really, really hard to actually do in the real world on problems. And if I'm asking you to do it over and over again, well, it's kind of difficult. So our question in this work was, OK, there's all these benefits to the inverse reinforcement learning approach, but there are these severe computational issues. So how can we try and actually speed this up quite a bit? You arrived at an approach that does not use reinforcement learning, in fact? Yeah. So it's effectively an approach that allows us to learn to make a sequence of decisions, but without the sort of part that makes reinforcement learning hard, which is exploration. So maybe a sort of a concrete example I like here is something like, imagine what we want to do is sort of act optimally in some sort of problem that looks basically like going down the paths of a binary tree. And basically, I tell you that, OK, the word function this person cared about is zero everywhere except for one of the leaf nodes in this tree. And my discriminator says, OK, I'm going to pick this one leaf node to be one, everywhere else to be zero. Then my learner needs to explore the entire tree to figure out where the one node is that is non-zero. So that's a huge waste of time and compute and everything. The sort of insight we had in this work was that, well, if I know the states the person actually was in, I knew the person always went to the left in this tree, went to the leftmost node, I don't actually need to look at the rest of the tree because they never went there. That's probably the wrong thing to do. So I should be focusing my optimization just on the leftmost path. If you think about it that way, I think it's pretty reasonable to say that, OK, if I really just focus on optimizing on states that were related to what I saw the expert do, I should be able to cut out a lot of the unnecessary exploration. And so did you cut out all exploration or did you cut out unnecessary exploration, as you said, by constraining the search base of the states that you're looking at? Yeah, that's a really good question. So if I cut out all exploration, that's very close to doing something that's fully offline. And at that point, we wouldn't really have any robustness to compounding errors because we could just end up in a place we didn't expect. But if what I do is I have enough exploration that I learn to recover my own mistakes, but not so much that I explore the entire space of the world to figure out do this one thing, I can balance these two things. So I can be both computationally efficient and not end up in situations I don't expect and don't know how to recover from. Sure. You kind of constrain the search base and then you use some alternate method that's not reinforcement learning to kind of navigate through it. Yeah. So what we did was actually, I think, a very simple idea, which was that we basically did something very close to reinforcement learning, but we just changed the start state distribution to be that of the policy you want to imitate. And because of that, right, I'm still doing sequential decision making. I'm still learning to recover from all these step mistakes. But I'm saying, hey, you don't need to start at the top of this tree and figure out how to get to the bottom of the tree every single time. I'm telling you, okay, you're only going to be on the left part of the tree. Just make sure you're good there and then you're fine. So this is kind of how you can sort of constrain the search space, but still actually doing search reinforcement learning. So I think it's really like still, it's sort of, I think, the best of both worlds in the sense that you are getting rid of the part of reinforcement learning that's challenging, which is exploration, while keeping the part of it that is helpful, which is actually learning to recover from your own mistakes. And so talk a little bit about how you kind of assessed and evaluated your results for the paper. Yeah. So we thus far only got to try out things in simulation. I'm really excited about trying this out on more real world problems soon. we basically picked some of these sort of Majoco sort of open AI gym environments. And what we tried to do is basically see, you know, how well can we sort of imitate some expert policies? So we know we trained some policy via reinforcement learning and we said, okay, we want to learn another policy that does the same thing. How can we do this quickly? And what we did was we picked problems that are very challenging exploration problems. So imagine something that you're controlling, like a four legged creature that walks through like a large maze, right? You don't go through every single path in this maze if you're doing it with traditional reinforcement learning. Yeah. But if we basically tell you, hey, these are the set of waypoints through the maze that you only need to care about, it's a much easier optimization problem. And when we tried it out on like this sort of problem, we saw like rather remarkable improvements in terms of the amount of interaction with the environment needed. And there's the kind of computational benefit of it. But I also would say that I think there's a bit of a safety benefit in the sense that if your environment is actually something that is in the real world, you don't want your learner going around and doing crazy things all the time. You want to sort of keep them in a reasonable place. And in this example, were you worried about kind of extracting waypoints from expert data or did you assume that that was a kind of a downstream or upstream task, depending on perspective? That's a good question. I guess I would say it's an upstream task, but it's one that wasn't too bad. Basically what we could do is basically say that, okay, you know, every time step or every few time steps, you know, where was the expert at that time? And then we could just grab that point and say, okay, your waypoint, we can start the learner from this waypoint and see where they go. And so in terms of did you have existing data sets that you were able to apply to this and benchmark results or were you, how did you benchmark your performance? Yeah. What we did is we picked one of these sort of like standard environments that sort of, you know, a bunch of different methods have been tried on. These environments also come with a set of data. So we actually took some of the data sets from this sort of offline RL literature and we used that set of data. And then we sort of implemented each of the methods ourselves. And do you see this? You've used the autonomous vehicle analogy several times in describing this work. Do you see this method scaling up to that application? That's a far way from the four-legged open AI gym type of environment. Well, I mean, if you have four wheels, but I think it's actually a very reasonable application. They're actually, it's one I'm pretty excited about. Another one I'm super excited about is I recently learned that a lot of the routes in Google Maps are now calculated using inverse reinforcement learning. I think it's actually one of the world's most widely deployed machine learning systems now. And I think that these sort of techniques could be really, really useful there for just drastically reducing the amount of compute you need to do to compute routes and stuff like that. And I think also for self-driving, I think it's the sort of thing where, you know, even to this day, I think a good chunk of the self-driving industry, really the sort of bread and butter of the way their autonomous systems work is using different sorts of reinforcement learning or using inverse reinforcement learning techniques. So I could see this being applicable to a really wide set of kind of real world systems. And it's also a set of, it's also an application that I think is very reasonable in the sense that one of the sort of tricky parts about our method and one thing we're trying to address in future work is that I need to have a sort of a simulator environment where I can just put the learner in some state to start them off, right? Perhaps for like real world robots is really challenging, right? Like if I'm trying to get the robot to do a backflip, I don't know how I like reset the robot to be flipped halfway up in the air and then start it, right? But a lot of kind of stuff in the self-driving space, training is done in simulation. So this is actually a very reasonable thing. Like it shouldn't be hard at all to just basically, you know, just move the car to a different position in the simulator. So it's the sort of thing where I think actually self-driving is a sort of a perfect application of this sort of algorithm. I'm also very excited about the Mac being application because I think both of them are settings where this sort of thing should be able to help a lot. The sort of third application I'm really excited about is kind of in this space of large language models. So if you think about the sort of training of these models, right, there's a often a sort of fine tuning stuff at the end called RLHF or reinforcement learning from human feedback. And there, once again, we're using a very expensive reinforcement learning procedure, but we also have data of what we actually wanted, right? We know this was a data that was generated that people preferred. So it feels like we should be able to do something very similar there to basically speed up that search a lot. And there, like, you know, given how compute-attentive it is to train these models, I could expect this to provide like really strong computational benefits. So I guess what I'm trying to say here is that I think that there is a wide set of problems that kind of satisfy the assumptions required for this method to work out well. And I'm really excited about those applications. But I also think it's a very interesting question to think about for the problems where we can't do this really complicated, you know, agile robotics. What are sort of alternative approaches that could be useful there? Awesome. You've also got a couple of workshop papers at the conference. One is complementing a policy with a different observation space. Can you tell us a little bit about that one and what you're trying to do there? Yeah, of course. So I think this sort of touches on what I was talking a little bit about earlier, which is, you know, trying to make decisions without, you know, access to all the observed features. So the setup for this paper, I think, was pretty interesting. So let's say we have data of some doctor or set of doctors interacting with some patients. So, you know, we get data of what treatment they gave and, you know, what notes they perhaps took down. And, you know, we also see whether the patient got better or worse, things like that. And then what we want to do is we want to learn some sort of decision support system, you know, some sort of computerized agent that's able to help them. But, you know, because this agent is a computer rather than a person, they're not going to be able to see the same set of things. They're not going to get the same set of observations. And the question is, OK, how do I actually figure out how to help here? And let me try to give you a concrete example of why this is actually not an easy thing to do. So let's say I have a doctor who is prescribing chemotherapy to patients. And let's say the sort of feature they observe is some test that tells you with a high probability that you know whether this person has cancer or not. And let's say this doctor is a really good doctor. So they basically only give you chemotherapy if you actually need it. Otherwise, they don't. And let's say just for the sake of argument, our computerized agent here doesn't observe the result of this test. Well, what it's going to see is that every single time someone got chemotherapy, they got better. So it's then going to say, OK, the optimal thing for me to do is prescribe everyone chemotherapy because it only makes people get better. And then if you try to say, OK, and you try to trust the system a bit more, unless the vast majority of people had an underlying cancer condition, people are going to get worse as a result. So the question in this paper was, OK, how do we still learn how to do this well? We spend a bunch of time using some techniques from the sort of causal inference literature to try and basically kind of correctly estimate the sort of effect of an intervention like giving someone a drug and try to use that to really learn these sort of decisions of that are able to actually help. And so is this an application of causal modeling approaches? Yeah, fully. OK, and so talk a little bit about the specific approach you took. Yeah, yeah. So we sort of considered a couple different settings in the paper and each required a sort of, I guess, kind of different approach or different set of assumptions. So I think maybe the easiest setting was basically, OK, at train time, but not at test time, I get to see both sets of observations. So for whatever reason, I also get to see what the doctor saw. But you know, at test time, I'm not going to get to see that. The system doesn't get to get that. You can basically use this technique called the backdoor adjustment that was pioneered by Judah Pearl back in the day. And that sort of lets you effectively use an important sampling correction to fix incorrect estimates. So basically, you'd be able to sort of figure out that, hey, you know, it's this feature I didn't observe that actually was causing this positive effect. So I shouldn't like kind of over misattributed something else. I shouldn't like overestimate the value of the chemotherapy. Of course, this is somewhat unrealistic setting. So then we spend some time thinking about harder settings. So one setting is, OK, you don't get to see what the doctor was looking at. But I do tell you, you know, what was their probability of taking this action? You know, so how likely were they actually to give this person chemotherapy? And then what you can do is you can again do a sort of important sampling technique to kind of fix things. The sort of hardest setting is when I don't give you either of those. I just give you the actions and I give you a different set of observations. And if you think about this, this is a really hard problem, right? There's no reason to believe without assumptions you could actually solve this problem, right? Like it's like it's like accessing your self-driving car to be able to stop, you know, this and see the stop sign. It's a really hard problem. So in this case, you're giving the model a set of actions that doctors took to, you know, treatment actions without showing the outcomes. So we're giving them the actions and the outcomes, but not what the doctor saw when they were choosing to give them that treatment. Not the observations. Yeah, yeah, exactly, exactly. Yeah, yeah. If we don't give them the outcomes, that's very, yeah, really hard. So, you know, it's like the way I like to think about this, it's almost like imagine if you and I were playing a game and it's to predict the outcome of a coin flip. And I get to see the outcome of the coin flip. I'm definitely going to win this game, right? So you can't do this without any assumptions. So in particular, there's this technique from the sort of econometrics literature called a proxy correction, which simply put, basically says that even if there's something I don't observe that influences a relationship I care about. So long as I actually have proxies for this, that kind of vary in interesting ways, I'm able to use these to basically kind of filter out the effect of this thing I don't observe. So we actually use a more modern version of those techniques to basically kind of correctly estimate the effect of actually giving a person a drug. I mean, it sounds like a lot of what you're trying to do here is to correct for kind of sampling imbalance in your data set. Is that a fair assessment of challenge? That's an interesting question. I guess it's it's a sort of correcting for sampling balance. And it's a very interesting sampling balance where it's one where the sampling was done based on something you don't observe. And because of that, if you make conclusions based on this data without accounting for that fact, you're going to draw incorrect conclusions. It's definitely a sort of that sort of thing, but it's a very precise sort of sampling balance. Right. So the generator function of the what's in your observation set and what's not is kind of out of scope. And so you've got this whole other set of observations that you just don't have access to. And you're trying to as efficiently as possible, kind of identify ways to modify the actions you would take on your observations based on some knowledge of what the rest of the world is like. Yeah, that's a great way of putting it. Yeah, exactly. And then the other workshop paper is one called Learning Shared Safety Constraints from Multitask Demonstrations. Talk a little bit about the setting there. Sure. Yeah. So I think for like a variety of tasks you would want, you know, some sort of agent to do, there's kind of a background set of shared safety constraints you might care about. Right. So, you know, regardless of whether somebody is cleaning the table or, you know, making a sandwich in your kitchen, they shouldn't set the kitchen on fire. That would be nice. Yeah, yeah, I think I'd be pretty mad if someone did that. And I think the question is, you know, how we can try and learn these sorts of constraints from demonstrations. And if you think about it, this is really similar to what I was talking about earlier, where we're trying to learn rewards from demonstrations. And I think what we're trying to do is use a very sort of similar flavor of approach, but here to try and learn these sort of like safety constraints. And the idea here is that you're it's similar in a sense to what we talked about previously. You've got a whole set of observations. The observations are all focused on kind of what to do and you want to infer what not to do. But there's certainly a lot of things that aren't done in your observation set. That's a really interesting insight. Exactly. And so I think the way we tried to frame this problem is basically this. Let's say I told you what the task the person was trying to do was. You sort of you know the reward function. And I also give you what they actually did. So any suboptimal action they took then has to be because, oh, there was a safety constraint. Right. Like the reason they didn't if they were trying to get to the destination as fast as possible. Right. If they didn't run through all the other cars, it must be because they don't want to hit the other cars. So you can use a sort of comparison between the optimal behavior under the reward function and the actual behavior you saw. To try to extract what explains this difference. But if you over apply that heuristic, then you also limit your ability to make the process more efficient, learn better ways to do it, things like that. Is that kind of the core limitation that you're fighting against? Yeah, good question. I think the core limitation we are fighting against is, well, this problem is a really kind of ill-posed problem in some sense. In the sense that if for everything I don't see, I could just say there's a constraint that you can't do that. But it's possible that there was just no need to do that, not that it was unsafe to do that. So the kind of key thing we were focusing on this work is saying, OK, how do we fix that problem? And our insight was that, well, what we need is just a lot of multitask data. You see people doing all these different things in the environment. Make a sandwich, you see them, you know, clean the table, you see them wash the dishes. And in none of these things, you see them, you know, set the kitchen on fire. You can probably safely assume you're not supposed to set the kitchen on fire. So it's sort of aggregating this data from a lot of different tasks. You don't kind of learn this overly conservative constraint. In a sense, to oversimplify, perhaps, like that seems like the obvious solution to the problem, right? If you're observing or if your model's kind of taking these observations and trying to identify what's unsafe and anything it doesn't see, it's going to deem unsafe. We'll just make sure it sees a lot and a lot of different things. If we're talking about the NRL type of setting, you know, we've previously talked about how expensive that can be. Do you also look at the efficiency aspects of it? Is there something about the way that you approach multitask-ness that helps to deal with that? Yeah, that's a really good question. So we didn't really explore it in much in this particular paper, but you could actually use the algorithm we talked about in the first paper for just, you know, basically resetting the states from the expert demonstrations to basically, you know, try and solve this problem faster. Right. So you could basically sort of take out the reinforcement learning part of this paper and just stick in the algorithm we had earlier. And I think basically everything would go through both in theory and in practice. So that's why I think I'm particularly excited about the sort of techniques we talked about in the first paper, because I think of it as a hammer that can be used for a really wide set of sequential decision making problems. And so what data sets did you use for this particular paper? Yeah. So for this particular paper, we actually wanted to make the problem really hard. So we took some of the standard offline RL benchmarks and we just made them much harder. And we actually have a result that I'm really excited about in this paper, which is that we have this, you know, this agent, four legged ant running on this maze, and we're able to recover all the walls of the maze, the full structure of the maze without the ant ever interacting with any of the walls. Oh, wow. Just by looking at the paths of the maze. That was really exciting to me because I was like, this is really good news. Like, I've seen people try to approach this problem before, but usually they're not able to solve anything beyond like a linear or tabular problem. There's this here where we're able to do something. I was like, I could actually actually imagine using this. And I think the reason that's true for this method is it's built on, you know, sort of this kind of strong theoretical and algorithmic foundation of kind of techniques on the inverse reinforcement manual literature, which, you know, given they work reliably, you know, in the real world for things like self-driving cars and mapping, it's not super surprising. They also worked well for this problem. So I think it's sort of by building on a rather solid piece of bedrock. I think I'm really happy that the results work as well as they did. And so besides applying that first paper that we talked about, lots of different areas, what are you most looking forward to in terms of your your research agenda? That's a good question. I think I have a few different things that I'm still trying to trying to figure out. So one of them is the sort of key assumption we're making in that first paper is that we have access to what people in the theory literature will call a generative model access to the environment, which basically means that I can just plop the learner in some random state and then see what they do from there. Right. And I'm very interested in the question of, well, if we can't do that, which is true for certain problems, how do we still curtail unnecessary exploration? And I think that's like a very interesting theoretical question. I don't know if I have like a super good answer to it yet. But I think basically you could imagine something of the form that anytime you go pretty far outside of the sort of state distribution of the expert, we just assume bad things happen and the learner should learn, hey, I should probably not do that. So that's one thing I'm very excited about. Another thing I'm pretty excited about these days is sort of these kind of space of large language models. I think they're a really interesting kind of domain because I think the sort of core way you kind of get them started right is with the supervised, you know, kind of training. Right. And I just spent a bunch of time arguing about basically you should care about these sort of better algorithms. You shouldn't just do supervised learning. Well, then LLMs, right. You just do supervised learning for at least getting them started and it works great. And I think the sort of insight there is, well, if you have a huge amount of data, like a whole internet's worth, and you have like a model with, you know, however many bajillion parameters, then perhaps you don't need to care as much about, you know, the right algorithms for these problems. But as soon as you start to get to a problem where you don't have as much data, then I think you need to care more about, you know, how do you do things in an efficient manner. And I think the place you see that for LLMs is in the sort of fine tuning steps. So specifically the sort of RLHF step there, right, because you actually need to get a person to rate, you know, which of these completions was better. You can get like an infinite amount of data of this. And at that point, you have to be a lot more careful about algorithms. So I'm really excited about, you know, trying to do that problem more efficiently. The other sort of problem there, I think, is really technically interesting is people always describe RLHF as sort of an alignment procedure. And the question is like, well, alignment with who, right? It's like with this pool of raiders, all of whom might have different preferences and stuff like that. And there's a lot of kind of literature in the sort of economics and social choice theory spaces about basically how do we aggregate different preferences in a way that is reasonable? And I think there's a very interesting question there about how do we do that for reinforcement learning and human feedback. I spent some time thinking about that. The other problem, which I pulled it on the back burner, one of the sort of settings in which I think you have this kind of flavor problem I really like, which is that you have repeated interaction and you have kind of partial durability is problems in the recommendation space. So if I'm recommending you content, right, content, I recommend you change your preferences in some way, right? Like if I tell you if I kind of introduce you to a new genre of music, well, then I'm probably going to, you know, want to listen to more music like that. But I don't actually ever observe your preferences, right? All I observe is that you clicked on this thing. So you're in the setting where it's very similar to the sort of settings I was thinking about earlier. And I think there's a really interesting space of trying to kind of adapt the algorithms I've talked about previously to these problems. And there's some preliminary work that's trying to get at this. I think there's some really beautiful work in the Spotify team on sort of treating recommendation as a sequential decision making problem rather than just like a one step problem. And you see like really improved benefits. Actually, I think podcast recommendations in Spotify are now done using reinforcement learning. And I think that some of the techniques I've been working on might be really useful for dealing with the fact that, hey, we don't actually observe everything that's going on here. And we need to be a little bit more careful about the way we make decisions. Nice. Awesome. Well, thanks so much for joining us to share a bit about your ICML papers and some of your research. Yeah. Thank you so much for having me. Awesome. Thanks, Coco. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimbleai.com. Of course, if you like what you hear on the podcast, please subscribe, rate and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}}, "transcribed_data": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone. Welcome to another episode of the TwiML AI podcast. I'm your host, Sam Charrington, and today I'm joined by Gokul Swamy. Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University. Wherever you're listening to today's show, make sure you're subscribed and be sure to like, rate, and review the show. Gokul, welcome to the podcast. Yeah. Thank you so much for having me. Hey, I'm looking forward to digging into our conversation. We'll be talking about your research into the fields of efficient interactive learning and making good decisions without observable boundaries, with a particular emphasis on a few of the papers that you are presenting at this year's ICML conference. Before we dig into that though, I'd love to have you share a little bit about your background and how you came to the field. Yeah, good question. So I just wrapped up my third year at CMU. I spent a bunch of time there working on different things that are about sort of algorithms for this field called imitation learning, which is broadly speaking about how you can try and learn to make good decisions from data. Before that, I spent a few years at Berkeley where I was a master's and undergrad student, and there I was working on methods for human-robot interaction. Even before that, I grew up in San Diego, where I tried to just maximize the amount of time I spent on the beach. Sounds worthwhile. Yeah. Tell us a little bit about your research interests and the focus of your work. Yeah. So I think over the last few years, what I've been really trying to think about is how we can try and learn to make sequence of decisions well from observing data, some sort of expert demonstrated during the same thing. So perhaps the most intuitive example of this is something like self-driving cars. So you put some person in a car, we strap the car up with sensors, we get them to drive all around Pittsburgh, and then we want to learn a program, let's the car do the same thing. And I spent a lot of time thinking about what are the right sorts of algorithms for that problem. And I think there's two parts of that that I'm most interested in. The first is how we do this efficiently. And I mean efficiently in various senses, both in terms of the amount of data you need to use, the amount of compute you need to use, stuff like that. The other thing I'm really interested in is if we're trying to get a car to do the same thing as a person, they don't have the same observation space in some sense, right? They don't see exactly the same way, they don't have the exact same sensors. And at that point, there's all these sorts of things that show up that affect a relationship you care about that you don't really observe. So let's say you're trying to predict from some variable X to some variable Y, and there's some unobserved variable U that affects both. This is usually called an unobserved confounder. And here I'm trying to predict from some sort of observations, some sort of how much I want to the steering wheel, how much I want to press the gas pedal, stuff like that. For example, your self-driving car might not be able to pick up on the fact that a hand gesture from somebody in the car across from them actually means something. And when you kind of have this sort of partial observability, I think it's a really interesting question of how do you still learn well? So I've been thinking about both in the sort of idealized setting, how do you do it efficiently, and in the sort of more real world setting where you don't perhaps get the same access to the same pieces of information, how do you make decisions well there? Awesome. And how does that tie into the research that you're presenting at the conference this year? Yeah, so I think we have a few papers at the conference, which I'm very thankful for. And they sort of touch on different parts of those topics. So we have one paper at the main conference, which is really focused on the question of how do we do imitation learning efficiently. So that paper, I think, is really cool because it's a paper where the theory is very elegant. It's not a very sort of complicated idea mathematically, but it really, really works in practice. So we're quite excited about that. Is this the inverse RL paper? Yeah, yeah. So I can maybe talk a little bit about that if that would be good. Yeah, let's dig into it. Sure. So just before you do the title of the paper is inverse reinforcement learning without reinforcement learning, which is an intriguing title. Thank you. Yeah. So broadly speaking, right, in science is we have sort of like forward problems and inverse problems, right? You know, the forward problem is usually, okay, I have some kind of objective function and I'm going to optimize it to get something. And the inverse problem is, okay, given behavior of the optimal thing, what was the thing I was trying to optimize in the first place? And in the sort of inverse reinforcement learning setting, right, the forward problem is reinforcement learning. So I give you a word function. I want you to find the optimal policy under this word function. The inverse problem is, okay, I gave you data from the optimal policy and I want you to figure out what was the word function that was being optimized here. So if you think about the driving example, right, I might want to try to extract a function that tells you, okay, how much does this person care about, you know, staying away from the cars that are in front of them? How much do they care about, you know, adhering to the speed limits, things like that? And I think it's a very natural question to ask, like, well, can't I just write down a function that does that? And it's actually really quite hard when you try to do it, right? Because I can tell you that, okay, I definitely care about observing the speed limit. I care about staying away from people that are in front of me. But exactly how much I care about those two things relative to each other is very hard to write down. So I think it makes a lot of sense to try and learn from data. Yeah. Yeah. It sounds difficult enough to do manually when you're only thinking about one of those relationships. But when you're talking about highly dimensional set of features, it sounds very, very difficult. For sure. Yeah. Yeah. And especially if you really want to have good human-like behavior in a variety of settings, right? Most of the people are just paying attention to one thing when they're driving, right? All these things are balancing in their head. So you really do need to try and actually learn from data how to do this. And so talk about the motivation behind trying to apply inverse RL without RL. Yeah. And maybe was that a motivation or was that a result? So I think the motivation here is really like how we can make it inverse reinforcement learning more computationally efficient. So there's sort of like pros and cons, I think, to the sort of inverse reinforcement learning approach to things. I think the pro is basically that you don't suffer from something called compounding errors. So the simplest approach you could think of trying to do for imitation learning, right, is basically purely offline supervised learning approach. So what I do is I get a set of expert states, get a set of expert actions, I just regress between them, and I just roll out my car. The issue is that, of course, at some point, you're going to make a mistake, and you're going to end up in a situation in which you didn't have any data before. And then you're not going to know what to do, and you're just going to keep making mistakes over and over again. So if you kind of look at the early self-driving work in the late 80s, this is sort of what they were trying to do. And the cars drove a little bit, but as soon as you tried to do something hard with them, they didn't work. The benefit of the inverse reinforcement learning approaches is because you're actually rolling out the learner's policy, seeing how things go, you're able to see, OK, when I turn left, this is actually where I ended up. You're not just looking at states from the expert state distribution. So if you think of it as this set of maybe in the distribution shift setting, basically what interaction gives us is it lets us get samples from the test distribution, because we can actually drive and see where we end up. So this is really nice, because it lets you see where you're going to end up, so you're not going to make mistakes you don't expect. But the challenge, of course, then, is that you have to repeatedly interact with the simulator over and over again. And you have to solve these hard reinforcement learning problems. So conceptually, the way inverse reinforcement learning works is basically it's almost like again, but in the space of trajectories. So your generator here is like a policy that's kind of coupled with world models or dynamics kind of give you trajectories. And your discriminator here basically is saying, OK, I want to look at the difference between expert and learner trajectories. And then you do a policy update that is, OK, let me take this learned discriminator and use it as a reward function and do reinforcement learning with that. So the issue then is that you need to repeatedly solve a hard reinforcement learning problem at every single step of this procedure. So that means you're going to spend a huge number of samples. We already know that reinforcement learning is really, really hard to actually do in the real world on problems. And if I'm asking you to do it over and over again, well, it's kind of difficult. So our question in this work was, OK, there's all these benefits to the inverse reinforcement learning approach, but there are these severe computational issues. So how can we try and actually speed this up quite a bit? You arrived at an approach that does not use reinforcement learning, in fact? Yeah. So it's effectively an approach that allows us to learn to make a sequence of decisions, but without the sort of part that makes reinforcement learning hard, which is exploration. So maybe a sort of a concrete example I like here is something like, imagine what we want to do is sort of act optimally in some sort of problem that looks basically like going down the paths of a binary tree. And basically, I tell you that, OK, the word function this person cared about is zero everywhere except for one of the leaf nodes in this tree. And my discriminator says, OK, I'm going to pick this one leaf node to be one, everywhere else to be zero. Then my learner needs to explore the entire tree to figure out where the one node is that is non-zero. So that's a huge waste of time and compute and everything. The sort of insight we had in this work was that, well, if I know the states the person actually was in, I knew the person always went to the left in this tree, went to the leftmost node, I don't actually need to look at the rest of the tree because they never went there. That's probably the wrong thing to do. So I should be focusing my optimization just on the leftmost path. If you think about it that way, I think it's pretty reasonable to say that, OK, if I really just focus on optimizing on states that were related to what I saw the expert do, I should be able to cut out a lot of the unnecessary exploration. And so did you cut out all exploration or did you cut out unnecessary exploration, as you said, by constraining the search base of the states that you're looking at? Yeah, that's a really good question. So if I cut out all exploration, that's very close to doing something that's fully offline. And at that point, we wouldn't really have any robustness to compounding errors because we could just end up in a place we didn't expect. But if what I do is I have enough exploration that I learn to recover my own mistakes, but not so much that I explore the entire space of the world to figure out do this one thing, I can balance these two things. So I can be both computationally efficient and not end up in situations I don't expect and don't know how to recover from. Sure. You kind of constrain the search base and then you use some alternate method that's not reinforcement learning to kind of navigate through it. Yeah. So what we did was actually, I think, a very simple idea, which was that we basically did something very close to reinforcement learning, but we just changed the start state distribution to be that of the policy you want to imitate. And because of that, right, I'm still doing sequential decision making. I'm still learning to recover from all these step mistakes. But I'm saying, hey, you don't need to start at the top of this tree and figure out how to get to the bottom of the tree every single time. I'm telling you, okay, you're only going to be on the left part of the tree. Just make sure you're good there and then you're fine. So this is kind of how you can sort of constrain the search space, but still actually doing search reinforcement learning. So I think it's really like still, it's sort of, I think, the best of both worlds in the sense that you are getting rid of the part of reinforcement learning that's challenging, which is exploration, while keeping the part of it that is helpful, which is actually learning to recover from your own mistakes. And so talk a little bit about how you kind of assessed and evaluated your results for the paper. Yeah. So we thus far only got to try out things in simulation. I'm really excited about trying this out on more real world problems soon. we basically picked some of these sort of Majoco sort of open AI gym environments. And what we tried to do is basically see, you know, how well can we sort of imitate some expert policies? So we know we trained some policy via reinforcement learning and we said, okay, we want to learn another policy that does the same thing. How can we do this quickly? And what we did was we picked problems that are very challenging exploration problems. So imagine something that you're controlling, like a four legged creature that walks through like a large maze, right? You don't go through every single path in this maze if you're doing it with traditional reinforcement learning. Yeah. But if we basically tell you, hey, these are the set of waypoints through the maze that you only need to care about, it's a much easier optimization problem. And when we tried it out on like this sort of problem, we saw like rather remarkable improvements in terms of the amount of interaction with the environment needed. And there's the kind of computational benefit of it. But I also would say that I think there's a bit of a safety benefit in the sense that if your environment is actually something that is in the real world, you don't want your learner going around and doing crazy things all the time. You want to sort of keep them in a reasonable place. And in this example, were you worried about kind of extracting waypoints from expert data or did you assume that that was a kind of a downstream or upstream task, depending on perspective? That's a good question. I guess I would say it's an upstream task, but it's one that wasn't too bad. Basically what we could do is basically say that, okay, you know, every time step or every few time steps, you know, where was the expert at that time? And then we could just grab that point and say, okay, your waypoint, we can start the learner from this waypoint and see where they go. And so in terms of did you have existing data sets that you were able to apply to this and benchmark results or were you, how did you benchmark your performance? Yeah. What we did is we picked one of these sort of like standard environments that sort of, you know, a bunch of different methods have been tried on. These environments also come with a set of data. So we actually took some of the data sets from this sort of offline RL literature and we used that set of data. And then we sort of implemented each of the methods ourselves. And do you see this? You've used the autonomous vehicle analogy several times in describing this work. Do you see this method scaling up to that application? That's a far way from the four-legged open AI gym type of environment. Well, I mean, if you have four wheels, but I think it's actually a very reasonable application. They're actually, it's one I'm pretty excited about. Another one I'm super excited about is I recently learned that a lot of the routes in Google Maps are now calculated using inverse reinforcement learning. I think it's actually one of the world's most widely deployed machine learning systems now. And I think that these sort of techniques could be really, really useful there for just drastically reducing the amount of compute you need to do to compute routes and stuff like that. And I think also for self-driving, I think it's the sort of thing where, you know, even to this day, I think a good chunk of the self-driving industry, really the sort of bread and butter of the way their autonomous systems work is using different sorts of reinforcement learning or using inverse reinforcement learning techniques. So I could see this being applicable to a really wide set of kind of real world systems. And it's also a set of, it's also an application that I think is very reasonable in the sense that one of the sort of tricky parts about our method and one thing we're trying to address in future work is that I need to have a sort of a simulator environment where I can just put the learner in some state to start them off, right? Perhaps for like real world robots is really challenging, right? Like if I'm trying to get the robot to do a backflip, I don't know how I like reset the robot to be flipped halfway up in the air and then start it, right? But a lot of kind of stuff in the self-driving space, training is done in simulation. So this is actually a very reasonable thing. Like it shouldn't be hard at all to just basically, you know, just move the car to a different position in the simulator. So it's the sort of thing where I think actually self-driving is a sort of a perfect application of this sort of algorithm. I'm also very excited about the Mac being application because I think both of them are settings where this sort of thing should be able to help a lot. The sort of third application I'm really excited about is kind of in this space of large language models. So if you think about the sort of training of these models, right, there's a often a sort of fine tuning stuff at the end called RLHF or reinforcement learning from human feedback. And there, once again, we're using a very expensive reinforcement learning procedure, but we also have data of what we actually wanted, right? We know this was a data that was generated that people preferred. So it feels like we should be able to do something very similar there to basically speed up that search a lot. And there, like, you know, given how compute-attentive it is to train these models, I could expect this to provide like really strong computational benefits. So I guess what I'm trying to say here is that I think that there is a wide set of problems that kind of satisfy the assumptions required for this method to work out well. And I'm really excited about those applications. But I also think it's a very interesting question to think about for the problems where we can't do this really complicated, you know, agile robotics. What are sort of alternative approaches that could be useful there? Awesome. You've also got a couple of workshop papers at the conference. One is complementing a policy with a different observation space. Can you tell us a little bit about that one and what you're trying to do there? Yeah, of course. So I think this sort of touches on what I was talking a little bit about earlier, which is, you know, trying to make decisions without, you know, access to all the observed features. So the setup for this paper, I think, was pretty interesting. So let's say we have data of some doctor or set of doctors interacting with some patients. So, you know, we get data of what treatment they gave and, you know, what notes they perhaps took down. And, you know, we also see whether the patient got better or worse, things like that. And then what we want to do is we want to learn some sort of decision support system, you know, some sort of computerized agent that's able to help them. But, you know, because this agent is a computer rather than a person, they're not going to be able to see the same set of things. They're not going to get the same set of observations. And the question is, OK, how do I actually figure out how to help here? And let me try to give you a concrete example of why this is actually not an easy thing to do. So let's say I have a doctor who is prescribing chemotherapy to patients. And let's say the sort of feature they observe is some test that tells you with a high probability that you know whether this person has cancer or not. And let's say this doctor is a really good doctor. So they basically only give you chemotherapy if you actually need it. Otherwise, they don't. And let's say just for the sake of argument, our computerized agent here doesn't observe the result of this test. Well, what it's going to see is that every single time someone got chemotherapy, they got better. So it's then going to say, OK, the optimal thing for me to do is prescribe everyone chemotherapy because it only makes people get better. And then if you try to say, OK, and you try to trust the system a bit more, unless the vast majority of people had an underlying cancer condition, people are going to get worse as a result. So the question in this paper was, OK, how do we still learn how to do this well? We spend a bunch of time using some techniques from the sort of causal inference literature to try and basically kind of correctly estimate the sort of effect of an intervention like giving someone a drug and try to use that to really learn these sort of decisions of that are able to actually help. And so is this an application of causal modeling approaches? Yeah, fully. OK, and so talk a little bit about the specific approach you took. Yeah, yeah. So we sort of considered a couple different settings in the paper and each required a sort of, I guess, kind of different approach or different set of assumptions. So I think maybe the easiest setting was basically, OK, at train time, but not at test time, I get to see both sets of observations. So for whatever reason, I also get to see what the doctor saw. But you know, at test time, I'm not going to get to see that. The system doesn't get to get that. You can basically use this technique called the backdoor adjustment that was pioneered by Judah Pearl back in the day. And that sort of lets you effectively use an important sampling correction to fix incorrect estimates. So basically, you'd be able to sort of figure out that, hey, you know, it's this feature I didn't observe that actually was causing this positive effect. So I shouldn't like kind of over misattributed something else. I shouldn't like overestimate the value of the chemotherapy. Of course, this is somewhat unrealistic setting. So then we spend some time thinking about harder settings. So one setting is, OK, you don't get to see what the doctor was looking at. But I do tell you, you know, what was their probability of taking this action? You know, so how likely were they actually to give this person chemotherapy? And then what you can do is you can again do a sort of important sampling technique to kind of fix things. The sort of hardest setting is when I don't give you either of those. I just give you the actions and I give you a different set of observations. And if you think about this, this is a really hard problem, right? There's no reason to believe without assumptions you could actually solve this problem, right? Like it's like it's like accessing your self-driving car to be able to stop, you know, this and see the stop sign. It's a really hard problem. So in this case, you're giving the model a set of actions that doctors took to, you know, treatment actions without showing the outcomes. So we're giving them the actions and the outcomes, but not what the doctor saw when they were choosing to give them that treatment. Not the observations. Yeah, yeah, exactly, exactly. Yeah, yeah. If we don't give them the outcomes, that's very, yeah, really hard. So, you know, it's like the way I like to think about this, it's almost like imagine if you and I were playing a game and it's to predict the outcome of a coin flip. And I get to see the outcome of the coin flip. I'm definitely going to win this game, right? So you can't do this without any assumptions. So in particular, there's this technique from the sort of econometrics literature called a proxy correction, which simply put, basically says that even if there's something I don't observe that influences a relationship I care about. So long as I actually have proxies for this, that kind of vary in interesting ways, I'm able to use these to basically kind of filter out the effect of this thing I don't observe. So we actually use a more modern version of those techniques to basically kind of correctly estimate the effect of actually giving a person a drug. I mean, it sounds like a lot of what you're trying to do here is to correct for kind of sampling imbalance in your data set. Is that a fair assessment of challenge? That's an interesting question. I guess it's it's a sort of correcting for sampling balance. And it's a very interesting sampling balance where it's one where the sampling was done based on something you don't observe. And because of that, if you make conclusions based on this data without accounting for that fact, you're going to draw incorrect conclusions. It's definitely a sort of that sort of thing, but it's a very precise sort of sampling balance. Right. So the generator function of the what's in your observation set and what's not is kind of out of scope. And so you've got this whole other set of observations that you just don't have access to. And you're trying to as efficiently as possible, kind of identify ways to modify the actions you would take on your observations based on some knowledge of what the rest of the world is like. Yeah, that's a great way of putting it. Yeah, exactly. And then the other workshop paper is one called Learning Shared Safety Constraints from Multitask Demonstrations. Talk a little bit about the setting there. Sure. Yeah. So I think for like a variety of tasks you would want, you know, some sort of agent to do, there's kind of a background set of shared safety constraints you might care about. Right. So, you know, regardless of whether somebody is cleaning the table or, you know, making a sandwich in your kitchen, they shouldn't set the kitchen on fire. That would be nice. Yeah, yeah, I think I'd be pretty mad if someone did that. And I think the question is, you know, how we can try and learn these sorts of constraints from demonstrations. And if you think about it, this is really similar to what I was talking about earlier, where we're trying to learn rewards from demonstrations. And I think what we're trying to do is use a very sort of similar flavor of approach, but here to try and learn these sort of like safety constraints. And the idea here is that you're it's similar in a sense to what we talked about previously. You've got a whole set of observations. The observations are all focused on kind of what to do and you want to infer what not to do. But there's certainly a lot of things that aren't done in your observation set. That's a really interesting insight. Exactly. And so I think the way we tried to frame this problem is basically this. Let's say I told you what the task the person was trying to do was. You sort of you know the reward function. And I also give you what they actually did. So any suboptimal action they took then has to be because, oh, there was a safety constraint. Right. Like the reason they didn't if they were trying to get to the destination as fast as possible. Right. If they didn't run through all the other cars, it must be because they don't want to hit the other cars. So you can use a sort of comparison between the optimal behavior under the reward function and the actual behavior you saw. To try to extract what explains this difference. But if you over apply that heuristic, then you also limit your ability to make the process more efficient, learn better ways to do it, things like that. Is that kind of the core limitation that you're fighting against? Yeah, good question. I think the core limitation we are fighting against is, well, this problem is a really kind of ill-posed problem in some sense. In the sense that if for everything I don't see, I could just say there's a constraint that you can't do that. But it's possible that there was just no need to do that, not that it was unsafe to do that. So the kind of key thing we were focusing on this work is saying, OK, how do we fix that problem? And our insight was that, well, what we need is just a lot of multitask data. You see people doing all these different things in the environment. Make a sandwich, you see them, you know, clean the table, you see them wash the dishes. And in none of these things, you see them, you know, set the kitchen on fire. You can probably safely assume you're not supposed to set the kitchen on fire. So it's sort of aggregating this data from a lot of different tasks. You don't kind of learn this overly conservative constraint. In a sense, to oversimplify, perhaps, like that seems like the obvious solution to the problem, right? If you're observing or if your model's kind of taking these observations and trying to identify what's unsafe and anything it doesn't see, it's going to deem unsafe. We'll just make sure it sees a lot and a lot of different things. If we're talking about the NRL type of setting, you know, we've previously talked about how expensive that can be. Do you also look at the efficiency aspects of it? Is there something about the way that you approach multitask-ness that helps to deal with that? Yeah, that's a really good question. So we didn't really explore it in much in this particular paper, but you could actually use the algorithm we talked about in the first paper for just, you know, basically resetting the states from the expert demonstrations to basically, you know, try and solve this problem faster. Right. So you could basically sort of take out the reinforcement learning part of this paper and just stick in the algorithm we had earlier. And I think basically everything would go through both in theory and in practice. So that's why I think I'm particularly excited about the sort of techniques we talked about in the first paper, because I think of it as a hammer that can be used for a really wide set of sequential decision making problems. And so what data sets did you use for this particular paper? Yeah. So for this particular paper, we actually wanted to make the problem really hard. So we took some of the standard offline RL benchmarks and we just made them much harder. And we actually have a result that I'm really excited about in this paper, which is that we have this, you know, this agent, four legged ant running on this maze, and we're able to recover all the walls of the maze, the full structure of the maze without the ant ever interacting with any of the walls. Oh, wow. Just by looking at the paths of the maze. That was really exciting to me because I was like, this is really good news. Like, I've seen people try to approach this problem before, but usually they're not able to solve anything beyond like a linear or tabular problem. There's this here where we're able to do something. I was like, I could actually actually imagine using this. And I think the reason that's true for this method is it's built on, you know, sort of this kind of strong theoretical and algorithmic foundation of kind of techniques on the inverse reinforcement manual literature, which, you know, given they work reliably, you know, in the real world for things like self-driving cars and mapping, it's not super surprising. They also worked well for this problem. So I think it's sort of by building on a rather solid piece of bedrock. I think I'm really happy that the results work as well as they did. And so besides applying that first paper that we talked about, lots of different areas, what are you most looking forward to in terms of your your research agenda? That's a good question. I think I have a few different things that I'm still trying to trying to figure out. So one of them is the sort of key assumption we're making in that first paper is that we have access to what people in the theory literature will call a generative model access to the environment, which basically means that I can just plop the learner in some random state and then see what they do from there. Right. And I'm very interested in the question of, well, if we can't do that, which is true for certain problems, how do we still curtail unnecessary exploration? And I think that's like a very interesting theoretical question. I don't know if I have like a super good answer to it yet. But I think basically you could imagine something of the form that anytime you go pretty far outside of the sort of state distribution of the expert, we just assume bad things happen and the learner should learn, hey, I should probably not do that. So that's one thing I'm very excited about. Another thing I'm pretty excited about these days is sort of these kind of space of large language models. I think they're a really interesting kind of domain because I think the sort of core way you kind of get them started right is with the supervised, you know, kind of training. Right. And I just spent a bunch of time arguing about basically you should care about these sort of better algorithms. You shouldn't just do supervised learning. Well, then LLMs, right. You just do supervised learning for at least getting them started and it works great. And I think the sort of insight there is, well, if you have a huge amount of data, like a whole internet's worth, and you have like a model with, you know, however many bajillion parameters, then perhaps you don't need to care as much about, you know, the right algorithms for these problems. But as soon as you start to get to a problem where you don't have as much data, then I think you need to care more about, you know, how do you do things in an efficient manner. And I think the place you see that for LLMs is in the sort of fine tuning steps. So specifically the sort of RLHF step there, right, because you actually need to get a person to rate, you know, which of these completions was better. You can get like an infinite amount of data of this. And at that point, you have to be a lot more careful about algorithms. So I'm really excited about, you know, trying to do that problem more efficiently. The other sort of problem there, I think, is really technically interesting is people always describe RLHF as sort of an alignment procedure. And the question is like, well, alignment with who, right? It's like with this pool of raiders, all of whom might have different preferences and stuff like that. And there's a lot of kind of literature in the sort of economics and social choice theory spaces about basically how do we aggregate different preferences in a way that is reasonable? And I think there's a very interesting question there about how do we do that for reinforcement learning and human feedback. I spent some time thinking about that. The other problem, which I pulled it on the back burner, one of the sort of settings in which I think you have this kind of flavor problem I really like, which is that you have repeated interaction and you have kind of partial durability is problems in the recommendation space. So if I'm recommending you content, right, content, I recommend you change your preferences in some way, right? Like if I tell you if I kind of introduce you to a new genre of music, well, then I'm probably going to, you know, want to listen to more music like that. But I don't actually ever observe your preferences, right? All I observe is that you clicked on this thing. So you're in the setting where it's very similar to the sort of settings I was thinking about earlier. And I think there's a really interesting space of trying to kind of adapt the algorithms I've talked about previously to these problems. And there's some preliminary work that's trying to get at this. I think there's some really beautiful work in the Spotify team on sort of treating recommendation as a sequential decision making problem rather than just like a one step problem. And you see like really improved benefits. Actually, I think podcast recommendations in Spotify are now done using reinforcement learning. And I think that some of the techniques I've been working on might be really useful for dealing with the fact that, hey, we don't actually observe everything that's going on here. And we need to be a little bit more careful about the way we make decisions. Nice. Awesome. Well, thanks so much for joining us to share a bit about your ICML papers and some of your research. Yeah. Thank you so much for having me. Awesome. Thanks, Coco. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimbleai.com. Of course, if you like what you hear on the podcast, please subscribe, rate and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "- TL;DR: Gokul Swamy shares his research on efficient interactive learning and making decisions without observable boundaries.\n- Main Point 1: Gokul's research focuses on how to learn to make good decisions from observed data.\n- Main Point 2: He is particularly interested in the efficiency of learning algorithms and how to make decisions in situations where not all relevant information is observable.\n- Main Point 3: Gokul's papers at the ICML conference discuss inverse reinforcement learning without reinforcement learning and learning shared safety constraints from multitask demonstrations.\n- Surprising Fact: The inverse reinforcement learning approach can drastically reduce the amount of data and computation needed for learning.\n- Inspiring Takeaway: Gokul's research has practical applications in self-driving cars, mapping, language models, and recommendation systems, and can greatly improve computational efficiency.", "podcast_guest": "Gokul Swamy", "podcast_highlights": "Chapter 1: Introduction and Background\nChapter 2: Efficient Imitation Learning\nChapter 3: Inverse Reinforcement Learning without Reinforcement Learning\nChapter 4: Learning Shared Safety Constraints from Multitask Demonstrations"}