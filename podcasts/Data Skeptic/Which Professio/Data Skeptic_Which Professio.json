{"detailed_guest_info": "The podcast guest is a graduate of the Wharton School of the University of Pennsylvania and is part of a network of over 99,000 alumni in over 150 countries. They are connected to a diverse group of professionals, with the majority located in North America, and also have significant representation in Asia, Europe, the Caribbean and Latin America, Africa and the Middle East, and Australia and New Zealand.", "podcast_details": {"podcast_details": {"links": [{"href": "https://dataskeptic.libsyn.com/rss", "rel": "self", "type": "application/rss+xml"}, {"rel": "alternate", "type": "text/html", "href": "https://dataskeptic.com"}], "title": "Data Skeptic", "title_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Data Skeptic"}, "published": "Tue, 15 Aug 2023 12:00:00 +0000", "published_parsed": [2023, 8, 15, 12, 0, 0, 1, 227, 0], "updated": "Tue, 15 Aug 2023 12:07:56 +0000", "updated_parsed": [2023, 8, 15, 12, 7, 56, 1, 227, 0], "generator_detail": {"name": "Libsyn WebEngine 2.0"}, "generator": "Libsyn WebEngine 2.0", "link": "https://dataskeptic.com", "language": "en", "rights": "Creative Commons Attribution License 3.0", "rights_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Creative Commons Attribution License 3.0"}, "docs": "https://dataskeptic.com", "authors": [{"email": "kyle@dataskeptic.com"}, {"name": "Kyle Polich", "email": "kyle@dataskeptic.com"}], "author": "Kyle Polich", "author_detail": {"email": "kyle@dataskeptic.com"}, "summary": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches."}, "image": {"href": "https://ssl-static.libsyn.com/p/assets/0/e/4/b/0e4bd71bb64c6e45/DS_-_New_Logo_assets_-_JL_DS_Logo_Stacked_-_Color_2.jpg"}, "tags": [{"term": "Skepticism", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Statistics", "scheme": "http://www.itunes.com/", "label": null}, {"term": "datamining", "scheme": "http://www.itunes.com/", "label": null}, {"term": "datascience", "scheme": "http://www.itunes.com/", "label": null}, {"term": "machinelearning", "scheme": "http://www.itunes.com/", "label": null}, {"term": "science", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Technology", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Science", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Mathematics", "scheme": "http://www.itunes.com/", "label": null}], "itunes_explicit": null, "publisher_detail": {"name": "Kyle Polich", "email": "kyle@dataskeptic.com"}, "content": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches.", "content_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches."}, "itunes_type": "episodic", "podcast_locked": {"owner": "kyle@dataskeptic.com"}}, "first_episode": {"title": "Which Professions Are Threatened by LLMs", "title_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Which Professions Are Threatened by LLMs"}, "itunes_title": "Which Professions Are Threatened by LLMs", "published": "Tue, 15 Aug 2023 12:00:00 +0000", "published_parsed": [2023, 8, 15, 12, 0, 0, 1, 227, 0], "id": "d9f5f61e-7073-43ef-a46a-98846f960fcf", "guidislink": false, "links": [{"rel": "alternate", "type": "text/html", "href": "https://dataskeptic.com/blog/episodes/2023/which-professions-are-threatened-by-llms"}, {"length": "48378297", "type": "audio/mpeg", "href": "https://traffic.libsyn.com/secure/dataskeptic/which-professions-are-threatened-by-llms.mp3?dest-id=201630", "rel": "enclosure"}], "link": "https://dataskeptic.com/blog/episodes/2023/which-professions-are-threatened-by-llms", "image": {"href": "https://ssl-static.libsyn.com/p/assets/c/c/0/1/cc01de4d4b696390e55e3c100dce7605/Daniel_Rock.jpg"}, "summary": "<p>On today\u2019s episode, we have Daniel Rock, an Assistant Professor of Operations Information and Decisions at the Wharton School of the University of Pennsylvania. Daniel\u2019s research focuses on the economics of AI and ML, specifically how digital technologies are changing the economy.</p> <p>Daniel discussed how AI has disrupted the job market in the past years. He also explained that it had created more winners than losers.</p> <p>Daniel spoke about the empirical study he and his coauthors did to quantify the threat LLMs pose to professionals. He shared how they used the O-NET dataset and the BLS occupational employment survey to measure the impact of LLMs on different professions. Using the radiology profession as an example, he listed tasks that LLMs could assume.</p> <p>Daniel broadly highlighted professions that are most and least exposed to LLMs proliferation. He also spoke about the risks of LLMs and his thoughts on implementing policies for regulating LLMs.</p>", "summary_detail": {"type": "text/html", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "<p>On today\u2019s episode, we have Daniel Rock, an Assistant Professor of Operations Information and Decisions at the Wharton School of the University of Pennsylvania. Daniel\u2019s research focuses on the economics of AI and ML, specifically how digital technologies are changing the economy.</p> <p>Daniel discussed how AI has disrupted the job market in the past years. He also explained that it had created more winners than losers.</p> <p>Daniel spoke about the empirical study he and his coauthors did to quantify the threat LLMs pose to professionals. He shared how they used the O-NET dataset and the BLS occupational employment survey to measure the impact of LLMs on different professions. Using the radiology profession as an example, he listed tasks that LLMs could assume.</p> <p>Daniel broadly highlighted professions that are most and least exposed to LLMs proliferation. He also spoke about the risks of LLMs and his thoughts on implementing policies for regulating LLMs.</p>"}, "content": [{"type": "text/html", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "<p>On today\u2019s episode, we have Daniel Rock, an Assistant Professor of Operations Information and Decisions at the Wharton School of the University of Pennsylvania. Daniel\u2019s research focuses on the economics of AI and ML, specifically how digital technologies are changing the economy.</p> <p>Daniel discussed how AI has disrupted the job market in the past years. He also explained that it had created more winners than losers.</p> <p>Daniel spoke about the empirical study he and his coauthors did to quantify the threat LLMs pose to professionals. He shared how they used the O-NET dataset and the BLS occupational employment survey to measure the impact of LLMs on different professions. Using the radiology profession as an example, he listed tasks that LLMs could assume.</p> <p>Daniel broadly highlighted professions that are most and least exposed to LLMs proliferation. He also spoke about the risks of LLMs and his thoughts on implementing policies for regulating LLMs.</p>"}], "itunes_duration": "38:48", "itunes_explicit": null, "subtitle": "On today\u2019s episode, we have Daniel Rock, an Assistant Professor of Operations Information and Decisions at the Wharton School of the University of Pennsylvania. Daniel\u2019s research focuses on the economics of AI and ML, specifically how digital...", "subtitle_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "On today\u2019s episode, we have Daniel Rock, an Assistant Professor of Operations Information and Decisions at the Wharton School of the University of Pennsylvania. Daniel\u2019s research focuses on the economics of AI and ML, specifically how digital..."}, "itunes_episodetype": "full"}, "transcribed_data": {"podcast_title": "Data Skeptic", "episode_title": "Which Professions Are Threatened by LLMs", "episode_image": "https://ssl-static.libsyn.com/p/assets/0/e/4/b/0e4bd71bb64c6e45/DS_-_New_Logo_assets_-_JL_DS_Logo_Stacked_-_Color_2.jpg", "episode_transcript": " Welcome to Data Skeptic Machine Intelligence, our podcast series exploring contemporary topics in artificial general intelligence and large language models. So jobs. Everybody wants a job, while not everybody. Ideally there'd be a job for everyone who wants one, hopefully a good job. And there's certainly a lot of concern that AI may replace jobs in the near future. I have found that this is a topic that many people are quick to express certainty about, as though they can prognosticate the future. To shape my opinion on the possibilities of future unemployment, I want a data-driven perspective. And that's why I was very happy when I came across the paper, GPTs are general purpose technologies, an early look at the labor market's impact potential for LLMs. Today's interview with Daniel Rock, co-author on this paper, dives deep into exactly which professions would we estimate are going to be impacted by the proliferation of large language models. I'm Daniel Rock. I'm an assistant professor of operations, information, and decisions at the Wharton School of the University of Pennsylvania. And can you share a few details on what falls under that umbrella? Sure, yeah. Well, I mean, it's the usual things. I do a lot of research and teaching. I focus on the economics of AI and ML in my research. But more broadly, I'm interested in how digital technologies are changing the economy. I teach courses. I have an undergraduate core course that I teach in optimization and simulation. And then I teach an MBA course, digital business strategy, and becoming more and more AI as we go on. I also have an advanced undergrad class that sometimes I offer. Has there been much to study historically about the economic impacts of AI and ML, or is this really a recent trend? There's a few waves of AI and ML. If you really go back, there are people talking about good old fashioned AI, where people are writing down rules systems that you can apply. And some of those things actually work. Detecting fraud in a bank or something, sometimes you can use rules around data to do a reasonably good job with that. Yeah, each new wave brings with it a new set of technologies. And AI is a very fuzzy term. And it generally just encompasses people writing code that in some way mimics a human decision process. But I think now we're getting to the point where we might be able to create new types of processes, setting aside the human piece of it. A lot of people are beginning to have a fear that AI could impact the job market. Looking to the future is a little harder. Let's look to the past first. Would you say that AI has in any way historically impacted the job market? I think we're starting to see shoots of where the supervised learning proliferation in the last decade started to affect things. And then if you want to be expansive about it and think about all digital technologies, you can include the internet or just rules-based software or things where we can write a recipe and then say, here's the code, go do this process. Certainly impacted the labor market in major ways. Now the way that it's impacted the labor market is not generally wholesale automation. Obviously unemployment is super low right now. People are still working. We haven't automated everything. The sense in which it has changed the labor market, there's always winners and losers when these new technologies come out. The overall effect tends to be that there are more winners than losers, but you still have to think about where are people going to be displaced and how can we help them. The more winners than losers is an interesting argument. Actually I find it kind of compelling. I would like to at least hope that technology creates jobs as it destroys some or maybe creates three where it destroys two, something along those lines. Is there any scholarship on how to measure something like that? Sure. Well, my first stop for work of that sort is the work of Gerona Zamoglu and David Otter both at MIT, investigating the idea of skill bias, technical change, and then what they've called the canonical model in the past, a task-based model. You can think of a job as a bundle of tasks and you bring your skills and abilities to that bundle of tasks. That bundle is kind of determined by supply and demand. It's what makes sense to put together in the same job and we can look at over time how those bundles have changed. There's a lot of new work creation. So Gerona Zamoglu and Pascual Restrepo, for example, have documented that new task creation is the key addition to the task-based model. We started allocating tasks to machines. Where do the new tasks come from? That's the thing that critically we kind of have to measure going forward, but there's a ton of new work creation over the last few decades. A lot of the typical jobs that you see today, especially in tech, they didn't exist 15, 20 years ago even. That's definitely happening. I like to joke that data scientists as a job will cease to exist. Well, I think a data scientist, I was talking about this with a colleague last week, a data scientist is likely to just refer to an analyst role of some kind. But recent history has been it's an analyst, it's a data engineer, it's a product manager, it's all these things rolled up at once. I kind of think those things are going to get broken apart so that people can specialize again. Yeah, I tend to agree. The title, computer programmer, I don't think is anyone's official professional title anymore, but it's not like that stopped being a job. Exactly. Yeah, we're all still writing code and throwing things in our monitors. The whole large language model revolution, if we were to call it that, is a relatively recent thing. It could very well be that it's going to displace a lot of people in the even short-term future. Maybe there's just a time to roll it out or maybe that's already happened or maybe it's not going to happen. Are there any early indicators? Yeah, large language models are extremely quick on the adoption side, especially for these sort of peripheral things. I think, as a kind of cranky economist, I recoil a little bit at the idea that all of the adoption will be super quick and we'll be living in the future we expect to happen in a super short amount of time. I think these technologies are sufficiently transformative and the potential is sufficiently interesting across so many different domains that even if we stop doing development on LLM today, which by the way, I'm not endorsing as an idea, but if we were to stop, it would still be a decade plus before all of the different applications have been unearthed and distributed. Some quick things, you can have GPT write essays or copy for you, which is a problem for many educators like myself where you have to reconfigure the incentives in your classroom and how you teach. Well, problem for some, it's an opportunity from the perspective of others. There are quick areas where there are quick wins, quick things that can be actually kind of transformative and disruptive in terms of just what's available right now, but we're going to have to do a lot of thinking about how to integrate these systems into our existing ones. Well, there's an idea here a lot of people share, especially in the programming world, saying, oh, these can make everyone a 10x developer. This is sort of like the, what was Dr. Who's thing, the sonic screwdriver. That's essentially what this could be in everyone's hands. Having had your own hands-on experience with large language models, to what degree are you impressed or unimpressed with them? You know, I waver between being like, I guess I keep adjusting my expectations, which I shouldn't do. Like if you had asked me three years ago, would this exist? I would have said probably not. Now I'm like, oh man, it gave me a wrong answer, like a mildly wrong answer that's so upsetting. I've integrated using mostly GPT-4 into my workflow. I have it help me with coding assistance or if I want to describe say a plot, I'm trying to make for a paper or something. I can give it to GPT-4 and it sometimes gives me reasonable suggestions. Recently I've found it's more useful as kind of like an on-demand stack overflow for whatever I want to do and it has about the same fidelity as stack overflow for answering my questions, which means, just to say, I usually can't just copy paste whatever it gave me. I have to reconfigure it in some way. I think we're going to get better there. There's a lot of like rote things that if you ask it to do something relatively simple, it can give you decent code for that very quickly. But if you start getting more complex, then all of a sudden, yeah, you're in trouble. So I think of it really as like a guiding tool and can it make all of us 10x developers? Well, I think the 10x developers, 100x developers, these people who are really out in the tail in terms of skills, they're creative in how they combine the parts and they're creative in terms of the problems that they want to solve and how effective they can be in doing that. I think GPT will make us all that good. Certainly hasn't made me that good. I wish it had, but no, I think I'm still a 1x or less developer when it comes to what I do, which is to say maybe I've been multiplied by 10 in the first place, but I don't want to admit how low I was on the totem pole to begin with. Well, it would be good to have some empirical way with a little bit of methodology to try and assess where the real threat is here. Is it highly skilled workers, low skilled workers? How can we begin to have some sort of a measurement of these processes? Yeah, this is an exciting thing. I don't know about you, but I think there's a somewhat justifiable anxiety about how exposed knowledge work is to these new technologies. So typically, we've looked in the past and we've seen you're going to do the same billing process hundreds of times. You can write a recipe down and say, okay, we're going to code that up. That's going to be finished. But now we're seeing massive advancement in what used to be the domain of very highly paid people. So some co-authors at OpenAI and all, we wrote a paper, it's called GPTs or GPTs, that is Generative Pre-trained Transformers are General Purpose Technologies. And we investigate this. We can talk a little bit more about what we did precisely there, but we were trying to figure out how exposed certain tasks are to AI. When exposed isn't good or bad, it just means how you do that task might change with the help of a large language model. So sorry, I don't mean to interchangeably use AI and large language model, but in this case, we're focusing on large language models. So what we get here is you can aggregate from tasks to jobs. So we have exposures at the task levels and measurements of exposure at the task level. We aggregate those exposure measurements to the job level where you do get a little bit of a diversification benefit. The typical job does like 20 to 30 tasks. And then you can aggregate from jobs to whatever, you know, industries, firms, regions, whatever you have a job level distribution. What we found was that the jobs that are highest paid tend to be more exposed, which is unlike earlier waves of technology. That can be good news in the sense that like we've seen a few applications where the lowest skill workers within a job have been accelerated, kind of like what you were saying. Can we make everybody a 10x developer? Well, some work by Eric Brunelso and Lindsay Raymond and Danielle Lee found that in a call center for customer service agents, you can make people 10x customer service agents with the help of generative AI coaches. So that's inequality mitigating. And that's good in the sense that everybody's getting more productive. You might have concerns that since it's the best people and their practices being used to teach the least productive people and catch them up to the frontier, that you might have like, I call it a bananas problem. We get a monoculture and bananas and then like all those bananas are really vulnerable to some sort of disease. Like kind of similar analogous idea where we like, okay, everybody who does it right or does it really well does it in the same way, but then maybe there's some corner case that they haven't discovered how to do well. So we wouldn't want everybody coding in the same way that might create vulnerabilities. There's like a benefit to people having a wide variety of approaches. Now getting back to the wage distribution, everything I've said so far there is about within jobs, like the top performers to the worst performers. Now we can go to our lawyers and doctors more exposed than factory line workers, low-paid workers like coal miners or like people, short order cooks, things like that where they're not paid as much and they're also not as exposed. We could think about how much are those jobs going to change? And it may be the case that you automate a bunch of this stuff and then it could be like that lawyers just don't get paid as much. But I think far more likely is that we get sort of a scattering where the best lawyers figure out how to be even more productive with all this stuff or maybe as a group lawyers get even better. So it might be inequality increasing in the long run. And I think that's an open question. We won't know for a while. Kind of got to keep your eye on what might happen there. Well, it's intuitive to me to say a short order cook is a low wage worker and an attorney is probably a high wage worker. Could we talk for a moment about the data set upon which though you get some concrete numbers to work with? Yeah, we start with the US government puts out this great data set called ONET. ONET documents all the tasks associated with a set of about a thousand jobs, a little less than a thousand jobs, and what those people do in that job category with other details like you can pull the wages and merge those over to the Bureau of Labor Statistics, occupational employment surveys that's got measurements of how many people are in each job. It's got measurements of the wage distribution, where those people are located and in a course sense. So you can go to MSA level, Metropolitan Statistical Area, or you could go to like the state level. They also have industry breakdowns. We haven't gotten to how we get the scores yet, but assume you've got the scores at each one of these task levels, you can merge those over and then you can get aggregate at whatever level you want. So in terms of breaking down a occupation into the tasks it performs, I wouldn't necessarily expect you to have perfect recall of the exact taxonomy in that data set, but could you give some vague examples of like what is a discrete task? One of the fun ones I like to pick on is radiologists because there are some folks in the CS community predicting that we wouldn't have any radiologists. A few years ago we have increased our demand for radiologists since that prediction, so far ML is not destroying their jobs. But all right, so some things that radiologists might do, looking right at the O-net page for them, they might obtain patients' histories from electronic records, patient interviews, dictated reports, or by communicating with referring clinicians. I guess that's one. Another is perform or interpret the outcomes of diagnostic imaging procedures, including magnetic resonance imaging, computer tomography, CTs, positron emission tomography, PET scanners, nuclear cardiology treadmill studies, mammography, or ultrasound. That one is pretty exposed to standard deep learning, like look at an x-ray and tell me where something might be wrong or anomalous. Obtaining patient histories, that's probably something you could bolt a large language model onto a database and instead of having to use SQL you'd use natural language to do it, which is kind of nice. And then there's communicate examination results or diagnostic information to referring physicians, patients, or families. That's not necessarily the sort of thing that you really want a machine doing, especially if the news ain't great. The way I think about radiologists in particular here, and there's 30 of these tasks, so we talked about three of them, I think of radiologists as data enabled doctors. And from that perspective, it's pretty clear that these new technologies augment what they're doing and make them potentially better at their job. There's a lot of evidence. We have a shout out to Centaur Labs. They're a really interesting company using AI with medical images. A friend of mine, Eric Duhaime, founded the company. They're kind of making this case that these Centaurs, that is a combination of humans and machines, provides better outcomes than either one can on their own. And there's a whole bunch of socio, the technical term, the socio-technical integration of these things. How do we bring the technology into our social systems and have it be effective when people don't necessarily trust algorithms or the patterns of how they trust algorithms or don't are sort of hidden? Then we have to discover them, especially when you've got workers with tons of agency about how they deploy the technology. This is going to take a while, but using data sets like ONET or the BLS to get some of this information, while not perfect, I mean, everybody's radiology job might be slightly different. These sorts of things help us get sort of a rough picture of what's going on, what people can do with the technology, and what they're actually doing at work. So, you know, if it doesn't fit perfectly, that's okay. We're just trying to, at a very high level, understand how much of everything is exposed and where might we think changes are going to happen. Well, to hear a radiologist's role broken down into those atomic tasks, it would make me a little concerned to be a radiologist to be going into the field right now. It does seem like many of those are under threat. Even the last one you mentioned, communicating the diagnosis, while yes, I think that's probably something we culturally want humans to do, if we're going to go to some debate night, I could take the other side and build a plausible case for why the machine might be a better deliverer of news. How do we decide how to count these things? Yeah, and there may have been, I think there was even a paper talking about how LLMs might have had better bedside manner. So that's fair. I do have 27 more tasks to throw your way if we want to have that longer debate. I think every, you know, there's a context that comes with most people's work. They make judgments over how to apply tools. And I don't think that's going away. So even if you're, say, we'll pick another one here, establish and enforce radiation protection standards for patients and staff. There's a lot of hidden stuff there. There's enforcement, there's figuring out what knowing what people do wrong. There's rewarding people for effective radiation protection standards, all that stuff. And then there's, of course, the option to take that task out of what radiologists do and assign it to someone else, reconfigure the job. Those things take the longest because eventually you're kind of like, hey, this job doesn't make a ton of sense. Let's combine its tasks with something else. And that's why, you know, Hal Varian taught a room of us this fact. There's only been one job in the post-World War II era that's been fully automated. And that's elevator operator. You know, I brought that up actually for another group. Someone said, well, my building has an elevator operator. So I guess they still do exist in some corners of the world. But for the most part, yeah, jobs don't get automated. They kind of vanish into other jobs and people change what they're doing. Physicians generally have a ton of training. And we found also generally jobs that do require more training are more exposed to OMS. These are people who are used to learning or at least were at some point in their career. I think there's a good opportunity to help them learn more and incorporate the technology into how they do things. Well, how do you measure that exposure? Yeah. So here's the fun part. We built a rubric and the rubric is very simple. The question is, could you use a large language model to double the productivity of a worker in this task without giving up or with no noticeable drop in quality? And the way we ask that is, could you do that in half the time? Could you do this task in half the time of the assistance of an LLM? And there's three possible answers. The first we call our category E0, exposure zero. No, the answer is no. And that shows up about half the time. Then we have another sort of easy answer. Yes, you could. And that shows up somewhat infrequently. Maybe quote me on this number, but something like 10 to 20% of the time, I think, depending on how you do it. And then there's E2, which is our yes, but kind of category. And the but is I would need additional systems around the LLM in order to bring about that kind of productivity gain. Additional software, maybe additional organizational processes and so on. And that's where there's the most Fazziness. There's some, if you repeat this with different, slightly different prompts, we ask humans and GPT-4 to answer these questions. And we modify the prompts a couple of times with GPT-4. You can get slightly different answers. Most of the diffusion between categories or the disagreement is saying that an E1, an exposed thing is actually an E2, you need additional systems or vice versa. Or that something that's not exposed might be exposed with additional systems. But very little is like absolutely not to for sure. Yes. So we took that as a good sign. Humans and GPT-4 tend to agree as well. In some of the runs we had for the exposure metric with GPT-4, the humans were a little bit more optimistic about what it might be able to do. Maybe, I don't know, maybe it's telling us it doesn't want to do those tasks. It's just going to lie. Maybe it's lazy. It doesn't want to work. Yeah. So that seems boring to me. Like give me the cool jobs. I don't want to do those other ones. But yeah, we run that exposure thing. We also built an automation rubric to those results. We'll release those, I think, within the next couple of months. That was a different question. That's just, could you do this with the machine on its own? Well, it begs the next question. Do you still need the humans if you can get the responses from the machine? Yeah, I think we do. It's an important... Yeah, we got to validate. I do think one of the contributions of the paper here is like, yeah, GPT-4 is pretty good, at least as a validation check on human collected data. So I've used crowdsourcing platforms in the past. And one of the big issues you have is like, are there any good answers in this data set? Or is my filtration process to get rid of the lousy answers? Is it working? Do they get the attention checks right? GPT-4, it may give you false answers, but it does so in a very different way than humans do. You can at least very cheaply check, trying to run the exact same thing against GPT-4, what kind of responses you might get. I want to be careful there to say, I don't think we're getting a truth here, but we are getting a qualitative... For sure, this will have its productivity double. There's all sorts of reasons, social, political, and so on, that it might not happen. But we're getting a broad kind of coarse view of where things might change and how pervasive the potential impact might be. Yeah. So I wouldn't overstress what was ranked third and fourth, as if that was some distinction, but directionally correct, what do you see going to the top of the exposure risk scale versus the bottom? Yeah, it's interesting. It's a lot of quantitative knowledge workers that end up near the top, along with people who write that sort of skill work. Alongside data scientists who are highly exposed, blockchain engineers, which is kind of funny that that's like its own category, but here we are. We have data scientists, blockchain engineers, mathematicians who are highly exposed. We've already seen some of these apps or extensions of large language models that help mathematicians write proofs. It's not a thing where you can get rid of the mathematician. They still very much need to be part of that process, but you can make a lot of what they do more efficient. So physicists, lawyers were up there. Poets were in there in the mix. There's actually a meta poet who uses large language models for what she does, and that's really cool to hear about that. And then there's a lot of clerical roles. So, you know, office clerics, people tend to not call themselves that, but that's kind of the job category. Switchboard operators, which, you know, I remain surprised that there continues to be a job. But switchboard operators, there's some really great work by Fiegun Bonggros, a series of papers on AT&T automating the switchboard operator. That job even has transformed, and now it's more customer service oriented rather than, hey, who would you like to call? Let me connect you. That's a job that still exists. So it's fun though, like when you separate out these jobs by their overall exposure versus their automation exposure, two things show up. One is that these are highly correlated. A lot of the same jobs where you've got, you know, overall exposure, you have automation exposure, and I would say our prompt kind of gears towards augmentation. Could you make a person faster in doing this job? Or not this job, I should say, this task. There's plenty of jobs where there's a ton of stuff that you could augment just by looking at it and saying, I don't think I could get rid of the person there, but they'd be faster. But a lot of the time it's the same place. There's a great paper by Otter, Chin, Solomons, and Siegmiller that talks about how historically that's, you know, they use patent data for this. Historically, there's been a positive correlation between augmentation and automation exposure. When you separate out the most exposed to automation roles, they tend to not be jobs like data scientist or physicist. They tend to be more on the clerical or switchboard operations side of things. That's concerning in some ways. I'll put it that way. Well, for someone intensely risk averse, what are the least exposed occupations out there? The least exposed? Yeah, I mean, I still think it's a great... There's a different kind of risk aversion that may block this. But if you'd like to go learn how to be an electrician or a plumber, so long as you're not too scared to get shocked. That kind of skilled trade contractors, those sorts of roles tend to be less exposed overall. But there's places like billing where large-lingual models and related technologies can help. There's a number of physical roles. Now, people pointed out that they're starting to use language models as a component of robotics software as well. And that's helping. There's some argument that we'll see an advanced in robotics software, but that's using an additional system. We haven't really gotten into that too much. That's maybe a next step for our research. And do you think the work is at the point where a high school guidance counselor should be taking these things into consideration as they do a lot of advice? I would say it's a bit premature for that. But what I would be really interested in seeing is the integration of large-language models and related technologies into educational programs. Well, the opportunity for education is very exciting. Do you think we have a vision for that yet, or is it really the Wild West? Some people are starting to put that vision together, and that is... I agree with you. It's super exciting. It's no surprise that when it's hard to innovate or create new productivity advances in a sector, that it grows to be more expensive and harder to surface people in those industries. So let's think of healthcare, housing, education. Those are the things everybody's upset are getting much more expensive and harder to supply. Thinking about customizing education to what you do or don't know as a student with large-language models, having the ability to find similar people struggling with similar problems and pair them with someone who's really good at solving those problems or helping them learn how to solve those problems is a super exciting opportunity. Well, our economy and how we work has evolved a lot over time. My understanding is most people were agricultural workers not all that long ago, and certainly that's kind of upside down now. Is this current LLM revolution unprecedented, or is this yet another historical event that future historians will look at and compare to all the others? I think it rhymes a lot with the IT revolution stuff. I think analogies to history, it's a bit fuzzy to me. Some of them are very helpful and some of them are not. In the big picture, we got really good at making corn or growing other stuff. So roughly 45% of the economy was devoted to agriculture. Now it's down to something like 3%. And that's partially because we have inelastic demand for corn. As much as we can make, if you drop the price by 10%, the demand is not going to expand by 10%. So it shrinks as a proportion of the economy. And we moved into manufacturing for manufacturing, warranty services, and so on, to the point where the things that are the least productive, the most elastic demand, that is, if we drop the price, we get the greatest increase in demand for it. Those grow to dominate the inputs in the economy. IT is a bit unusual because when we drop the cost of it, the demand expanded by more 20 years ago and in the lead up there. So I think if you think about intelligent software in a new way with large language models, we could well be in that kind of sweet spot where we're making these things much, much more productive and then they're expanding as a proportion of demand, at least for the short run. That's what I would be really excited to see. Pulling people into new types of work. I could see education expanding even more as a proportion of the economy because we realized that you can have 10 workers come in and take on a class of a thousand people for very specific kinds of adjustments to their education. And then more people cycle in and out. And there's just this kind of teaching economy that we build where human capital production gets much more efficient. And people are experimenting with new models for that. I was talking with a colleague, Lee Brancetter at CMU. He's doing some work in this area. And you think about how you might be able to help people learn math where if they're making a particular type of mistake, the old pattern of having a teacher in a classroom with 30 people and they're responsible for whatever shows up. That can be sort of thrown out the window. And it's like, let's find an expert in helping people do better at this particular sort of task. I think that's super cool. Not to say that that's Lee's approach necessarily, but that's kind of one way I could see it going. I think Lee's trying a few different things and it sounds like they're working. What do you think about the near-term future? Is something like your work procedure we should repeat? Are these things going to evolve fast? Should we have like an exposure score tracker system over time? That's definitely something I'd like to do. I'd like to build ongoing time-varying measures of exposures of different types. And I'd like to tie that to what's actually happening in the economy. I think that should be part of our statistics apparatus. Who's winning? Who's maybe struggling as a result of technology? But also just generally, which types of jobs are changing the most? Which ones are a little bit more static? I think we didn't talk a ton about the risks of large language models. And I hear something from computer science-oriented colleagues, probably more than economists, that you get cross-pollination on both sides of this. Where the computer scientists are worried about automating away all the jobs and the economists tend to say that's probably not going to happen. With history as a little bit of an anchor point for the economists, there's informational friction going on between people at work and how they pass off processes to other people to pick them up and go do other things. So concretely, I might, with a co-author, do a certain analysis. But then there's 20 different things that they need to understand before they can take the table that I made and write a paragraph in a paper to say this is what we're talking about. And all of us would be on the same page. You multiply that up throughout the economy and there's always a ton more work to do. The social skills might become more important as a result of that. We don't really know. There's this equilibrium outcome where supply and demand, all of this stuff, shifts as a result of the new technology being available. We need to build entirely new systems. And because of that equilibrium change, it's very hard to predict the future. So in a sense, the work that my co-authors and I have done is more to say it's super hard right now to say what's going to happen in the future. It's a little bit like we think it's a sufficiently impactful technology that it could change a lot. Asking computer scientists which jobs are going to go away or not is a little like asking what, after the steam engine was first built to pump water out of mines, what is this going to do to the world economy? It's very hard to say. I mean, to think GPUs are in many ways possible because there were a lot of people playing video games. So the path dependence of this is confusing. So that's the one big thing. And then the other set of risks, I'm much more sympathetic to arguments around, say, the bias of these algorithms, misuse, harmful use, generation of misinformation than I am about existential risks where these things are going to kill all of us. Because the argument that the machine would do that on its own is just less likely to me than that dumber machines with people, working with people could accomplish similar types of awful outcomes on their own. So I think our regulatory policy should be geared towards preventing people with sort of bad intentions for the rest of us from accessing the skill they need to do something damaging. And that's sort of what I see as being the key challenge with mitigating risk for the next 10 to 20 years. There's reasonable people to disagree, but that's kind of where I've come down on it. Well, with regard to risk, do you have any thoughts on what US policy should be? Yeah, like it could be maybe sit back and watch or heavily legislate or anywhere in between. All right. So there's a few categories of things. There's one category where I think we should wait and see, particularly like competition policy. I don't think that we're going to have only a few companies dominate all the market here, necessarily. It's very early to say that. And there's been a big beacon shot up saying, hey, there's value here. So I think we need a little bit of time to evaluate that side of things. On the other hand, there are things where we should definitely regulate. And we don't necessarily need to regulate with AI. I mean, like if you're going to use AI to build a bio weapon, the law shouldn't necessarily say, don't use AI to build bio weapons. The law should say, don't build bio weapons at all. And as far as I know, that is illegal. So this gets to a broader trend, I think, when it comes to AI. As much as it can do for us, we should still make people responsible for the output. So these hallucinations that we see or mistakes that it makes, if you're going to, say, turn in a legal brief with false citations in it, you're still responsible for the output of the model. It's not necessarily going to be true what it gives you. And that's where it's very hard to replace human expertise in those sorts of processes. So I tend to be somewhat optimistic about this, but I don't think the technology traces out the exact path or set of paths that we want it to without some good policy kind of setting guard rails and saying, yeah, please, please don't use this to build bio weapons. We don't need 80 COVID that won't do it. Well, what's next for you in your research? I'm going to certainly be continuing along this economics of AI path. And I'm interested in seeing what those complementary innovations to make AI work, what are the conditions under which AI does or doesn't lead to good outcomes in firms and organizations. I'm hoping to do a little bit more of that applied micro sort of run. Let's see if we can run an experiment and lift the productivity of certain workers. There's been some great work doing stuff like that. But also getting at those other interesting outcomes, like how long does it take people to learn? Can you reduce training costs with these sorts of systems? Can we bring about better integration of technology from other points of view? Like does it make using Git easier? And that has some knock on effects. Like I hate using Git, but I have to use Git. So these sorts of things. And then I'm also really interested in LLMs and generative AI. People talk a lot about the decoder side. Like it generates this text, it generates the image, generates the code. We also need to talk about the encoder side. Like what does it mean to be able to take unstructured data and do a lossy compression of it and have computers operate on that lossy compression to generate entirely new types of software? So those sorts of things I'm thinking about these days. Well, from the economic angle, are you more optimistic or pessimistic when thinking about AI? I'm very much on the optimistic side. I think there's a handful of application developments that by themselves I think would be tremendously impactful and great for us. And we just have to figure out how to bring that about. I don't think the job apocalypse is coming anytime soon, but even if it were, I think there would be ways to manage it effectively. It's all about how much does our social fabric have an integration tolerance? What's that muscle look like? I think the gains of these technologies are too enormous to ignore. So we can't have many of the problems that people have with it, or I think are going to be solvable with some additional engineering. And I don't think, to quote a good friend of mine, James Mylan, we can't have the attitude that we've tried nothing and we're a lot of ideas. A very good line, yeah. Well, Daniel, is there anywhere listeners can follow you online? Yeah, sure. I'm on Twitter. My handle is just danielrock at danielrock. I'm also the co-founder of a generative startup called Work Helix, where we're helping companies to understand their exposure to generative AI and what kind of playbook they might be able to run to deploy it. You can always just drop me an email, I guess. My email is on my website. So if you have questions, shoot them my way. I'm happy to engage in conversation with anyone who's interested. Awesome. We'll have links to all the above in the show notes for people to follow up. Great to talk with you. Have a great day."}}, "transcribed_data": {"podcast_title": "Data Skeptic", "episode_title": "Which Professions Are Threatened by LLMs", "episode_image": "https://ssl-static.libsyn.com/p/assets/0/e/4/b/0e4bd71bb64c6e45/DS_-_New_Logo_assets_-_JL_DS_Logo_Stacked_-_Color_2.jpg", "episode_transcript": " Welcome to Data Skeptic Machine Intelligence, our podcast series exploring contemporary topics in artificial general intelligence and large language models. So jobs. Everybody wants a job, while not everybody. Ideally there'd be a job for everyone who wants one, hopefully a good job. And there's certainly a lot of concern that AI may replace jobs in the near future. I have found that this is a topic that many people are quick to express certainty about, as though they can prognosticate the future. To shape my opinion on the possibilities of future unemployment, I want a data-driven perspective. And that's why I was very happy when I came across the paper, GPTs are general purpose technologies, an early look at the labor market's impact potential for LLMs. Today's interview with Daniel Rock, co-author on this paper, dives deep into exactly which professions would we estimate are going to be impacted by the proliferation of large language models. I'm Daniel Rock. I'm an assistant professor of operations, information, and decisions at the Wharton School of the University of Pennsylvania. And can you share a few details on what falls under that umbrella? Sure, yeah. Well, I mean, it's the usual things. I do a lot of research and teaching. I focus on the economics of AI and ML in my research. But more broadly, I'm interested in how digital technologies are changing the economy. I teach courses. I have an undergraduate core course that I teach in optimization and simulation. And then I teach an MBA course, digital business strategy, and becoming more and more AI as we go on. I also have an advanced undergrad class that sometimes I offer. Has there been much to study historically about the economic impacts of AI and ML, or is this really a recent trend? There's a few waves of AI and ML. If you really go back, there are people talking about good old fashioned AI, where people are writing down rules systems that you can apply. And some of those things actually work. Detecting fraud in a bank or something, sometimes you can use rules around data to do a reasonably good job with that. Yeah, each new wave brings with it a new set of technologies. And AI is a very fuzzy term. And it generally just encompasses people writing code that in some way mimics a human decision process. But I think now we're getting to the point where we might be able to create new types of processes, setting aside the human piece of it. A lot of people are beginning to have a fear that AI could impact the job market. Looking to the future is a little harder. Let's look to the past first. Would you say that AI has in any way historically impacted the job market? I think we're starting to see shoots of where the supervised learning proliferation in the last decade started to affect things. And then if you want to be expansive about it and think about all digital technologies, you can include the internet or just rules-based software or things where we can write a recipe and then say, here's the code, go do this process. Certainly impacted the labor market in major ways. Now the way that it's impacted the labor market is not generally wholesale automation. Obviously unemployment is super low right now. People are still working. We haven't automated everything. The sense in which it has changed the labor market, there's always winners and losers when these new technologies come out. The overall effect tends to be that there are more winners than losers, but you still have to think about where are people going to be displaced and how can we help them. The more winners than losers is an interesting argument. Actually I find it kind of compelling. I would like to at least hope that technology creates jobs as it destroys some or maybe creates three where it destroys two, something along those lines. Is there any scholarship on how to measure something like that? Sure. Well, my first stop for work of that sort is the work of Gerona Zamoglu and David Otter both at MIT, investigating the idea of skill bias, technical change, and then what they've called the canonical model in the past, a task-based model. You can think of a job as a bundle of tasks and you bring your skills and abilities to that bundle of tasks. That bundle is kind of determined by supply and demand. It's what makes sense to put together in the same job and we can look at over time how those bundles have changed. There's a lot of new work creation. So Gerona Zamoglu and Pascual Restrepo, for example, have documented that new task creation is the key addition to the task-based model. We started allocating tasks to machines. Where do the new tasks come from? That's the thing that critically we kind of have to measure going forward, but there's a ton of new work creation over the last few decades. A lot of the typical jobs that you see today, especially in tech, they didn't exist 15, 20 years ago even. That's definitely happening. I like to joke that data scientists as a job will cease to exist. Well, I think a data scientist, I was talking about this with a colleague last week, a data scientist is likely to just refer to an analyst role of some kind. But recent history has been it's an analyst, it's a data engineer, it's a product manager, it's all these things rolled up at once. I kind of think those things are going to get broken apart so that people can specialize again. Yeah, I tend to agree. The title, computer programmer, I don't think is anyone's official professional title anymore, but it's not like that stopped being a job. Exactly. Yeah, we're all still writing code and throwing things in our monitors. The whole large language model revolution, if we were to call it that, is a relatively recent thing. It could very well be that it's going to displace a lot of people in the even short-term future. Maybe there's just a time to roll it out or maybe that's already happened or maybe it's not going to happen. Are there any early indicators? Yeah, large language models are extremely quick on the adoption side, especially for these sort of peripheral things. I think, as a kind of cranky economist, I recoil a little bit at the idea that all of the adoption will be super quick and we'll be living in the future we expect to happen in a super short amount of time. I think these technologies are sufficiently transformative and the potential is sufficiently interesting across so many different domains that even if we stop doing development on LLM today, which by the way, I'm not endorsing as an idea, but if we were to stop, it would still be a decade plus before all of the different applications have been unearthed and distributed. Some quick things, you can have GPT write essays or copy for you, which is a problem for many educators like myself where you have to reconfigure the incentives in your classroom and how you teach. Well, problem for some, it's an opportunity from the perspective of others. There are quick areas where there are quick wins, quick things that can be actually kind of transformative and disruptive in terms of just what's available right now, but we're going to have to do a lot of thinking about how to integrate these systems into our existing ones. Well, there's an idea here a lot of people share, especially in the programming world, saying, oh, these can make everyone a 10x developer. This is sort of like the, what was Dr. Who's thing, the sonic screwdriver. That's essentially what this could be in everyone's hands. Having had your own hands-on experience with large language models, to what degree are you impressed or unimpressed with them? You know, I waver between being like, I guess I keep adjusting my expectations, which I shouldn't do. Like if you had asked me three years ago, would this exist? I would have said probably not. Now I'm like, oh man, it gave me a wrong answer, like a mildly wrong answer that's so upsetting. I've integrated using mostly GPT-4 into my workflow. I have it help me with coding assistance or if I want to describe say a plot, I'm trying to make for a paper or something. I can give it to GPT-4 and it sometimes gives me reasonable suggestions. Recently I've found it's more useful as kind of like an on-demand stack overflow for whatever I want to do and it has about the same fidelity as stack overflow for answering my questions, which means, just to say, I usually can't just copy paste whatever it gave me. I have to reconfigure it in some way. I think we're going to get better there. There's a lot of like rote things that if you ask it to do something relatively simple, it can give you decent code for that very quickly. But if you start getting more complex, then all of a sudden, yeah, you're in trouble. So I think of it really as like a guiding tool and can it make all of us 10x developers? Well, I think the 10x developers, 100x developers, these people who are really out in the tail in terms of skills, they're creative in how they combine the parts and they're creative in terms of the problems that they want to solve and how effective they can be in doing that. I think GPT will make us all that good. Certainly hasn't made me that good. I wish it had, but no, I think I'm still a 1x or less developer when it comes to what I do, which is to say maybe I've been multiplied by 10 in the first place, but I don't want to admit how low I was on the totem pole to begin with. Well, it would be good to have some empirical way with a little bit of methodology to try and assess where the real threat is here. Is it highly skilled workers, low skilled workers? How can we begin to have some sort of a measurement of these processes? Yeah, this is an exciting thing. I don't know about you, but I think there's a somewhat justifiable anxiety about how exposed knowledge work is to these new technologies. So typically, we've looked in the past and we've seen you're going to do the same billing process hundreds of times. You can write a recipe down and say, okay, we're going to code that up. That's going to be finished. But now we're seeing massive advancement in what used to be the domain of very highly paid people. So some co-authors at OpenAI and all, we wrote a paper, it's called GPTs or GPTs, that is Generative Pre-trained Transformers are General Purpose Technologies. And we investigate this. We can talk a little bit more about what we did precisely there, but we were trying to figure out how exposed certain tasks are to AI. When exposed isn't good or bad, it just means how you do that task might change with the help of a large language model. So sorry, I don't mean to interchangeably use AI and large language model, but in this case, we're focusing on large language models. So what we get here is you can aggregate from tasks to jobs. So we have exposures at the task levels and measurements of exposure at the task level. We aggregate those exposure measurements to the job level where you do get a little bit of a diversification benefit. The typical job does like 20 to 30 tasks. And then you can aggregate from jobs to whatever, you know, industries, firms, regions, whatever you have a job level distribution. What we found was that the jobs that are highest paid tend to be more exposed, which is unlike earlier waves of technology. That can be good news in the sense that like we've seen a few applications where the lowest skill workers within a job have been accelerated, kind of like what you were saying. Can we make everybody a 10x developer? Well, some work by Eric Brunelso and Lindsay Raymond and Danielle Lee found that in a call center for customer service agents, you can make people 10x customer service agents with the help of generative AI coaches. So that's inequality mitigating. And that's good in the sense that everybody's getting more productive. You might have concerns that since it's the best people and their practices being used to teach the least productive people and catch them up to the frontier, that you might have like, I call it a bananas problem. We get a monoculture and bananas and then like all those bananas are really vulnerable to some sort of disease. Like kind of similar analogous idea where we like, okay, everybody who does it right or does it really well does it in the same way, but then maybe there's some corner case that they haven't discovered how to do well. So we wouldn't want everybody coding in the same way that might create vulnerabilities. There's like a benefit to people having a wide variety of approaches. Now getting back to the wage distribution, everything I've said so far there is about within jobs, like the top performers to the worst performers. Now we can go to our lawyers and doctors more exposed than factory line workers, low-paid workers like coal miners or like people, short order cooks, things like that where they're not paid as much and they're also not as exposed. We could think about how much are those jobs going to change? And it may be the case that you automate a bunch of this stuff and then it could be like that lawyers just don't get paid as much. But I think far more likely is that we get sort of a scattering where the best lawyers figure out how to be even more productive with all this stuff or maybe as a group lawyers get even better. So it might be inequality increasing in the long run. And I think that's an open question. We won't know for a while. Kind of got to keep your eye on what might happen there. Well, it's intuitive to me to say a short order cook is a low wage worker and an attorney is probably a high wage worker. Could we talk for a moment about the data set upon which though you get some concrete numbers to work with? Yeah, we start with the US government puts out this great data set called ONET. ONET documents all the tasks associated with a set of about a thousand jobs, a little less than a thousand jobs, and what those people do in that job category with other details like you can pull the wages and merge those over to the Bureau of Labor Statistics, occupational employment surveys that's got measurements of how many people are in each job. It's got measurements of the wage distribution, where those people are located and in a course sense. So you can go to MSA level, Metropolitan Statistical Area, or you could go to like the state level. They also have industry breakdowns. We haven't gotten to how we get the scores yet, but assume you've got the scores at each one of these task levels, you can merge those over and then you can get aggregate at whatever level you want. So in terms of breaking down a occupation into the tasks it performs, I wouldn't necessarily expect you to have perfect recall of the exact taxonomy in that data set, but could you give some vague examples of like what is a discrete task? One of the fun ones I like to pick on is radiologists because there are some folks in the CS community predicting that we wouldn't have any radiologists. A few years ago we have increased our demand for radiologists since that prediction, so far ML is not destroying their jobs. But all right, so some things that radiologists might do, looking right at the O-net page for them, they might obtain patients' histories from electronic records, patient interviews, dictated reports, or by communicating with referring clinicians. I guess that's one. Another is perform or interpret the outcomes of diagnostic imaging procedures, including magnetic resonance imaging, computer tomography, CTs, positron emission tomography, PET scanners, nuclear cardiology treadmill studies, mammography, or ultrasound. That one is pretty exposed to standard deep learning, like look at an x-ray and tell me where something might be wrong or anomalous. Obtaining patient histories, that's probably something you could bolt a large language model onto a database and instead of having to use SQL you'd use natural language to do it, which is kind of nice. And then there's communicate examination results or diagnostic information to referring physicians, patients, or families. That's not necessarily the sort of thing that you really want a machine doing, especially if the news ain't great. The way I think about radiologists in particular here, and there's 30 of these tasks, so we talked about three of them, I think of radiologists as data enabled doctors. And from that perspective, it's pretty clear that these new technologies augment what they're doing and make them potentially better at their job. There's a lot of evidence. We have a shout out to Centaur Labs. They're a really interesting company using AI with medical images. A friend of mine, Eric Duhaime, founded the company. They're kind of making this case that these Centaurs, that is a combination of humans and machines, provides better outcomes than either one can on their own. And there's a whole bunch of socio, the technical term, the socio-technical integration of these things. How do we bring the technology into our social systems and have it be effective when people don't necessarily trust algorithms or the patterns of how they trust algorithms or don't are sort of hidden? Then we have to discover them, especially when you've got workers with tons of agency about how they deploy the technology. This is going to take a while, but using data sets like ONET or the BLS to get some of this information, while not perfect, I mean, everybody's radiology job might be slightly different. These sorts of things help us get sort of a rough picture of what's going on, what people can do with the technology, and what they're actually doing at work. So, you know, if it doesn't fit perfectly, that's okay. We're just trying to, at a very high level, understand how much of everything is exposed and where might we think changes are going to happen. Well, to hear a radiologist's role broken down into those atomic tasks, it would make me a little concerned to be a radiologist to be going into the field right now. It does seem like many of those are under threat. Even the last one you mentioned, communicating the diagnosis, while yes, I think that's probably something we culturally want humans to do, if we're going to go to some debate night, I could take the other side and build a plausible case for why the machine might be a better deliverer of news. How do we decide how to count these things? Yeah, and there may have been, I think there was even a paper talking about how LLMs might have had better bedside manner. So that's fair. I do have 27 more tasks to throw your way if we want to have that longer debate. I think every, you know, there's a context that comes with most people's work. They make judgments over how to apply tools. And I don't think that's going away. So even if you're, say, we'll pick another one here, establish and enforce radiation protection standards for patients and staff. There's a lot of hidden stuff there. There's enforcement, there's figuring out what knowing what people do wrong. There's rewarding people for effective radiation protection standards, all that stuff. And then there's, of course, the option to take that task out of what radiologists do and assign it to someone else, reconfigure the job. Those things take the longest because eventually you're kind of like, hey, this job doesn't make a ton of sense. Let's combine its tasks with something else. And that's why, you know, Hal Varian taught a room of us this fact. There's only been one job in the post-World War II era that's been fully automated. And that's elevator operator. You know, I brought that up actually for another group. Someone said, well, my building has an elevator operator. So I guess they still do exist in some corners of the world. But for the most part, yeah, jobs don't get automated. They kind of vanish into other jobs and people change what they're doing. Physicians generally have a ton of training. And we found also generally jobs that do require more training are more exposed to OMS. These are people who are used to learning or at least were at some point in their career. I think there's a good opportunity to help them learn more and incorporate the technology into how they do things. Well, how do you measure that exposure? Yeah. So here's the fun part. We built a rubric and the rubric is very simple. The question is, could you use a large language model to double the productivity of a worker in this task without giving up or with no noticeable drop in quality? And the way we ask that is, could you do that in half the time? Could you do this task in half the time of the assistance of an LLM? And there's three possible answers. The first we call our category E0, exposure zero. No, the answer is no. And that shows up about half the time. Then we have another sort of easy answer. Yes, you could. And that shows up somewhat infrequently. Maybe quote me on this number, but something like 10 to 20% of the time, I think, depending on how you do it. And then there's E2, which is our yes, but kind of category. And the but is I would need additional systems around the LLM in order to bring about that kind of productivity gain. Additional software, maybe additional organizational processes and so on. And that's where there's the most Fazziness. There's some, if you repeat this with different, slightly different prompts, we ask humans and GPT-4 to answer these questions. And we modify the prompts a couple of times with GPT-4. You can get slightly different answers. Most of the diffusion between categories or the disagreement is saying that an E1, an exposed thing is actually an E2, you need additional systems or vice versa. Or that something that's not exposed might be exposed with additional systems. But very little is like absolutely not to for sure. Yes. So we took that as a good sign. Humans and GPT-4 tend to agree as well. In some of the runs we had for the exposure metric with GPT-4, the humans were a little bit more optimistic about what it might be able to do. Maybe, I don't know, maybe it's telling us it doesn't want to do those tasks. It's just going to lie. Maybe it's lazy. It doesn't want to work. Yeah. So that seems boring to me. Like give me the cool jobs. I don't want to do those other ones. But yeah, we run that exposure thing. We also built an automation rubric to those results. We'll release those, I think, within the next couple of months. That was a different question. That's just, could you do this with the machine on its own? Well, it begs the next question. Do you still need the humans if you can get the responses from the machine? Yeah, I think we do. It's an important... Yeah, we got to validate. I do think one of the contributions of the paper here is like, yeah, GPT-4 is pretty good, at least as a validation check on human collected data. So I've used crowdsourcing platforms in the past. And one of the big issues you have is like, are there any good answers in this data set? Or is my filtration process to get rid of the lousy answers? Is it working? Do they get the attention checks right? GPT-4, it may give you false answers, but it does so in a very different way than humans do. You can at least very cheaply check, trying to run the exact same thing against GPT-4, what kind of responses you might get. I want to be careful there to say, I don't think we're getting a truth here, but we are getting a qualitative... For sure, this will have its productivity double. There's all sorts of reasons, social, political, and so on, that it might not happen. But we're getting a broad kind of coarse view of where things might change and how pervasive the potential impact might be. Yeah. So I wouldn't overstress what was ranked third and fourth, as if that was some distinction, but directionally correct, what do you see going to the top of the exposure risk scale versus the bottom? Yeah, it's interesting. It's a lot of quantitative knowledge workers that end up near the top, along with people who write that sort of skill work. Alongside data scientists who are highly exposed, blockchain engineers, which is kind of funny that that's like its own category, but here we are. We have data scientists, blockchain engineers, mathematicians who are highly exposed. We've already seen some of these apps or extensions of large language models that help mathematicians write proofs. It's not a thing where you can get rid of the mathematician. They still very much need to be part of that process, but you can make a lot of what they do more efficient. So physicists, lawyers were up there. Poets were in there in the mix. There's actually a meta poet who uses large language models for what she does, and that's really cool to hear about that. And then there's a lot of clerical roles. So, you know, office clerics, people tend to not call themselves that, but that's kind of the job category. Switchboard operators, which, you know, I remain surprised that there continues to be a job. But switchboard operators, there's some really great work by Fiegun Bonggros, a series of papers on AT&T automating the switchboard operator. That job even has transformed, and now it's more customer service oriented rather than, hey, who would you like to call? Let me connect you. That's a job that still exists. So it's fun though, like when you separate out these jobs by their overall exposure versus their automation exposure, two things show up. One is that these are highly correlated. A lot of the same jobs where you've got, you know, overall exposure, you have automation exposure, and I would say our prompt kind of gears towards augmentation. Could you make a person faster in doing this job? Or not this job, I should say, this task. There's plenty of jobs where there's a ton of stuff that you could augment just by looking at it and saying, I don't think I could get rid of the person there, but they'd be faster. But a lot of the time it's the same place. There's a great paper by Otter, Chin, Solomons, and Siegmiller that talks about how historically that's, you know, they use patent data for this. Historically, there's been a positive correlation between augmentation and automation exposure. When you separate out the most exposed to automation roles, they tend to not be jobs like data scientist or physicist. They tend to be more on the clerical or switchboard operations side of things. That's concerning in some ways. I'll put it that way. Well, for someone intensely risk averse, what are the least exposed occupations out there? The least exposed? Yeah, I mean, I still think it's a great... There's a different kind of risk aversion that may block this. But if you'd like to go learn how to be an electrician or a plumber, so long as you're not too scared to get shocked. That kind of skilled trade contractors, those sorts of roles tend to be less exposed overall. But there's places like billing where large-lingual models and related technologies can help. There's a number of physical roles. Now, people pointed out that they're starting to use language models as a component of robotics software as well. And that's helping. There's some argument that we'll see an advanced in robotics software, but that's using an additional system. We haven't really gotten into that too much. That's maybe a next step for our research. And do you think the work is at the point where a high school guidance counselor should be taking these things into consideration as they do a lot of advice? I would say it's a bit premature for that. But what I would be really interested in seeing is the integration of large-language models and related technologies into educational programs. Well, the opportunity for education is very exciting. Do you think we have a vision for that yet, or is it really the Wild West? Some people are starting to put that vision together, and that is... I agree with you. It's super exciting. It's no surprise that when it's hard to innovate or create new productivity advances in a sector, that it grows to be more expensive and harder to surface people in those industries. So let's think of healthcare, housing, education. Those are the things everybody's upset are getting much more expensive and harder to supply. Thinking about customizing education to what you do or don't know as a student with large-language models, having the ability to find similar people struggling with similar problems and pair them with someone who's really good at solving those problems or helping them learn how to solve those problems is a super exciting opportunity. Well, our economy and how we work has evolved a lot over time. My understanding is most people were agricultural workers not all that long ago, and certainly that's kind of upside down now. Is this current LLM revolution unprecedented, or is this yet another historical event that future historians will look at and compare to all the others? I think it rhymes a lot with the IT revolution stuff. I think analogies to history, it's a bit fuzzy to me. Some of them are very helpful and some of them are not. In the big picture, we got really good at making corn or growing other stuff. So roughly 45% of the economy was devoted to agriculture. Now it's down to something like 3%. And that's partially because we have inelastic demand for corn. As much as we can make, if you drop the price by 10%, the demand is not going to expand by 10%. So it shrinks as a proportion of the economy. And we moved into manufacturing for manufacturing, warranty services, and so on, to the point where the things that are the least productive, the most elastic demand, that is, if we drop the price, we get the greatest increase in demand for it. Those grow to dominate the inputs in the economy. IT is a bit unusual because when we drop the cost of it, the demand expanded by more 20 years ago and in the lead up there. So I think if you think about intelligent software in a new way with large language models, we could well be in that kind of sweet spot where we're making these things much, much more productive and then they're expanding as a proportion of demand, at least for the short run. That's what I would be really excited to see. Pulling people into new types of work. I could see education expanding even more as a proportion of the economy because we realized that you can have 10 workers come in and take on a class of a thousand people for very specific kinds of adjustments to their education. And then more people cycle in and out. And there's just this kind of teaching economy that we build where human capital production gets much more efficient. And people are experimenting with new models for that. I was talking with a colleague, Lee Brancetter at CMU. He's doing some work in this area. And you think about how you might be able to help people learn math where if they're making a particular type of mistake, the old pattern of having a teacher in a classroom with 30 people and they're responsible for whatever shows up. That can be sort of thrown out the window. And it's like, let's find an expert in helping people do better at this particular sort of task. I think that's super cool. Not to say that that's Lee's approach necessarily, but that's kind of one way I could see it going. I think Lee's trying a few different things and it sounds like they're working. What do you think about the near-term future? Is something like your work procedure we should repeat? Are these things going to evolve fast? Should we have like an exposure score tracker system over time? That's definitely something I'd like to do. I'd like to build ongoing time-varying measures of exposures of different types. And I'd like to tie that to what's actually happening in the economy. I think that should be part of our statistics apparatus. Who's winning? Who's maybe struggling as a result of technology? But also just generally, which types of jobs are changing the most? Which ones are a little bit more static? I think we didn't talk a ton about the risks of large language models. And I hear something from computer science-oriented colleagues, probably more than economists, that you get cross-pollination on both sides of this. Where the computer scientists are worried about automating away all the jobs and the economists tend to say that's probably not going to happen. With history as a little bit of an anchor point for the economists, there's informational friction going on between people at work and how they pass off processes to other people to pick them up and go do other things. So concretely, I might, with a co-author, do a certain analysis. But then there's 20 different things that they need to understand before they can take the table that I made and write a paragraph in a paper to say this is what we're talking about. And all of us would be on the same page. You multiply that up throughout the economy and there's always a ton more work to do. The social skills might become more important as a result of that. We don't really know. There's this equilibrium outcome where supply and demand, all of this stuff, shifts as a result of the new technology being available. We need to build entirely new systems. And because of that equilibrium change, it's very hard to predict the future. So in a sense, the work that my co-authors and I have done is more to say it's super hard right now to say what's going to happen in the future. It's a little bit like we think it's a sufficiently impactful technology that it could change a lot. Asking computer scientists which jobs are going to go away or not is a little like asking what, after the steam engine was first built to pump water out of mines, what is this going to do to the world economy? It's very hard to say. I mean, to think GPUs are in many ways possible because there were a lot of people playing video games. So the path dependence of this is confusing. So that's the one big thing. And then the other set of risks, I'm much more sympathetic to arguments around, say, the bias of these algorithms, misuse, harmful use, generation of misinformation than I am about existential risks where these things are going to kill all of us. Because the argument that the machine would do that on its own is just less likely to me than that dumber machines with people, working with people could accomplish similar types of awful outcomes on their own. So I think our regulatory policy should be geared towards preventing people with sort of bad intentions for the rest of us from accessing the skill they need to do something damaging. And that's sort of what I see as being the key challenge with mitigating risk for the next 10 to 20 years. There's reasonable people to disagree, but that's kind of where I've come down on it. Well, with regard to risk, do you have any thoughts on what US policy should be? Yeah, like it could be maybe sit back and watch or heavily legislate or anywhere in between. All right. So there's a few categories of things. There's one category where I think we should wait and see, particularly like competition policy. I don't think that we're going to have only a few companies dominate all the market here, necessarily. It's very early to say that. And there's been a big beacon shot up saying, hey, there's value here. So I think we need a little bit of time to evaluate that side of things. On the other hand, there are things where we should definitely regulate. And we don't necessarily need to regulate with AI. I mean, like if you're going to use AI to build a bio weapon, the law shouldn't necessarily say, don't use AI to build bio weapons. The law should say, don't build bio weapons at all. And as far as I know, that is illegal. So this gets to a broader trend, I think, when it comes to AI. As much as it can do for us, we should still make people responsible for the output. So these hallucinations that we see or mistakes that it makes, if you're going to, say, turn in a legal brief with false citations in it, you're still responsible for the output of the model. It's not necessarily going to be true what it gives you. And that's where it's very hard to replace human expertise in those sorts of processes. So I tend to be somewhat optimistic about this, but I don't think the technology traces out the exact path or set of paths that we want it to without some good policy kind of setting guard rails and saying, yeah, please, please don't use this to build bio weapons. We don't need 80 COVID that won't do it. Well, what's next for you in your research? I'm going to certainly be continuing along this economics of AI path. And I'm interested in seeing what those complementary innovations to make AI work, what are the conditions under which AI does or doesn't lead to good outcomes in firms and organizations. I'm hoping to do a little bit more of that applied micro sort of run. Let's see if we can run an experiment and lift the productivity of certain workers. There's been some great work doing stuff like that. But also getting at those other interesting outcomes, like how long does it take people to learn? Can you reduce training costs with these sorts of systems? Can we bring about better integration of technology from other points of view? Like does it make using Git easier? And that has some knock on effects. Like I hate using Git, but I have to use Git. So these sorts of things. And then I'm also really interested in LLMs and generative AI. People talk a lot about the decoder side. Like it generates this text, it generates the image, generates the code. We also need to talk about the encoder side. Like what does it mean to be able to take unstructured data and do a lossy compression of it and have computers operate on that lossy compression to generate entirely new types of software? So those sorts of things I'm thinking about these days. Well, from the economic angle, are you more optimistic or pessimistic when thinking about AI? I'm very much on the optimistic side. I think there's a handful of application developments that by themselves I think would be tremendously impactful and great for us. And we just have to figure out how to bring that about. I don't think the job apocalypse is coming anytime soon, but even if it were, I think there would be ways to manage it effectively. It's all about how much does our social fabric have an integration tolerance? What's that muscle look like? I think the gains of these technologies are too enormous to ignore. So we can't have many of the problems that people have with it, or I think are going to be solvable with some additional engineering. And I don't think, to quote a good friend of mine, James Mylan, we can't have the attitude that we've tried nothing and we're a lot of ideas. A very good line, yeah. Well, Daniel, is there anywhere listeners can follow you online? Yeah, sure. I'm on Twitter. My handle is just danielrock at danielrock. I'm also the co-founder of a generative startup called Work Helix, where we're helping companies to understand their exposure to generative AI and what kind of playbook they might be able to run to deploy it. You can always just drop me an email, I guess. My email is on my website. So if you have questions, shoot them my way. I'm happy to engage in conversation with anyone who's interested. Awesome. We'll have links to all the above in the show notes for people to follow up. Great to talk with you. Have a great day."}, "podcast_summary": "- TL;DR: In this podcast episode, Daniel Rock, an assistant professor at the Wharton School of the University of Pennsylvania, discusses the impact of large language models (LLMs) on the labor market.\n- LLMs are becoming increasingly pervasive and can potentially double workers' productivity in certain tasks without a noticeable drop in quality.\n- Some professions that are highly exposed to LLMs include data scientists, blockchain engineers, mathematicians, switchboard operators, and clerical roles.\n- Highly skilled jobs, such as those in the healthcare and legal fields, are more likely to be affected by LLMs compared to low-skilled jobs.\n- The integration of LLMs into education holds promising opportunities for personalized learning and increased efficiency in teaching.\n- The future impact of LLMs on the job market is uncertain, but it is important to have policies in place to address potential risks such as bias, misuse, and misinformation.\n- Daniel Rock is optimistic about the potential benefits of LLMs and believes they can bring about significant advancements, but careful consideration and regulation are necessary to ensure positive outcomes.", "podcast_guest": "Daniel Rock", "podcast_highlights": "Chapter 1: Introduction - The Impact of AI on Jobs\nChapter 2: The Historical Impact of AI and ML on the Job Market\nChapter 3: Measuring the Impact of AI on the Labor Market\nChapter 4: The Exposure of Different Professions to Large Language Models\nChapter 5: The Potential of Large Language Models to Augment Jobs\nChapter 6: The Risks and Challenges of Large Language Models\nChapter 7: Potential Policy Solutions and Regulations for AI\nChapter 8: The Optimistic Outlook for AI and the Future of Work"}