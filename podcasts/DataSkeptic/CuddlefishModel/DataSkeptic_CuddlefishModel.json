{"detailed_guest_info": "Wang Hongyi was a secret police official during the Tang Dynasty and Wu Zetian's Zhou Dynasty. He gained a reputation for falsely reporting crimes, resulting in the deaths of innocent people. Despite his actions, he was given titles and positions of power by Empress Dowager Wu, but eventually faced accusations of corruption and was exiled, where he met his demise after falsely claiming to be recalled by edict.", "podcast_details": {"podcast_details": {"links": [{"href": "https://dataskeptic.libsyn.com/rss", "rel": "self", "type": "application/rss+xml"}, {"rel": "alternate", "type": "text/html", "href": "https://dataskeptic.com"}], "title": "Data Skeptic", "title_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Data Skeptic"}, "published": "Mon, 21 Aug 2023 14:38:00 +0000", "published_parsed": [2023, 8, 21, 14, 38, 0, 0, 233, 0], "updated": "Mon, 21 Aug 2023 14:47:12 +0000", "updated_parsed": [2023, 8, 21, 14, 47, 12, 0, 233, 0], "generator_detail": {"name": "Libsyn WebEngine 2.0"}, "generator": "Libsyn WebEngine 2.0", "link": "https://dataskeptic.com", "language": "en", "rights": "Creative Commons Attribution License 3.0", "rights_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Creative Commons Attribution License 3.0"}, "docs": "https://dataskeptic.com", "authors": [{"email": "kyle@dataskeptic.com"}, {"name": "Kyle Polich", "email": "kyle@dataskeptic.com"}], "author": "Kyle Polich", "author_detail": {"email": "kyle@dataskeptic.com"}, "summary": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches.", "summary_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches."}, "image": {"href": "https://ssl-static.libsyn.com/p/assets/0/e/4/b/0e4bd71bb64c6e45/DS_-_New_Logo_assets_-_JL_DS_Logo_Stacked_-_Color_2.jpg"}, "tags": [{"term": "Skepticism", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Statistics", "scheme": "http://www.itunes.com/", "label": null}, {"term": "datamining", "scheme": "http://www.itunes.com/", "label": null}, {"term": "datascience", "scheme": "http://www.itunes.com/", "label": null}, {"term": "machinelearning", "scheme": "http://www.itunes.com/", "label": null}, {"term": "science", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Technology", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Science", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Mathematics", "scheme": "http://www.itunes.com/", "label": null}], "itunes_explicit": null, "publisher_detail": {"name": "Kyle Polich", "email": "kyle@dataskeptic.com"}, "content": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches.", "content_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "The Data Skeptic Podcast features interviews and discussion of topics related to data science, statistics, machine learning, artificial intelligence and the like, all from the perspective of applying critical thinking and the scientific method to evaluate the veracity of claims and efficacy of approaches."}, "itunes_type": "episodic", "podcast_locked": {"owner": "kyle@dataskeptic.com"}}, "first_episode": {"title": "Cuddlefish Model Tuning", "title_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Cuddlefish Model Tuning"}, "itunes_title": "Cuddlefish Model Tuning", "published": "Mon, 21 Aug 2023 14:38:00 +0000", "published_parsed": [2023, 8, 21, 14, 38, 0, 0, 233, 0], "id": "455d9731-ed3f-47c1-9afd-772d76cb3efd", "guidislink": false, "links": [{"rel": "alternate", "type": "text/html", "href": "https://dataskeptic.com/blog/episodes/2023/cuddlefish-model-tuning"}, {"length": "36923750", "type": "audio/mpeg", "href": "https://traffic.libsyn.com/secure/dataskeptic/cuddlefish-model-tuning.mp3?dest-id=201630", "rel": "enclosure"}], "link": "https://dataskeptic.com/blog/episodes/2023/cuddlefish-model-tuning", "image": {"href": "https://ssl-static.libsyn.com/p/assets/6/1/8/9/618938d3664c499ae55e3c100dce7605/Hongyi_Wang_1.png"}, "summary": "<p>Hongyi Wang, a Senior Researcher at the Machine Learning Department at Carnegie Mellon University, joins us. His research is in the intersection of systems and machine learning. He discussed his research paper, Cuttlefish: Low-Rank Model Training without All the Tuning, on today\u2019s show.</p> <p>Hogyi started by sharing his thoughts on whether developers need to learn how to fine-tune models. He then spoke about the need to optimize the training of ML models, especially as these models grow bigger. He discussed how data centers have the hardware to train these large models but not the community. He then spoke about the Low-Rank Adaptation (LoRa) technique and where it is used.</p> <p>Hongyi discussed the Cuttlefish model and how it edges LoRa. He shared the use cases of Cattlefish and who should use it. Rounding up, he gave his advice on how people can get into the machine learning field. He also shared his future research ideas.</p>", "summary_detail": {"type": "text/html", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "<p>Hongyi Wang, a Senior Researcher at the Machine Learning Department at Carnegie Mellon University, joins us. His research is in the intersection of systems and machine learning. He discussed his research paper, Cuttlefish: Low-Rank Model Training without All the Tuning, on today\u2019s show.</p> <p>Hogyi started by sharing his thoughts on whether developers need to learn how to fine-tune models. He then spoke about the need to optimize the training of ML models, especially as these models grow bigger. He discussed how data centers have the hardware to train these large models but not the community. He then spoke about the Low-Rank Adaptation (LoRa) technique and where it is used.</p> <p>Hongyi discussed the Cuttlefish model and how it edges LoRa. He shared the use cases of Cattlefish and who should use it. Rounding up, he gave his advice on how people can get into the machine learning field. He also shared his future research ideas.</p>"}, "content": [{"type": "text/html", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "<p>Hongyi Wang, a Senior Researcher at the Machine Learning Department at Carnegie Mellon University, joins us. His research is in the intersection of systems and machine learning. He discussed his research paper, Cuttlefish: Low-Rank Model Training without All the Tuning, on today\u2019s show.</p> <p>Hogyi started by sharing his thoughts on whether developers need to learn how to fine-tune models. He then spoke about the need to optimize the training of ML models, especially as these models grow bigger. He discussed how data centers have the hardware to train these large models but not the community. He then spoke about the Low-Rank Adaptation (LoRa) technique and where it is used.</p> <p>Hongyi discussed the Cuttlefish model and how it edges LoRa. He shared the use cases of Cattlefish and who should use it. Rounding up, he gave his advice on how people can get into the machine learning field. He also shared his future research ideas.</p>"}], "itunes_duration": "27:08", "itunes_explicit": null, "subtitle": "Hongyi Wang, a Senior Researcher at the Machine Learning Department at Carnegie Mellon University, joins us. His research is in the intersection of systems and machine learning. He discussed his research paper, Cuttlefish: Low-Rank Model Training...", "subtitle_detail": {"type": "text/plain", "language": null, "base": "https://dataskeptic.libsyn.com/rss", "value": "Hongyi Wang, a Senior Researcher at the Machine Learning Department at Carnegie Mellon University, joins us. His research is in the intersection of systems and machine learning. He discussed his research paper, Cuttlefish: Low-Rank Model Training..."}, "itunes_episodetype": "full"}, "transcribed_data": {"podcast_title": "Data Skeptic", "episode_title": "Cuddlefish Model Tuning", "episode_image": "https://ssl-static.libsyn.com/p/assets/0/e/4/b/0e4bd71bb64c6e45/DS_-_New_Logo_assets_-_JL_DS_Logo_Stacked_-_Color_2.jpg", "episode_transcript": " Welcome to data skeptic machine intelligence, our podcast series exploring contemporary topics in artificial general intelligence and large language models. Do all of you know about the grandmother neuron paradox? I'm not sure if it's called exactly that, but the idea that, well, the fallacy that maybe a surgeon could open up your brain, take a real precision tweezers, grab a single neuron and yank it out of your brain. And as long as that surgeon picked exactly the right neuron, you would completely forget your grandmother. Which one? I'm not sure. You ought to have two, right? Maybe more. But this nonsensical idea demonstrates for us that the brain is a little bit more holographic. Information is distributed across it. And I think it's worth anthropomorphizing the grandmother neuron idea to large language models. Take any deep learning model in fact, and remove one single parameter. I mean, heck, this is what dropout does. That neural network should be virtually unchanged by one small perturbation in its design. So surely they can stand to lose a few parameters. How many exactly? I guess that depends on what sort of trade-off you're willing to have. At present, my assessment is that this is a true practitioner's art, at least in the state of the art as it is in 2023. There's a lot of discussion about LoRa, low rank approximation, but even that feels pretty low level requiring a lot of wizardry on a part of the person deploying it. That's why I was totally intrigued when I came across the paper we're going to discuss in today's interview. That paper is Cuttlefish, low rank model training without all the tuning. You know, sometimes when I look at deep learning models and I consider all the tuning, it looks like a Moog synthesizer with dozens of knobs. Maybe Wendy Carlos knows what to do with them. But I have no idea how to fine tune all those oscillators and whatnot. Could I get just one knob? Maybe that's what Cuttlefish can give me. Let's find out. My name is Hongyi Wang. I'm currently with the machine learning department at Carnegie Mellon University. I'm a senior researcher there. I previously got my PhD degree from the computer science department at the University of Wisconsin-Madison. So my research is mainly about the intersection of system and machine learning. Now it's called MLC, fancy name. They're kind of like optimize the system to run your machine learning code or model or algorithm better on your hardware. So basically that's what I'm doing. Can you tell me a little bit about the focus of your research? I mostly study distributed machine learning. So basically parallel computing. Currently as the model becomes bigger and data also becomes bigger, so one GPU seems not enough to train our model or even do the so-called fine tuning these days. So my research is about how you can use more, utilize more CPU cores or more GPUs to accelerate your computing during training or model fine tuning. But there the problem we care about is like when you use 1000 GPUs together, how can you get 1000 times faster? So that's what we call scaling problem. Now we are kind of doing kind of well for the smaller model. Let's say if you want to train a residual network or convolution network, it seems to be fine. We solve the scaling problem okay. But for large language models, sometimes we need to go beyond what we call data parallelism. That makes the problem harder. So we are working hard on that recently. So I have written a fair amount of Java code in my professional career and on one or two occasions had a collaborator who knew about the underlying architecture of the JVM much better than I did and was able to go do some very fine tuned optimizations about the way garbage collection work to make everything just function a lot better. I don't have that skill set and I kind of don't want it. I'd rather just sort of do what I know there and let someone else handle that. But then I try and carry that analogy over to deep learning where I am aware of things like batch normalization and the vanishing gradient and concepts like this or even the transformer architecture and how it works. Do you think it's necessary that people who use deep learning know the fundamentals and the methods in that way or are we maybe entering a world where someone could just engineer on top of that technology? Right. I think that's pretty kind of like a visionary problem and we're actually working very hard on that. So as I just mentioned, if I do not have a system like knowledge, I just work on fundamental science, let's say, and I want to use the foundation model or kind of like deep learning to kind of like power my research. It's basically kind of like impossible in the past. But now we can kind of like have PyTorch, like TensorFlow, it seems to become easier, but still very hard. Let's say if I were even given, let's say, like 1000 GPU from Microsoft, I don't even know how to use the 1000 GPU together. So that's the problem. We are kind of like solving, kind of like come up with another layer in between from application people and engineer and also the hardware. So that's what we are doing. Hopefully we can automatically deploy your code on the hardware in the optimized way or in kind of like an optimal way. Yeah, I know it won't be the focus of our main discussion today for the paper, Cuttlefish, that I invite you to talk about. But before we get into that, could you just reference a little bit about the hardware? Is that like FPGAs or what are you thinking there? Actually, yeah, I know the world is currently GPU dominated or TPU dominated, but we're actually thinking about a wider picture. So there are a whole bunch of like used hardware everywhere, right? Even in the AWS cloud, there are data centers, there are a lot of like used GPUs, also the community hardware. So one of my research goals is to try to combine everything together, kind of like reuse the machine, everything, your CPU cores. Then there, like if you kind of calculate the theoretical flaws, that's actually comparable if we use as much, you know, those used hardware as possible. It's comparable to the A100. But the problem is there is how you can use them efficiently together. So that's what some of my past work and my future work will try to tackle. Well, you know, theoretically you can solve this, right? You can build a very, very fancy cost model and you solve how you can parallelize your computation across all the hardware. It's basically a graph partitioning problem, but theoretically we can solve that. It's kind of like the engineering problem. How can we like scheduling things well? How can we use them cleverly? And how can we even handle the fault tolerance? So like when we have a whole bunch of hardware together, it's very likely like some of them will fail and it seems like that. Yeah, that's definitely possible. And I'm devoting my time to work on that. Well, if we look back in recent history, it's not like NLP has changed overnight. I mean, maybe it did in some ways when some of these amazing models came out. But we can look back to like Word2vec as a pretty impressive technology that's, I guess, a decade old. At what point or was there a point when you first took notice and said, wow, something's happening here? Mostly, when I joined this community, it's mostly there's no NLP scaling problem yet, I would say. So at that time, people are excited about like VGG, how you can scale VGG on ImageNet training. So that already becomes a problem because the ImageNet is huge. And also, there's a whole bunch of images there. Currently, if I remember correctly, it's kind of like one million images inside of the dataset. So even kind of like one epoch, now it's called epoch of training, it takes a whole bunch of time. So at that time, we were trying to optimize things like how can we make the model go pass through the data passes as many as possible and as fast as possible. So I would say, you know, like after ImageNet challenging the benchmark, things start to become different. Like all of the people are starting to care about how to make things faster. And how can we optimize all the software stack to try to keep the top of the benchmark as much as possible. And now we have some other benchmark, you know, kind of like people are hitting the, let's say, VKUNA leaderboard or Puggingface leaderboard. It seems to be the newer version of ImageNet in this current era. So I've heard some reports about the cost of running some of these large language models. Maybe it costs one penny for a thousand requests of chat GPT or something like that. And it sounds so low when you describe it as a commodity. But actually, in some sense, maybe it's expensive because we're doing things inefficiently or something along those lines. Where are the opportunities to make efficiency gains here? Right. So kind of in terms of the data center computing, I think it's if we can, everything comes to the most advanced GPU. I think that has been optimized quite well, actually. So in the sense that people may not even want to use, there's a whole bunch of different parallelism. So we have data parallelism model and also the pipeline parallelism. So different of them. And the people have realized that when you can use them together, you can reach to some sort of optimal performance. But now in the data center setting, I don't think people want to even use that. They just use the 40 shard data parallel released by it's also called Deep Speed Zero Stage 3 or whatever. They're just using that. It's because the hardware is highly optimized in the data center. The kind of cross node bandwidth is high enough. So it's fine. So everything is optimized quite well. But when we consider if we want to scale things out of the data center, we want to combine all the reuse hardware or the community hardware. So that's different. So the communication can be really the bottleneck there. So we consider there are two clusters located on different, one on the East Coast, one on the West Coast. So even one communication can take quite a long time. So how can we do that? How can we reduce the communication among there and try to do less communication as much as possible, focus more on local computing? Yeah, that's I think where the opportunity is. Well, there's a popular technique I've heard called LoRa, the low rank approximation, where I guess my, you can correct me if I'm wrong, but my understanding of it is take the big model that's been trained in some expensive and maybe inefficient way and then find a way to compress it or reduce it or something like that. Yeah. Is that fair? And is that a good strategy? Right. I think it really depends. So my personal experience is that if we are really GPU or computing power constraint, say if we can't even train the 7 billion model, so LoRa is a perfect technology to democratize the model to a whole bunch of people. Right. So currently I think LoRa, the LoRa software stack and also it's Varon, let's say the QLoRa is optimized really well to basically just run our MacBook. So that's perfect. I think a lot of amazing applications have been built on top of that to generate their own images, things like that, StableDiffusion. For some more serious, let's say accuracy driven application, let's say I myself is building a medical chatbot right now and I'm just doing some like a pre-training, plus fine tuning on some medical publications. Let's say there's a kind of like something called PubMed. There, I think if we have enough or kind of like decent amount of computing resources, LoRa seems to lead to some accuracy drops there. So let's say if you tune a 65 billion model using LoRa, it won't perform as good as we find full fine tune with a 30 billion model. So I think it really depends on applications. But I personally think LoRa is a very amazing work for democratizing the powerfulness of the large language model to a wide amount of users. Well, how would you contrast LoRa to Cuttlefish? And maybe also can you at the same time introduce Cuttlefish? Oh, yeah, definitely. So Cuttlefish is actually not even the first attempt for this line of work. Discuss with the LoRa team. We have had a few official meetings, actually. So Cuttlefish is something like...so we just treat the model as a model is just too huge. So at the beginning, we are thinking about how can we make kind of like make things a little bit easier. Turns out we can just treat a low rank model. So low rank is basically it's kind of like basically linear algebra. So our model is basically a whole bunch of...we can understand that the whole bunch of matrix concatenating together and we are kind of computing one matrix multiplication by one matrix multiplication. So low rank approximation is basically we approximate the matrix multiplication using fewer number of flaws. And also we can also make the matrix a little bit smaller, such that the computation can be a bit faster. We also save a little bit of GPU memory and also communication become faster because we are essentially training a smaller model. That's basically the basic technology behind Cuttlefish. And actually, at the beginning, we published a paper in 2021 called Pufferfish. So that's very initial work. It's just doing this approximation and training from scratch. And turns out when you train the low rank model, it doesn't work as well as the full rank model because we kind of like to reduce a lot of model capacity. It doesn't generalize well. Then we kind of like introduce several technologies there. Some of the layers, it's hard to approximate, right? If you kind of like approximate that, it will cause a huge amount of accuracy drop. For some of the layers, it's fine. So that's the first technology. We're just selectively factorizing some of the layers in the neural network. A second technology is that you may not want to go low rank in the very beginning. So it's mostly like the sparse training, right? So in the beginning, you want to do this and you gradually make your model become sparse. In the full rank to low rank training, we're also doing things like that. In the beginning, we are doing full rank model and then we gradually start to convert from full rank to low rank. So that's called PowerFish published in 2021. Cardiffish took that a step further. It's basically, so when we switch from full rank to low rank and which layers we don't want to factorize, so those are a lot of like heavy hyperparameter tuning. And for each of the layer, how much should we probe? Right? That's also a kind of like a fundamental problem faced in the sparse training community for each of the layer. You may not want to sparsify them in the same ratio. Cardiffish takes everything automatically and we kind of like make the training. During the training, we detect some of the heuristics and we try to make everything automatic. So basically select each of the layers, select only the layer that's tolerable for low rank factorization and automatically detect when should we switch from full rank to low rank. Basically make everything more hyperparameter free. It's not totally hyperparameter free, but something like more automatic. Well, if we're to maybe frame it in terms of trade-offs, if I want to take advantage of these efficiencies, do I have to give something up? And if so, what? Most of the critical part is that we have to give up some of the accuracy if we don't do it quite well. The entire work is basically depend on how much the redundancy is in the model. So if the model parameter does not contain redundancy at all, this method will fail. So I have to be very honest. It's about how to detect the model's parameter's redundancy. If there's no redundancy, don't use this. And if there's enough redundancy, then we should detect what's the level of the redundancy and we should remove the exact amount of the redundancy. So in the PowerFish paper, we did that in a very heuristic way. Sometimes it won't work. You can, like as you mentioned, we will sacrifice some accuracy. In Cuttlefish, we are trying to optimize that really well to automatically detect the redundancy level and then we just remove those appropriate amount of redundancy and turns out it kind of like mitigates the accuracy drop very well. I appreciate your point. If there's no redundancy, there's no advantage to be gained. But when I hear about these models that have increasingly billions of parameters, it's hard to believe there isn't redundancy here. Is that a fair intuition? I think so. So I think the scaling law paper is a very, very good one. I actually encourage people to read. So basically that's basically telling us that the data should scale proportionate to the model size, right? So otherwise, if we just scale the model up, let's say to one training or whatever, it will only increase the redundancy in the model parameters. That will make PowerFish work better, Cuttlefish work better, but that's not what we want to see actually in the application. Like we should really control that very well. I guess that's basically my intuition, but there's no kind of like a qualitative way to do that yet. I really encourage people to look into that research direction. Well, when I think about the people training large language models, in my head, there's two categories. There's people like the ones at OpenAI who are working on the foundational models. Maybe training the next DaVinci or ChatGPT or whatever it is. And then there are those who want to extend it to their own domain. Which or both who should be using Cuttlefish? Well, I really want to say both, but for the first principle, Cuttlefish is designed for pre-training, I would say. For fine tuning, I think Laura works reasonably well, but it doesn't mean like the Cuttlefish cannot support fine tuning. It's mostly like you take the model checkpoint, you just decompose them and you just keep fine-tune on your own data set. Yeah, I would say mostly for pre-training, at least in the current stage. And can you talk a little bit about some empirical results? How do you measure the improvement gains you get with the methods? First of all, it's very, the most straightforward one is the model size, right? So like when we kind of like decompose the model. So currently, let's say, actually right now I'm training a 1 billion llama 2 model, like using Cuttlefish. We can kind of like reach to, we just train something like a 60% smaller, to some model with a 60%, you can roughly understand that's a 600 million, let's say llama 2 model, subset of the pile data set. It's already reached to the same perplexity in the pre-training stage. So, you know, that's the first rationale there. And also, like, since we don't have to load everything in the GPU, right? So there's kind of like a second gain there is basically the GPU memory. And also the most important thing is basically we compute things faster. So currently we can reach to 1.4 times faster end-to-end training speed compared to the full rank or kind of like a dense llama model. So those are the gains. The loose areas, like we kind of like a drop a little bit of the perplexity. So roughly speaking the same, but still a little bit of drop. I'm still working on that, tweaking different, I guess, hyperparameters, and then try to try to make things better. And in a lot of more general cases, I would have to do something with my own hyperparameters, maybe a grid search or I don't know, some clever way of picking them. Can I now step away from that and let Cuttlefish kind of auto optimize those sorts of things for me? So the best we can do is say if we have a concrete set of hyperparameters used for training the dense model, let's say llama, I think that's kind of like there's a standard like a set of hyperparameters, like what's the learning rate you need to use? What's that batch size? I think it's around two million things like that. So you have a kind of like a standard set of hyperparameter to get started with. Cuttlefish is doing something to guarantee you that if you still use the same set of hyperparameters, you will reach to good accuracy. That's what we can guarantee. But there's a more fundamental question that when you train low-rank model, is there, you know, does there exist a better set of hyperparameter that can make the, you know, like the low-rank model to better the for the model? I think there is. But like from the design principle, we don't want to, you know, like dump that effort into users. We just want to make the process as easy as possible. We can just take the original set of hyperparameter, take our technology and just train everything will be handled. There's no extra hyperparameter to tune. Everything will converge almost as same as the original model. Well, I suppose everyone's data set is going to be a little bit different and unique, but at least in the paper you report it's 1.2 times faster to train and 5.6 times smaller than a full-ranked model. Is there any reason with those good stats in mind not to just adopt these processes? So I think that's probably my fault. So we still need a very good software to basically support things like that. So because there's a whole bunch of amazing algorithms, I think one reason people are not using that is basically the software engineering has not been done right. So that's something I'm always trying to do is basically I want to write a model compiler, basically taking whatever kind of model and then, you know, the model compiler will basically just go through all the layers and try to detect redundancy there. And then if the redundancy key to some of the criteria, it will basically just do the transfer that from full-rank to low-rank automatically. I think that support is not there yet. So that's I should work on that. And I'm actually trying to work on that right now. So that's the first thing. The second thing is basically, as you mentioned, for different applications, I think there's still some of the hyperparameter has to be tuned appropriately. Otherwise, I'm not sure if the low-rank method will work as good as, let's say, the vision and the language models. Those are the two mainstream models. But I think a lot of people care about, let's say, graph data and or even like tabular data. So those are things like I haven't explored very well yet. Yeah, but I think the most important part is the software engineering that has been appropriately and very efficiently such that people would like to really adopt. Well, when I think historically, if someone said, I'm interested in machine learning, I would encourage them to get probably an advanced degree, focus in math and stats and algorithms and things along these lines. It seems like maybe there are more lines of opportunity in machine learning now. For example, like people interested in software are not always interested in compilers, but some people are compiler people. Do you have any advice for people who want to take a sort of really engineering MLOps kind of approach? What are good inroads to be learning this area? I would really recommend people to just start to play with, let's say, just download the Lama2, you know, just the checkpoint and just start to, let's say, put that into or just fine tune on their own data. I just use a Hanging Face. I think Hanging Face provides very good software support for everything. And just start to try to play and process the data, the tokenizing, like what does that even mean? And just factorizing, you know, just start to do the fine tuning. And then when they are working on that, they will start to see what actually the task is. Right. So it's basically a next-door prediction. And then they will kind of like try to realize which part doesn't go right and what's the like, you know, the loss function means or what does gradient mean. Yeah, I think everything will naturally come into play. Good advice. Yeah, I learned a lot by getting my hands dirty. I didn't learn everything in a classroom, for sure. Same thing here. Well, yeah, classroom is also good. And yeah, I guess people still, if you can go to a college or a graduate school, I think, you know, the principle learning, machine learning in the principle of the way is still important to understand, you know, what's actually going on, you know, what does generalization mean, things like that. But currently, it seems like the whole field is going into the, I think, toward application engineering, heavy style. So that's why I think practice would really be important. And people, as you mentioned, I really agree with you, Kyle, like people should get their hands dirty and try to really experience things. What's coming next for you in your research? My vision is mostly currently like into kind of a two separated field. As for people in academia, we shouldn't really compete with, try to compete with OpenAI. I think that's a little bit impossible to achieve AGI there. What we should really do in the MLCs community is basically I'm interested in helping, let's say, people do fundamental science. There's a whole area, so-called AI for science. I see big opportunities there that people can leverage the power of foundation model to really help their applications, let's say gene protein structure prediction, like a drug prediction, and also the kind of like a medical diagnosis assistant I just mentioned to you. So that's the one thing I'm trying to do right now. So how can we leverage the powerfulness of the foundation model to build the domain specific applications and really to help like mostly scientists in my case. And another is basically how to democratize, really democratize the foundation model to wider amount of people. So like when people do not have like advanced computing resources, what can we do for them? So the software can be optimized really well to make people also use foundation model really well. It turns out actually we can also like groups the community hardware together to even train the powerful foundation model. So that's what I'm excited about for the future research. And is there anywhere listeners can follow you online if they want to keep up with that as well? Yeah, definitely. So one thing is definitely Twitter. I think I already shared my Twitter account. And another is basically my Google Scholar page and my personal homepage. Yeah, I can probably send it to you after the recording. That'd be good. Yeah. We'll have links to all the above in the show notes for people to follow up with. Well, thank you so much for taking the time to come on and share your work. Oh, yeah. Thanks a lot for having me, Kyle. This is really, really exciting."}}, "transcribed_data": {"podcast_title": "Data Skeptic", "episode_title": "Cuddlefish Model Tuning", "episode_image": "https://ssl-static.libsyn.com/p/assets/0/e/4/b/0e4bd71bb64c6e45/DS_-_New_Logo_assets_-_JL_DS_Logo_Stacked_-_Color_2.jpg", "episode_transcript": " Welcome to data skeptic machine intelligence, our podcast series exploring contemporary topics in artificial general intelligence and large language models. Do all of you know about the grandmother neuron paradox? I'm not sure if it's called exactly that, but the idea that, well, the fallacy that maybe a surgeon could open up your brain, take a real precision tweezers, grab a single neuron and yank it out of your brain. And as long as that surgeon picked exactly the right neuron, you would completely forget your grandmother. Which one? I'm not sure. You ought to have two, right? Maybe more. But this nonsensical idea demonstrates for us that the brain is a little bit more holographic. Information is distributed across it. And I think it's worth anthropomorphizing the grandmother neuron idea to large language models. Take any deep learning model in fact, and remove one single parameter. I mean, heck, this is what dropout does. That neural network should be virtually unchanged by one small perturbation in its design. So surely they can stand to lose a few parameters. How many exactly? I guess that depends on what sort of trade-off you're willing to have. At present, my assessment is that this is a true practitioner's art, at least in the state of the art as it is in 2023. There's a lot of discussion about LoRa, low rank approximation, but even that feels pretty low level requiring a lot of wizardry on a part of the person deploying it. That's why I was totally intrigued when I came across the paper we're going to discuss in today's interview. That paper is Cuttlefish, low rank model training without all the tuning. You know, sometimes when I look at deep learning models and I consider all the tuning, it looks like a Moog synthesizer with dozens of knobs. Maybe Wendy Carlos knows what to do with them. But I have no idea how to fine tune all those oscillators and whatnot. Could I get just one knob? Maybe that's what Cuttlefish can give me. Let's find out. My name is Hongyi Wang. I'm currently with the machine learning department at Carnegie Mellon University. I'm a senior researcher there. I previously got my PhD degree from the computer science department at the University of Wisconsin-Madison. So my research is mainly about the intersection of system and machine learning. Now it's called MLC, fancy name. They're kind of like optimize the system to run your machine learning code or model or algorithm better on your hardware. So basically that's what I'm doing. Can you tell me a little bit about the focus of your research? I mostly study distributed machine learning. So basically parallel computing. Currently as the model becomes bigger and data also becomes bigger, so one GPU seems not enough to train our model or even do the so-called fine tuning these days. So my research is about how you can use more, utilize more CPU cores or more GPUs to accelerate your computing during training or model fine tuning. But there the problem we care about is like when you use 1000 GPUs together, how can you get 1000 times faster? So that's what we call scaling problem. Now we are kind of doing kind of well for the smaller model. Let's say if you want to train a residual network or convolution network, it seems to be fine. We solve the scaling problem okay. But for large language models, sometimes we need to go beyond what we call data parallelism. That makes the problem harder. So we are working hard on that recently. So I have written a fair amount of Java code in my professional career and on one or two occasions had a collaborator who knew about the underlying architecture of the JVM much better than I did and was able to go do some very fine tuned optimizations about the way garbage collection work to make everything just function a lot better. I don't have that skill set and I kind of don't want it. I'd rather just sort of do what I know there and let someone else handle that. But then I try and carry that analogy over to deep learning where I am aware of things like batch normalization and the vanishing gradient and concepts like this or even the transformer architecture and how it works. Do you think it's necessary that people who use deep learning know the fundamentals and the methods in that way or are we maybe entering a world where someone could just engineer on top of that technology? Right. I think that's pretty kind of like a visionary problem and we're actually working very hard on that. So as I just mentioned, if I do not have a system like knowledge, I just work on fundamental science, let's say, and I want to use the foundation model or kind of like deep learning to kind of like power my research. It's basically kind of like impossible in the past. But now we can kind of like have PyTorch, like TensorFlow, it seems to become easier, but still very hard. Let's say if I were even given, let's say, like 1000 GPU from Microsoft, I don't even know how to use the 1000 GPU together. So that's the problem. We are kind of like solving, kind of like come up with another layer in between from application people and engineer and also the hardware. So that's what we are doing. Hopefully we can automatically deploy your code on the hardware in the optimized way or in kind of like an optimal way. Yeah, I know it won't be the focus of our main discussion today for the paper, Cuttlefish, that I invite you to talk about. But before we get into that, could you just reference a little bit about the hardware? Is that like FPGAs or what are you thinking there? Actually, yeah, I know the world is currently GPU dominated or TPU dominated, but we're actually thinking about a wider picture. So there are a whole bunch of like used hardware everywhere, right? Even in the AWS cloud, there are data centers, there are a lot of like used GPUs, also the community hardware. So one of my research goals is to try to combine everything together, kind of like reuse the machine, everything, your CPU cores. Then there, like if you kind of calculate the theoretical flaws, that's actually comparable if we use as much, you know, those used hardware as possible. It's comparable to the A100. But the problem is there is how you can use them efficiently together. So that's what some of my past work and my future work will try to tackle. Well, you know, theoretically you can solve this, right? You can build a very, very fancy cost model and you solve how you can parallelize your computation across all the hardware. It's basically a graph partitioning problem, but theoretically we can solve that. It's kind of like the engineering problem. How can we like scheduling things well? How can we use them cleverly? And how can we even handle the fault tolerance? So like when we have a whole bunch of hardware together, it's very likely like some of them will fail and it seems like that. Yeah, that's definitely possible. And I'm devoting my time to work on that. Well, if we look back in recent history, it's not like NLP has changed overnight. I mean, maybe it did in some ways when some of these amazing models came out. But we can look back to like Word2vec as a pretty impressive technology that's, I guess, a decade old. At what point or was there a point when you first took notice and said, wow, something's happening here? Mostly, when I joined this community, it's mostly there's no NLP scaling problem yet, I would say. So at that time, people are excited about like VGG, how you can scale VGG on ImageNet training. So that already becomes a problem because the ImageNet is huge. And also, there's a whole bunch of images there. Currently, if I remember correctly, it's kind of like one million images inside of the dataset. So even kind of like one epoch, now it's called epoch of training, it takes a whole bunch of time. So at that time, we were trying to optimize things like how can we make the model go pass through the data passes as many as possible and as fast as possible. So I would say, you know, like after ImageNet challenging the benchmark, things start to become different. Like all of the people are starting to care about how to make things faster. And how can we optimize all the software stack to try to keep the top of the benchmark as much as possible. And now we have some other benchmark, you know, kind of like people are hitting the, let's say, VKUNA leaderboard or Puggingface leaderboard. It seems to be the newer version of ImageNet in this current era. So I've heard some reports about the cost of running some of these large language models. Maybe it costs one penny for a thousand requests of chat GPT or something like that. And it sounds so low when you describe it as a commodity. But actually, in some sense, maybe it's expensive because we're doing things inefficiently or something along those lines. Where are the opportunities to make efficiency gains here? Right. So kind of in terms of the data center computing, I think it's if we can, everything comes to the most advanced GPU. I think that has been optimized quite well, actually. So in the sense that people may not even want to use, there's a whole bunch of different parallelism. So we have data parallelism model and also the pipeline parallelism. So different of them. And the people have realized that when you can use them together, you can reach to some sort of optimal performance. But now in the data center setting, I don't think people want to even use that. They just use the 40 shard data parallel released by it's also called Deep Speed Zero Stage 3 or whatever. They're just using that. It's because the hardware is highly optimized in the data center. The kind of cross node bandwidth is high enough. So it's fine. So everything is optimized quite well. But when we consider if we want to scale things out of the data center, we want to combine all the reuse hardware or the community hardware. So that's different. So the communication can be really the bottleneck there. So we consider there are two clusters located on different, one on the East Coast, one on the West Coast. So even one communication can take quite a long time. So how can we do that? How can we reduce the communication among there and try to do less communication as much as possible, focus more on local computing? Yeah, that's I think where the opportunity is. Well, there's a popular technique I've heard called LoRa, the low rank approximation, where I guess my, you can correct me if I'm wrong, but my understanding of it is take the big model that's been trained in some expensive and maybe inefficient way and then find a way to compress it or reduce it or something like that. Yeah. Is that fair? And is that a good strategy? Right. I think it really depends. So my personal experience is that if we are really GPU or computing power constraint, say if we can't even train the 7 billion model, so LoRa is a perfect technology to democratize the model to a whole bunch of people. Right. So currently I think LoRa, the LoRa software stack and also it's Varon, let's say the QLoRa is optimized really well to basically just run our MacBook. So that's perfect. I think a lot of amazing applications have been built on top of that to generate their own images, things like that, StableDiffusion. For some more serious, let's say accuracy driven application, let's say I myself is building a medical chatbot right now and I'm just doing some like a pre-training, plus fine tuning on some medical publications. Let's say there's a kind of like something called PubMed. There, I think if we have enough or kind of like decent amount of computing resources, LoRa seems to lead to some accuracy drops there. So let's say if you tune a 65 billion model using LoRa, it won't perform as good as we find full fine tune with a 30 billion model. So I think it really depends on applications. But I personally think LoRa is a very amazing work for democratizing the powerfulness of the large language model to a wide amount of users. Well, how would you contrast LoRa to Cuttlefish? And maybe also can you at the same time introduce Cuttlefish? Oh, yeah, definitely. So Cuttlefish is actually not even the first attempt for this line of work. Discuss with the LoRa team. We have had a few official meetings, actually. So Cuttlefish is something like...so we just treat the model as a model is just too huge. So at the beginning, we are thinking about how can we make kind of like make things a little bit easier. Turns out we can just treat a low rank model. So low rank is basically it's kind of like basically linear algebra. So our model is basically a whole bunch of...we can understand that the whole bunch of matrix concatenating together and we are kind of computing one matrix multiplication by one matrix multiplication. So low rank approximation is basically we approximate the matrix multiplication using fewer number of flaws. And also we can also make the matrix a little bit smaller, such that the computation can be a bit faster. We also save a little bit of GPU memory and also communication become faster because we are essentially training a smaller model. That's basically the basic technology behind Cuttlefish. And actually, at the beginning, we published a paper in 2021 called Pufferfish. So that's very initial work. It's just doing this approximation and training from scratch. And turns out when you train the low rank model, it doesn't work as well as the full rank model because we kind of like to reduce a lot of model capacity. It doesn't generalize well. Then we kind of like introduce several technologies there. Some of the layers, it's hard to approximate, right? If you kind of like approximate that, it will cause a huge amount of accuracy drop. For some of the layers, it's fine. So that's the first technology. We're just selectively factorizing some of the layers in the neural network. A second technology is that you may not want to go low rank in the very beginning. So it's mostly like the sparse training, right? So in the beginning, you want to do this and you gradually make your model become sparse. In the full rank to low rank training, we're also doing things like that. In the beginning, we are doing full rank model and then we gradually start to convert from full rank to low rank. So that's called PowerFish published in 2021. Cardiffish took that a step further. It's basically, so when we switch from full rank to low rank and which layers we don't want to factorize, so those are a lot of like heavy hyperparameter tuning. And for each of the layer, how much should we probe? Right? That's also a kind of like a fundamental problem faced in the sparse training community for each of the layer. You may not want to sparsify them in the same ratio. Cardiffish takes everything automatically and we kind of like make the training. During the training, we detect some of the heuristics and we try to make everything automatic. So basically select each of the layers, select only the layer that's tolerable for low rank factorization and automatically detect when should we switch from full rank to low rank. Basically make everything more hyperparameter free. It's not totally hyperparameter free, but something like more automatic. Well, if we're to maybe frame it in terms of trade-offs, if I want to take advantage of these efficiencies, do I have to give something up? And if so, what? Most of the critical part is that we have to give up some of the accuracy if we don't do it quite well. The entire work is basically depend on how much the redundancy is in the model. So if the model parameter does not contain redundancy at all, this method will fail. So I have to be very honest. It's about how to detect the model's parameter's redundancy. If there's no redundancy, don't use this. And if there's enough redundancy, then we should detect what's the level of the redundancy and we should remove the exact amount of the redundancy. So in the PowerFish paper, we did that in a very heuristic way. Sometimes it won't work. You can, like as you mentioned, we will sacrifice some accuracy. In Cuttlefish, we are trying to optimize that really well to automatically detect the redundancy level and then we just remove those appropriate amount of redundancy and turns out it kind of like mitigates the accuracy drop very well. I appreciate your point. If there's no redundancy, there's no advantage to be gained. But when I hear about these models that have increasingly billions of parameters, it's hard to believe there isn't redundancy here. Is that a fair intuition? I think so. So I think the scaling law paper is a very, very good one. I actually encourage people to read. So basically that's basically telling us that the data should scale proportionate to the model size, right? So otherwise, if we just scale the model up, let's say to one training or whatever, it will only increase the redundancy in the model parameters. That will make PowerFish work better, Cuttlefish work better, but that's not what we want to see actually in the application. Like we should really control that very well. I guess that's basically my intuition, but there's no kind of like a qualitative way to do that yet. I really encourage people to look into that research direction. Well, when I think about the people training large language models, in my head, there's two categories. There's people like the ones at OpenAI who are working on the foundational models. Maybe training the next DaVinci or ChatGPT or whatever it is. And then there are those who want to extend it to their own domain. Which or both who should be using Cuttlefish? Well, I really want to say both, but for the first principle, Cuttlefish is designed for pre-training, I would say. For fine tuning, I think Laura works reasonably well, but it doesn't mean like the Cuttlefish cannot support fine tuning. It's mostly like you take the model checkpoint, you just decompose them and you just keep fine-tune on your own data set. Yeah, I would say mostly for pre-training, at least in the current stage. And can you talk a little bit about some empirical results? How do you measure the improvement gains you get with the methods? First of all, it's very, the most straightforward one is the model size, right? So like when we kind of like decompose the model. So currently, let's say, actually right now I'm training a 1 billion llama 2 model, like using Cuttlefish. We can kind of like reach to, we just train something like a 60% smaller, to some model with a 60%, you can roughly understand that's a 600 million, let's say llama 2 model, subset of the pile data set. It's already reached to the same perplexity in the pre-training stage. So, you know, that's the first rationale there. And also, like, since we don't have to load everything in the GPU, right? So there's kind of like a second gain there is basically the GPU memory. And also the most important thing is basically we compute things faster. So currently we can reach to 1.4 times faster end-to-end training speed compared to the full rank or kind of like a dense llama model. So those are the gains. The loose areas, like we kind of like a drop a little bit of the perplexity. So roughly speaking the same, but still a little bit of drop. I'm still working on that, tweaking different, I guess, hyperparameters, and then try to try to make things better. And in a lot of more general cases, I would have to do something with my own hyperparameters, maybe a grid search or I don't know, some clever way of picking them. Can I now step away from that and let Cuttlefish kind of auto optimize those sorts of things for me? So the best we can do is say if we have a concrete set of hyperparameters used for training the dense model, let's say llama, I think that's kind of like there's a standard like a set of hyperparameters, like what's the learning rate you need to use? What's that batch size? I think it's around two million things like that. So you have a kind of like a standard set of hyperparameter to get started with. Cuttlefish is doing something to guarantee you that if you still use the same set of hyperparameters, you will reach to good accuracy. That's what we can guarantee. But there's a more fundamental question that when you train low-rank model, is there, you know, does there exist a better set of hyperparameter that can make the, you know, like the low-rank model to better the for the model? I think there is. But like from the design principle, we don't want to, you know, like dump that effort into users. We just want to make the process as easy as possible. We can just take the original set of hyperparameter, take our technology and just train everything will be handled. There's no extra hyperparameter to tune. Everything will converge almost as same as the original model. Well, I suppose everyone's data set is going to be a little bit different and unique, but at least in the paper you report it's 1.2 times faster to train and 5.6 times smaller than a full-ranked model. Is there any reason with those good stats in mind not to just adopt these processes? So I think that's probably my fault. So we still need a very good software to basically support things like that. So because there's a whole bunch of amazing algorithms, I think one reason people are not using that is basically the software engineering has not been done right. So that's something I'm always trying to do is basically I want to write a model compiler, basically taking whatever kind of model and then, you know, the model compiler will basically just go through all the layers and try to detect redundancy there. And then if the redundancy key to some of the criteria, it will basically just do the transfer that from full-rank to low-rank automatically. I think that support is not there yet. So that's I should work on that. And I'm actually trying to work on that right now. So that's the first thing. The second thing is basically, as you mentioned, for different applications, I think there's still some of the hyperparameter has to be tuned appropriately. Otherwise, I'm not sure if the low-rank method will work as good as, let's say, the vision and the language models. Those are the two mainstream models. But I think a lot of people care about, let's say, graph data and or even like tabular data. So those are things like I haven't explored very well yet. Yeah, but I think the most important part is the software engineering that has been appropriately and very efficiently such that people would like to really adopt. Well, when I think historically, if someone said, I'm interested in machine learning, I would encourage them to get probably an advanced degree, focus in math and stats and algorithms and things along these lines. It seems like maybe there are more lines of opportunity in machine learning now. For example, like people interested in software are not always interested in compilers, but some people are compiler people. Do you have any advice for people who want to take a sort of really engineering MLOps kind of approach? What are good inroads to be learning this area? I would really recommend people to just start to play with, let's say, just download the Lama2, you know, just the checkpoint and just start to, let's say, put that into or just fine tune on their own data. I just use a Hanging Face. I think Hanging Face provides very good software support for everything. And just start to try to play and process the data, the tokenizing, like what does that even mean? And just factorizing, you know, just start to do the fine tuning. And then when they are working on that, they will start to see what actually the task is. Right. So it's basically a next-door prediction. And then they will kind of like try to realize which part doesn't go right and what's the like, you know, the loss function means or what does gradient mean. Yeah, I think everything will naturally come into play. Good advice. Yeah, I learned a lot by getting my hands dirty. I didn't learn everything in a classroom, for sure. Same thing here. Well, yeah, classroom is also good. And yeah, I guess people still, if you can go to a college or a graduate school, I think, you know, the principle learning, machine learning in the principle of the way is still important to understand, you know, what's actually going on, you know, what does generalization mean, things like that. But currently, it seems like the whole field is going into the, I think, toward application engineering, heavy style. So that's why I think practice would really be important. And people, as you mentioned, I really agree with you, Kyle, like people should get their hands dirty and try to really experience things. What's coming next for you in your research? My vision is mostly currently like into kind of a two separated field. As for people in academia, we shouldn't really compete with, try to compete with OpenAI. I think that's a little bit impossible to achieve AGI there. What we should really do in the MLCs community is basically I'm interested in helping, let's say, people do fundamental science. There's a whole area, so-called AI for science. I see big opportunities there that people can leverage the power of foundation model to really help their applications, let's say gene protein structure prediction, like a drug prediction, and also the kind of like a medical diagnosis assistant I just mentioned to you. So that's the one thing I'm trying to do right now. So how can we leverage the powerfulness of the foundation model to build the domain specific applications and really to help like mostly scientists in my case. And another is basically how to democratize, really democratize the foundation model to wider amount of people. So like when people do not have like advanced computing resources, what can we do for them? So the software can be optimized really well to make people also use foundation model really well. It turns out actually we can also like groups the community hardware together to even train the powerful foundation model. So that's what I'm excited about for the future research. And is there anywhere listeners can follow you online if they want to keep up with that as well? Yeah, definitely. So one thing is definitely Twitter. I think I already shared my Twitter account. And another is basically my Google Scholar page and my personal homepage. Yeah, I can probably send it to you after the recording. That'd be good. Yeah. We'll have links to all the above in the show notes for people to follow up with. Well, thank you so much for taking the time to come on and share your work. Oh, yeah. Thanks a lot for having me, Kyle. This is really, really exciting."}, "podcast_summary": "- TL;DR: Cuttlefish is a low-rank model training method that improves efficiency and reduces model size.\n- Bullet Point 1: Cuttlefish uses low-rank approximation to reduce the size of large language models.\n- Bullet Point 2: The method accelerates training and reduces GPU memory usage.\n- Bullet Point 3: Cuttlefish achieves comparable accuracy to full-rank models with improved efficiency.\n- Surprising Fact: Cuttlefish automatically detects redundancy in model parameters and optimizes the training process.\n- Inspiring Takeaway: Cuttlefish aims to democratize the use of large language models and make them more accessible to a wider range of users.", "podcast_guest": "Hongyi Wang", "podcast_highlights": "Chapter 1: The Grandmother Neuron Paradox and the Holographic Nature of the Brain\nChapter 2: Introduction to Cuttlefish: Low Rank Model Training without Tuning\nChapter 3: The Intersection of System and Machine Learning in Distributed Machine Learning\nChapter 4: Challenges of Scaling Large Language Models\nChapter 5: The Need for Simplified Deep Learning and Automation in Engineering\nChapter 6: Empirical Results and Trade-offs of Cuttlefish\nChapter 7: The Future of Machine Learning Engineering and AI for Science"}