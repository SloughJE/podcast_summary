{"detailed_guest_info": "Chancelor Johnathan Bennett, professionally known as Chance the Rapper, is an American rapper, singer-songwriter, and record producer. He gained mainstream recognition with his mixtapes \"Acid Rap\" and \"Coloring Book,\" which earned him three Grammy Awards. Additionally, he is a member of the Chicago collective Savemoney and is involved in social activism in his hometown.", "podcast_details": {"podcast_details": {"links": [{"href": "https://feeds.megaphone.fm/VMP9331026707", "rel": "self", "type": "application/rss+xml"}], "title": "Unexplainable", "title_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "Unexplainable"}, "language": "en", "rights": "\u00a9 2021 Vox Media, Inc. All Rights Reserved", "rights_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "\u00a9 2021 Vox Media, Inc. All Rights Reserved"}, "subtitle": "What we don't know is awesome", "subtitle_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "What we don't know is awesome"}, "image": {"href": "https://megaphone.imgix.net/podcasts/bf04a680-6afe-11eb-a86a-3f07259749a3/image/uploads_2F1612893420722-zz4k8c4m7kp-6b9ff956d013a296cbd10bb83d784130_2FUnexplainable_3000x3000.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress"}, "itunes_explicit": null, "itunes_type": "episodic", "authors": [{"name": "Vox", "email": "unexplainable@vox.com"}], "author": "Vox", "author_detail": {"name": "Vox", "email": "unexplainable@vox.com"}, "summary": "Unexplainable takes listeners right up to the edge of what we know ... and then keeps right on going. This Vox podcast explores scientific mysteries, unanswered questions, and all the things we learn by diving into the unknown. New episodes every Wednesday.", "summary_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "Unexplainable takes listeners right up to the edge of what we know ... and then keeps right on going. This Vox podcast explores scientific mysteries, unanswered questions, and all the things we learn by diving into the unknown. New episodes every Wednesday."}, "content": "<p>Unexplainable takes listeners right up to the edge of what we know ... and then keeps right on going. This Vox podcast explores scientific mysteries, unanswered questions, and all the things we learn by diving into the unknown. New episodes every Wednesday.</p>", "content_detail": {"type": "text/html", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "<p>Unexplainable takes listeners right up to the edge of what we know ... and then keeps right on going. This Vox podcast explores scientific mysteries, unanswered questions, and all the things we learn by diving into the unknown. New episodes every Wednesday.</p>"}, "publisher_detail": {"name": "Vox", "email": "unexplainable@vox.com"}, "tags": [{"term": "Science", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Life Sciences", "scheme": "http://www.itunes.com/", "label": null}, {"term": "Natural Sciences", "scheme": "http://www.itunes.com/", "label": null}]}, "first_episode": {"title": "Can we talk to animals?", "title_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "Can we talk to animals?"}, "links": [{"rel": "alternate", "type": "text/html", "href": "https://www.vox.com/unexplainable"}, {"length": "0", "type": "audio/mpeg", "href": "https://www.podtrac.com/pts/redirect.mp3/pdst.fm/e/chtbl.com/track/524GE/traffic.megaphone.fm/VMP1713766522.mp3?updated=1692131938", "rel": "enclosure"}], "link": "https://www.vox.com/unexplainable", "summary": "Two scientists explain how AI might help us translate animal communication, and what we might learn from their squawks, chirps, songs, and chatter. This episode was recorded live at the Aspen Ideas Festival.\nFor more, go to http://vox.com/unexplainable\nIt\u2019s a great place to view show transcripts and read more about the topics on our show.\nAlso, email us! unexplainable@vox.com\nWe read every email.\nSupport Unexplainable by making a financial contribution to Vox! bit.ly/givepodcasts\nLearn more about your ad choices. Visit podcastchoices.com/adchoices", "summary_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "Two scientists explain how AI might help us translate animal communication, and what we might learn from their squawks, chirps, songs, and chatter. This episode was recorded live at the Aspen Ideas Festival.\nFor more, go to http://vox.com/unexplainable\nIt\u2019s a great place to view show transcripts and read more about the topics on our show.\nAlso, email us! unexplainable@vox.com\nWe read every email.\nSupport Unexplainable by making a financial contribution to Vox! bit.ly/givepodcasts\nLearn more about your ad choices. Visit podcastchoices.com/adchoices"}, "published": "Wed, 16 Aug 2023 08:00:00 -0000", "published_parsed": [2023, 8, 16, 8, 0, 0, 2, 228, 0], "itunes_title": "Can we talk to animals?", "itunes_episodetype": "full", "authors": [{"name": "Vox"}], "author": "Vox", "author_detail": {"name": "Vox"}, "subtitle": "Two scientists explain how AI might help us translate animal communication, and what we might learn from their squawks, chirps, songs, and chatter.", "subtitle_detail": {"type": "text/plain", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "Two scientists explain how AI might help us translate animal communication, and what we might learn from their squawks, chirps, songs, and chatter."}, "content": [{"type": "text/html", "language": null, "base": "https://feeds.megaphone.fm/VMP9331026707", "value": "<p>Two scientists explain how AI might help us translate animal communication, and what we might learn from their squawks, chirps, songs, and chatter. This episode was recorded live at the Aspen Ideas Festival.</p><p>For more, go to <a href=\"http://vox.com/unexplainable\">http://vox.com/unexplainable</a></p><p>It\u2019s a great place to view show transcripts and read more about the topics on our show.</p><p>Also, email us! <a href=\"https://www.vox.com/pages/support-now?utm_campaign=contribution&amp;utm_medium=referral&amp;utm_source=podcast\">unexplainable@vox.com</a></p><p>We read every email.</p><p>Support Unexplainable by making a financial contribution to Vox! <a href=\"https://www.vox.com/pages/support-now?utm_campaign=contribution&amp;utm_medium=referral&amp;utm_source=podcast\">bit.ly/givepodcasts</a></p><p> </p><p>Learn more about your ad choices. Visit <a href=\"https://podcastchoices.com/adchoices\">podcastchoices.com/adchoices</a></p>"}], "itunes_duration": "2061", "itunes_explicit": null, "id": "b5009d5c-3504-11ed-ac0e-6393e641e514", "guidislink": false}, "transcribed_data": {"podcast_title": "Unexplainable", "episode_title": "Can we talk to animals?", "episode_image": "https://megaphone.imgix.net/podcasts/bf04a680-6afe-11eb-a86a-3f07259749a3/image/uploads_2F1612893420722-zz4k8c4m7kp-6b9ff956d013a296cbd10bb83d784130_2FUnexplainable_3000x3000.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " Ten years ago, Chance the Rapper released a mixtape called Acid Rap. It was full of a certain joy and exuberance that feels lacking in hip hop today. I asked him why that is. I think it's just worse. Like, I think it's just worse in terms of public safety. You know, even the weather, like the earth is not as lit as it was in 2013. Chance the Rapper on how hip hop has changed. This week on Intuit Vulture's pop culture podcast. It's unexplainable. I'm Noam Hassenfeld. A couple months ago, I went to the Aspen Ideas Festival in Colorado. It's this festival where tons of scientists, journalists, politicians, teachers, all kinds of people get together, hang out, and present their ideas. I was there to tape a live conversation for Unexplainable about animal communication, and I got to talk with two brilliant scientists, Karen Bakker and Azar Raskin. Karen's a professor at the University of British Columbia, and she's written a book all about animal communication called The Sounds of Life. And Azar is a co-founder of the Earth Species Project, which is a nonprofit that's trying to decode animal communication using AI. We had a wide-ranging conversation that started with the basic discovery that our world is filled with way more animal sounds than a lot of people initially thought. And from there, we talked about what those sounds might mean, whether there's some form of language, and whether scientists might someday be able to use AI to actually decipher what animals are saying. We've edited this a bit for clarity and length. Here's our conversation. So just to start, before we get into how researchers are trying to decode animal sounds today, I think it's worth talking about how they realized they were worth decoding in the first place. So Karen, you have a story in your book about how European and American scientists really started to discover that these sounds were even worth exploring. And you talk about whale song. So I wonder if you could just set the scene for us a little. What was the state of research on ocean sounds maybe 100 years or so ago? Yeah, so cast your mind back to about 100 years ago. There's a widespread assumption that only humans possess language, and moreover, that other species do not possess complex communication capacity. This is, of course, a blind spot in Western science that we're going to unpack throughout the course of the conversation today. But at the time, one of the most remarkable aspects of Western science was really a kind of a CINABO mission. There wasn't a lot of work done recording animal sounds. There wasn't a lot of work done trying to decode them. And that all changed when a small number of researchers, some of them associated with then classified Navy efforts to listen to underwater ocean sounds, began attempting to decode and categorize the really complex underwater sounds they were hearing in the ocean. So these scientists made early efforts mostly to record the pretty profound and amazing sounds that whales make, which are now well known to us. But at the time, we're pretty astounding to Western scientists. So you know, the kind of operatic uulations of humpback whales, the very staccato, powerful eardrum-blasting sounds of sperm whales. There's a whole symphony under the ocean of which Western science was largely unaware. Their work came to public attention when a renegade scientist named Roger Payne and his brilliant wife, Katie Payne, a classically trained musician, took some classified recordings that Navy scientists had given them and published them as an album, which remains the best-selling nature album of all time, went platinum, and was one of the major things that changed the dynamic around the campaign to end industrial whaling. What was arguably one of the things that saved many whale species from extinction. So whales were our re-entry as Western scientists into something indigenous cultures had long known, that many, many species are capable of complex communication. And from there, many other species have begun essentially to reveal themselves to Western science. But I want to emphasize right at the start, this is long-held human knowledge that somehow we have forgotten that we had forgotten and we have just begun remembering really how to remember. So you know, talking about things that we've forgotten or maybe things that we were not aware of until very recently, you know, it's not just these sounds under the water, right? It's not just deep sounds that we haven't been able to hear because we didn't have submarines. I wonder if you can tell us a bit about some of these sounds that we can't hear without technology that are around us all the time. The vast majority of these sounds are inaudible to the naked human ear. They are either above our hearing range in the high ultrasound or below our human hearing range in the deep infrasound. There is an evolutionary reason for this called the acoustic niche hypothesis. So in most ecosystems, what you get is much like radio stations on the radio dial. You'll have different species essentially broadcasting acoustic communication at a specific set of frequencies, a band, and also being able to hear in that same frequency range. We hear pretty much at the frequencies that we are able to vocalize at. So the ability to record beyond human hearing range is only about 100 years old for ultrasound and infrasound. But what AI does is it allows the parsing, the categorizing of that data at scale. And so we reversed a very fundamental constraint of 20th century biology. We used to have basically a scarcity of data. Now we have a hyperabundance of data because of cheap digital recording devices, many, many such recording devices from the Arctic to the Amazon. And now we have AI that can do some, not all, automated tracking, categorizing, parsing. That doesn't mean we can actually then make the next step to translating, but it gets us a lot further. So you're talking about now using AI, using this more data we've had, using AI to analyze it. Az, I wonder if you can talk a little bit about the research you've done with using AI to map things like shapes of languages. How does AI help us translate between languages when we might not be able to understand them? Yeah. So I'm going to tell this story in two parts. And the reason why we started Earth Species in 2017 was that something fundamental changed. Because if you're going to try to translate a language without a Rosetta Stone, that didn't exist in human history up until 2017. Then that changed. So what changed? AI gained the ability to build shapes that represent languages. For those of you that are AI people in the room, these are called latent spaces or embedding spaces. But these shapes are really interesting because they turn semantic relationships into geometric relationships. What does that mean? Think about a language like English. Now imagine a galaxy where every star is a word. And words that mean similar things are near each other and then words that share a semantic relationship share a geometric relationship. So king is to man as woman is to queen. So in this shape, king is the same distance direction to man as woman is to queen. So you just do king minus man. That gives you a distance and direction. You add that to boy and it'll equal prince. You add that to girl, it'll equal princess. All the internal relationships of a language are encoded in this shape. And if you think about it, dog has relationship to man and to wolf and to fur and to yelp and to howl. It fixes it in a point in space in this shape. And then if you solve the massive multi-dimensional Sudoku puzzle of every concept to every other concept, out pops this rigid structure representing all the internal relationships of a language. The computer doesn't know what anything means. It just knows how they relate. And here was the deep insight from 2017. Could the shape of two languages possibly be the same? So I'm holding a Portuguese word cloud here. You're holding an English one. And the mathematical relationship between the stars in my galaxy that represent woman, queen, king, man is more or less the same mathematical, that is spatial relationship in your word cloud. If I were to throw up Cree or Inuktitut, a little bit less overlap, but more or less a lot of these concepts are invariant across human languages. And that is why we are able to really effectively translate now using AI between our different word clouds. That's exactly right. You literally just rotate one shape on top of the other. And even though there are words in one language that don't exist in the other, the point which is dog ends up in the same spot in both. And you sort of blur your eyes in the same shape. And that works for English and Spanish. And you're like, cool, those are related languages, obviously, but also works for Finnish, which is a weird language, and Aramaic, and Urdu, Esperanto. Every human language sort of roughly fits in this universal human meaning shape. And that was the moment that we're like, there's a path through. Do you think if we build this shape for animal communication, it fits somewhere in the universal human meaning shape and the parts that overlap, we should be able to directly translate into the human experience and the parts that don't overlap, we should be able to see complexity. And so this gives us the ability to start getting like blurry Polaroid images of things that are beyond the human imagination. Let's go back one step to this notion that we can translate between our different word clouds, English and Inuktitut, and then extrapolate that to non-human communication system. So first of all, you have to imagine that scientists now have the ability to create these kind of latent spaces or word clouds with non-human communication regimes. For example, there is now an elephant dictionary with thousands of sounds and field biologists have painstakingly documented what each of those signals mean. African elephants, for example, have a specific signal for honeybee. They're terrified of honeybees. They can get into their trunks and their ears and sting them. And so there's a very specific behavior elephants display when they hear the sound. So tested through playback experiments, we have this elephant dictionary, but there are many ifs here because the assumption that the underlying worldview, the felt, lived, embodied experience of an elephant, the umwelt, as researchers call it, is anything like a human is one we haven't yet proven. And so my word cloud may not contain any concepts that actually overlap with Asa's, with the exception of a very few. So it may be that AI translation is either a dead end or it would only allow us to develop a small subset of translatable concepts and that there are other rules governing these non-human communication systems that we've yet to figure out. I'll just give a couple examples. It may be that animal species have different languages for different times of year. A little bit like, you know, if you're familiar with Indian classical music, ragas, you know, there's the morning raga, the evening raga. So you have to throw out all of your assumptions about language. Maybe they have different languages for different parts of the world if they're migratory. The Bering Strait may have a different language than the warm waters of Hawaii where you give birth. So we are just really at the beginning of trying to figure out whether what Asa's trying to do is achievable. My personal bet, and I'd love to hear your view, is that there will be an incomplete translation. We will be able to detect names. We'll be able to detect alarm calls and we'll be able to detect the labels that are given to sort of features of the environment that are linguistically invariant. But there are many, many more complex concepts that we're going to have to invent entirely new types of science to begin understanding. And those are going to combine field observations with AI. Absolutely. I just want to say that like all of our work is built off of decades of painstaking research done by biologists out in the field. And everything we do is hence in collaboration. But then just to add a couple thoughts to what you're saying is one, the way we're describing doing this kind of translation with rotation, that's 2017 AI tech. It's now sort of stone age. There are like many other techniques which I can talk about that like that becomes just one tool among many. But why should we expect, like I agree, like the umwelt of a sperm whale might be so completely different. It spends 80 percent of its life in complete darkness a kilometer deep in the ocean. Why should we expect there to be any overlap whatsoever? And I'll give two examples for why there might be overlap. And then I'll sort of talk about the parts that like why I think this goes even beyond language. The first example is the mirror test. It's like how do you know whether another being has self-awareness? One way you might discover that you would paint a dot on them where they are unaware of that dot. Then you put a mirror in front of them. And if they see the dot and they start to like try to get it off, that shows that they're connecting the image in the mirror with themselves, that they have a self-image. Now if they don't respond, that doesn't actually tell you anything. Researchers thought for the longest time that elephants couldn't pass the mirror test, but it turned out they're just using small mirrors. So, but a number of species do pass this kind of mirror test. And that means if they're communicating, they may well be communicating about a rich interiority, like a self-awareness, one of the most profound things that we have. Another example, as I have this in my presentations, an incredible video of lemurs biting centipedes to get high. So they're like taking hits off of centipedes. They get super cuddly. They enter these translational spaces. They're like, they're super cuddly. They enter these trans-like states. It's sort of like a proto-burning man. And like dolphins do the same thing. They will intentionally inflate puffer fish also to get high off of their venom and pass them around in the original puff-puff pass. Right? So gorillas and chimpanzees will spin. They'll hang on a vine and spin to get really dizzy. Transcendent states of consciousness appears to be a thing that we share and desire across many species. So that too is a very profound thing that if we communicate or if they communicate, they may well communicate about. So there are some like, I think, really interesting areas of overlap. This conversation we're having, you know, on one sense, it can feel like we're just, we're like speeding ahead and there's this one other hurdle we have to pass, which is like figuring out this translation. The AI is almost there. We're also using a lot of anthropomorphic language. And I feel like that is definitely, I understand why we would do that. It's not, we don't have better words. I mean, I want to clarify that scientists do not use that language. They use very technical terms. So for example, scientists would not use the term name. They would say individual vocal label or vocal signature. Equally, most of the scientists studying the communicative regimes of non-human species would use the term would use the term communication, not language, because language is sufficiently anthropocentrically defined in terms of, you know, complex combinatorial capacity, symbolic content, syntax, so on and so forth, that it is as yet to be proven that other species have quote unquote language. So I just want to clarify that although Aza and I, in a sort of a public communication of science way, are using these terms, the scientific community is pretty rigorous, perhaps incorrectly so, but nonetheless pretty rigorous about setting a boundary between humans and non-human species. But one of the things that this research may eventually do is create a sufficient weight of evidence that we do indeed say, ah yes, other species have language. We may need to change our definition of language in order to do so. Or ah yes, other species do convey symbolic meaning through language. And here's how. We're not there yet. So progressively, I think this science is going to lead us somewhere very, very interesting in terms of asking these fundamental questions on the basis of a huge amount of empirical evidence. I wonder, you know, we briefly mentioned the Umfeldt question about what does it mean to be a bat or a whale or any of these things that perceive the world completely differently. And I guess I just wonder if we could be communicating something to the animal that the animal would be potentially understanding in a different way and acting in a way that looks like what we expect the animal to act because that's how we are understanding things. But will we ever, is it even possible to imagine that we could actually communicate where we both know that we are understanding each other? I mean the same problem exists between any two humans. Yeah, the myth of communication is that it ever happened in the first place. Yeah, so I think the practical pragmatic scientific responses, playback experiments. And that is how these are tested. We assume this acoustic signal means this. We can play it back in the field or in lab controlled conditions. We see the response as what we predict. Elephant honeybee alarm call leads to very specific physical behaviors. The group of elephants coming together, dusting themselves. Now beyond that, I mean the act of communication is a profound mystery. The ability of any two beings on Earth to believe they can actually understand one another, it is actually quite magical. So science is one of the ways of approaching some of the great mysteries and communication is one of them. But the reason I think this captures so much public imagination, Aysen and I have talked about this in the past, is because this is also a great mystery which has been the subject of much reflection in various mythological and spiritual traditions. And so that is some of the richness of this work. It's also some of the controversy that it inspires in the scientific community. We're going to take a quick break from the conversation here. When we come back, we're going to ask whether we should be trying to translate animal communication at all. If you're thinking of Lays, you can thank your farmers for making your stay golden moments possible. Lays, stay golden. To learn more, head to goldengrowshere.com. This episode is brought to you by Nerds Candy. Calling all the gaming tech savvy nerds out there. The team carriers and keyboard warriors. The achievement hunters and part-time LARPers. The tech wizards and boss destroyers. When nerds come together, we live louder. Leveling up in our own way. So let's raise our nerds in unison. The sweet, tangy, crunchy candy that's perfect for sharing. Nerds, shake things up. Shop now at nerdscandy.com. The hamster sense is required. One issue we haven't touched on is sort of a should issue. And assuming we can use AI to analyze these language shapes, to figure out in some basic sense what animals may be saying to each other. Maybe we can communicate with them. Maybe we can listen to them. Knowing humans, I can imagine there would be some maybe not friendly ways to approach that situation. And I'm wondering what you think about the danger of being able to understand animals and being able to communicate with them. So I want to give, excuse me, like a really specific example of the new responsibility that we as a species are going to have to show up to in the very, very near future. So you guys I'm sure have encountered chat GPT. You can build chat bots like chat GPT in Chinese, even if you do not speak Chinese. And you've probably also seen like all of the deep fake stuff. Now it is possible with just three seconds of anyone's voice to continue to speak in their voice and say what they were saying. And what this means is that in, you know, within months to short number of years, we will be able to build essentially synthetic chat bots, synthetic whales, synthetic belugas, synthetic tool using crows that can speak in a way that, you know, they don't understand they're not speaking to one of their own. Sort of imagine you had the superpower and your superpower was like being able to walk up to somebody whose language you don't understand. You sort of cock your ear and you listen, you're like, okay, I see this pattern after this pattern, you start to babble with those patterns. And you don't know what you're saying. The other person's like, yeah, wow. It's Douglas Adams, Babel fish. It is except here's the plot twist, you'll be able to communicate before you understand. And so this is this is actually the case, we're actually starting our first experiments with zebra finch likely later this year, we're doing real time two way communication with captive population to see, can we do that start to cross this communications barrier by being able to speak before we understand and this is fascinating. It obviously lets us start to get to decode much faster. But humpback whale song goes viral, right? Like song sung off the coast of Australia can like go, you know, 1000 kilometers, they the the humpback whale song will be picked up by the world population within a season or two sometimes. And so if we're not careful, right, if we just create a whale that starts to sing, especially before we understand what it's what it means, we could be messing up with wisdom tradition, right, creating a kind of whale QAnon, we don't know. And that means before that happens, because that means like, it's a very crazy thing to think about. I didn't think we were gonna get here this quickly. The next, you know, 12 months, five years, certainly before 2030, we will have the capacity to real time two way communication, animal to AI, not necessarily animal to human. And we need to have a kind of Geneva Convention for cross species communication, a prime directive, sets of norms, ways that IRBs review, there are a whole bunch of things we need to set up. And I think you can talk about. Yeah, and I think there are even more nefarious uses precision hunting, precision fishing, of course, coaching, yeah, poaching, this will enable the acceleration of the kind of cat and mouse game between poachers and game keepers, no doubt. There also is the specter of being able to domesticate species that were formerly not domesticatable by humans. So we may be able to use this in certain contexts. And this is what my next book is about for biodiversity conservation goals. At the same time, it could allow bad actors, and keep in mind how big the multi billion dollar global illegal wildlife trade is, right, to further capitalize on their ability to ensnare animals that have so far been, you know, sort of out of reach. So the Geneva Convention long term for multi species dialogue, great. Prior to that, I think we've got a more immediate problem on our hands, given the biodiversity crisis, with respect to nefarious uses of these technologies. The only saving grace is that the AI may not be really as good as we think. So, you know, first of all, we're being very, very self centered here, as usual, we're humans, we're assuming other species actually want to talk to us. They may be like, boring, you know, or they may just assume that that these sounds, which are gibberish, you know, are to be avoided, rightfully so, or they may simply be able to detect it's not being made by another living member of their species and avoid. So my hope is that they're going to reveal, we're going to reveal ourselves to be slightly stupider than we think, they're going to reveal themselves to be smarter than we believe. And maybe that'll create a bit more breathing room. But no doubt, longer term, deep fake AI technology creates a whole bunch of risks. And do you think we should, given these risks, you think this is something that scientists should push forward? Yeah, I mean, I believe we should have a moratorium. A's and I don't agree on this point. I think there are certainly thresholds that if we cross, we should have a moratorium, we should we should stop. Absolutely. Here's the thing. We are mutilating the tree of life. And at some point, we are going to cut the branch upon which humanity depends. Right. So we are in the land of Hail Mary passes. The hope for, I think, working on showing and really, the point is not really to talk to animals, the point is really to understand and listen. And along the path to that, we are creating the technology that solves the fundamental problems we see across all of conservation, biology and anthology research, every biologist we talk to needs to do classification, denoising, detection of signals to understand biodiversity, to understand their behaviors. And so the tools we build as we head towards decoding are the fundamental tools that are accelerating conservation biology, which to the extent that conservation science accelerates conservation, like we're trying to broad scale do that. But then there are these moments when we get shifts in perspective, and that changes everything we talked about songs of the humpback whale, but also when human beings went to the moon, and when human beings were dosed with that overview effect and seeing us as a pale blue dot suspended in space, planet, spaceship Earth. Right. That's when the EPA came into existence. Noah was born, modern environmental movement started, Clean Air Axe was passed, and that was in the Nixon era. Right. And so the goal here is like there are moments in history, which superpower movements, there are no silver bullets, but maybe they're silver buckshot. And maybe if we know this is coming, we can arm every other conservation org out there, rights for nature, personhood for non-humans, Eowar's Half Earth, much bigger marine protected zones. When this becomes the thing that the entire world sees and becomes the top of politicians' priority list, suddenly I think we can accelerate and be a force multiplier for every other conservation and climate action out there. And that I think is the reason why it's worth pursuing. I wonder if, yeah, I mean, just before we finish, I wonder, could you just say a bit about why you think we should have a moratorium? I will, but I do also want to build on Asa's point. So the climate change and biodiversity crisis are intimately interrelated, and the fundamental challenge of the next 20 or 30 years, as we add a couple billion more humans to the planet, is a sort of Noah's Ark-like challenge. How many species will be around at the end of our lifetimes? And acoustics, regardless of whether we actually achieve interspecies communication, is a powerful tool in the conservationists' view of the world. In the conservationists' toolkit, because simply through the use of digital bioacoustics, you have a very low cost, very effective monitoring regime that is much less invasive than human monitoring. And this is now something that is being set up around the world. I can't go into the technical details for lack of time, but very simply, bio and ecoacoustic indices allow us to tell simply by listening the extent to which climate change is disrupting species, species migrations and movements, and species abundance or disappearance, etc. So we may never achieve interspecies communication, but what we can do, and we all should be doing if we're interested in environmental work, is supporting the inclusion of digital acoustics, bio and ecoacoustics, into conservation work as a low cost, minimally invasive, very powerful tool. So I hope that's a take-home message for all of you. The question of the moratorium, I think, is really one about something humans are not very good at, that is having the ability to create a space in between what we think we're capable of doing and reaching for that thing. So there is a, in the tech community, in the scientific community, there is a can-do ethos. I can do that, and so I want to do it. But there is a should-do question here that I think requires very, very careful consideration, and I see no, and I hear I disagree with you, compelling reason right now to continue the work that could be so damaging to other species, that could lead to precision hunting and poaching. Without getting a lot of the ethical frameworks in place, it would mean updating the Convention on International Trade in Endangered Species, updating a lot of international environmental regulatory frameworks, AI governance poses a more general problem, right? So my view is we need to get our house in order on that, and just like from time to time human genomics research has hit pause, and there are certain no-go areas, like cloning humans, I think we can come up with a set of no-go areas for AI science in this regard that would allow technical progress to still be made, but not be invoking the kind of risks that we barely have even begun to understand. And on that we actually, I think, agree, which is that as we show up to the new responsibility of the power, you have to ask how is that power bound to wisdom, and how knowing that this technology is coming, because regardless the ability to emulate any signal, that's being pushed by the market forces on the human domain, so we need to accelerate as fast as possible all of the ethics and legal updates. It's Pandora's box, right? Every new technology creates a new responsibility. I think there's a small enough set of researchers doing this that we could actually do a better job this time at sorting out the responsibility before we unleash the technology. I completely agree. Thanks so much to Aza Raskin and Karen Bacher and to the Aspen Institute. If you want to read more about the history here, I recommend Karen's book, The Sounds of Life. It's got the whale story we mentioned, but also some fascinating stories about everything from chatty turtles to bee communication. You can also watch a video of the full discussion we had at Aspenideas.org. This episode was produced by Bird Pinkerton. It was edited by Brian Resnick and Meredith Hodnut, who also manages our team. We had mixing from Christian Ayala, music from me, Serena Solon checked the facts and we're so happy to have her on the team, and Manding Nguyen is not afraid of spiders. If you enjoyed the show, it would bring us a lot of joy if you leave a review or just send us thoughts directly. You can email us at unexplainable at vox.com. We read every email. Unexplainable is part of the Vox Media Podcast Network and we'll be back next week."}}, "transcribed_data": {"podcast_title": "Unexplainable", "episode_title": "Can we talk to animals?", "episode_image": "https://megaphone.imgix.net/podcasts/bf04a680-6afe-11eb-a86a-3f07259749a3/image/uploads_2F1612893420722-zz4k8c4m7kp-6b9ff956d013a296cbd10bb83d784130_2FUnexplainable_3000x3000.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " Ten years ago, Chance the Rapper released a mixtape called Acid Rap. It was full of a certain joy and exuberance that feels lacking in hip hop today. I asked him why that is. I think it's just worse. Like, I think it's just worse in terms of public safety. You know, even the weather, like the earth is not as lit as it was in 2013. Chance the Rapper on how hip hop has changed. This week on Intuit Vulture's pop culture podcast. It's unexplainable. I'm Noam Hassenfeld. A couple months ago, I went to the Aspen Ideas Festival in Colorado. It's this festival where tons of scientists, journalists, politicians, teachers, all kinds of people get together, hang out, and present their ideas. I was there to tape a live conversation for Unexplainable about animal communication, and I got to talk with two brilliant scientists, Karen Bakker and Azar Raskin. Karen's a professor at the University of British Columbia, and she's written a book all about animal communication called The Sounds of Life. And Azar is a co-founder of the Earth Species Project, which is a nonprofit that's trying to decode animal communication using AI. We had a wide-ranging conversation that started with the basic discovery that our world is filled with way more animal sounds than a lot of people initially thought. And from there, we talked about what those sounds might mean, whether there's some form of language, and whether scientists might someday be able to use AI to actually decipher what animals are saying. We've edited this a bit for clarity and length. Here's our conversation. So just to start, before we get into how researchers are trying to decode animal sounds today, I think it's worth talking about how they realized they were worth decoding in the first place. So Karen, you have a story in your book about how European and American scientists really started to discover that these sounds were even worth exploring. And you talk about whale song. So I wonder if you could just set the scene for us a little. What was the state of research on ocean sounds maybe 100 years or so ago? Yeah, so cast your mind back to about 100 years ago. There's a widespread assumption that only humans possess language, and moreover, that other species do not possess complex communication capacity. This is, of course, a blind spot in Western science that we're going to unpack throughout the course of the conversation today. But at the time, one of the most remarkable aspects of Western science was really a kind of a CINABO mission. There wasn't a lot of work done recording animal sounds. There wasn't a lot of work done trying to decode them. And that all changed when a small number of researchers, some of them associated with then classified Navy efforts to listen to underwater ocean sounds, began attempting to decode and categorize the really complex underwater sounds they were hearing in the ocean. So these scientists made early efforts mostly to record the pretty profound and amazing sounds that whales make, which are now well known to us. But at the time, we're pretty astounding to Western scientists. So you know, the kind of operatic uulations of humpback whales, the very staccato, powerful eardrum-blasting sounds of sperm whales. There's a whole symphony under the ocean of which Western science was largely unaware. Their work came to public attention when a renegade scientist named Roger Payne and his brilliant wife, Katie Payne, a classically trained musician, took some classified recordings that Navy scientists had given them and published them as an album, which remains the best-selling nature album of all time, went platinum, and was one of the major things that changed the dynamic around the campaign to end industrial whaling. What was arguably one of the things that saved many whale species from extinction. So whales were our re-entry as Western scientists into something indigenous cultures had long known, that many, many species are capable of complex communication. And from there, many other species have begun essentially to reveal themselves to Western science. But I want to emphasize right at the start, this is long-held human knowledge that somehow we have forgotten that we had forgotten and we have just begun remembering really how to remember. So you know, talking about things that we've forgotten or maybe things that we were not aware of until very recently, you know, it's not just these sounds under the water, right? It's not just deep sounds that we haven't been able to hear because we didn't have submarines. I wonder if you can tell us a bit about some of these sounds that we can't hear without technology that are around us all the time. The vast majority of these sounds are inaudible to the naked human ear. They are either above our hearing range in the high ultrasound or below our human hearing range in the deep infrasound. There is an evolutionary reason for this called the acoustic niche hypothesis. So in most ecosystems, what you get is much like radio stations on the radio dial. You'll have different species essentially broadcasting acoustic communication at a specific set of frequencies, a band, and also being able to hear in that same frequency range. We hear pretty much at the frequencies that we are able to vocalize at. So the ability to record beyond human hearing range is only about 100 years old for ultrasound and infrasound. But what AI does is it allows the parsing, the categorizing of that data at scale. And so we reversed a very fundamental constraint of 20th century biology. We used to have basically a scarcity of data. Now we have a hyperabundance of data because of cheap digital recording devices, many, many such recording devices from the Arctic to the Amazon. And now we have AI that can do some, not all, automated tracking, categorizing, parsing. That doesn't mean we can actually then make the next step to translating, but it gets us a lot further. So you're talking about now using AI, using this more data we've had, using AI to analyze it. Az, I wonder if you can talk a little bit about the research you've done with using AI to map things like shapes of languages. How does AI help us translate between languages when we might not be able to understand them? Yeah. So I'm going to tell this story in two parts. And the reason why we started Earth Species in 2017 was that something fundamental changed. Because if you're going to try to translate a language without a Rosetta Stone, that didn't exist in human history up until 2017. Then that changed. So what changed? AI gained the ability to build shapes that represent languages. For those of you that are AI people in the room, these are called latent spaces or embedding spaces. But these shapes are really interesting because they turn semantic relationships into geometric relationships. What does that mean? Think about a language like English. Now imagine a galaxy where every star is a word. And words that mean similar things are near each other and then words that share a semantic relationship share a geometric relationship. So king is to man as woman is to queen. So in this shape, king is the same distance direction to man as woman is to queen. So you just do king minus man. That gives you a distance and direction. You add that to boy and it'll equal prince. You add that to girl, it'll equal princess. All the internal relationships of a language are encoded in this shape. And if you think about it, dog has relationship to man and to wolf and to fur and to yelp and to howl. It fixes it in a point in space in this shape. And then if you solve the massive multi-dimensional Sudoku puzzle of every concept to every other concept, out pops this rigid structure representing all the internal relationships of a language. The computer doesn't know what anything means. It just knows how they relate. And here was the deep insight from 2017. Could the shape of two languages possibly be the same? So I'm holding a Portuguese word cloud here. You're holding an English one. And the mathematical relationship between the stars in my galaxy that represent woman, queen, king, man is more or less the same mathematical, that is spatial relationship in your word cloud. If I were to throw up Cree or Inuktitut, a little bit less overlap, but more or less a lot of these concepts are invariant across human languages. And that is why we are able to really effectively translate now using AI between our different word clouds. That's exactly right. You literally just rotate one shape on top of the other. And even though there are words in one language that don't exist in the other, the point which is dog ends up in the same spot in both. And you sort of blur your eyes in the same shape. And that works for English and Spanish. And you're like, cool, those are related languages, obviously, but also works for Finnish, which is a weird language, and Aramaic, and Urdu, Esperanto. Every human language sort of roughly fits in this universal human meaning shape. And that was the moment that we're like, there's a path through. Do you think if we build this shape for animal communication, it fits somewhere in the universal human meaning shape and the parts that overlap, we should be able to directly translate into the human experience and the parts that don't overlap, we should be able to see complexity. And so this gives us the ability to start getting like blurry Polaroid images of things that are beyond the human imagination. Let's go back one step to this notion that we can translate between our different word clouds, English and Inuktitut, and then extrapolate that to non-human communication system. So first of all, you have to imagine that scientists now have the ability to create these kind of latent spaces or word clouds with non-human communication regimes. For example, there is now an elephant dictionary with thousands of sounds and field biologists have painstakingly documented what each of those signals mean. African elephants, for example, have a specific signal for honeybee. They're terrified of honeybees. They can get into their trunks and their ears and sting them. And so there's a very specific behavior elephants display when they hear the sound. So tested through playback experiments, we have this elephant dictionary, but there are many ifs here because the assumption that the underlying worldview, the felt, lived, embodied experience of an elephant, the umwelt, as researchers call it, is anything like a human is one we haven't yet proven. And so my word cloud may not contain any concepts that actually overlap with Asa's, with the exception of a very few. So it may be that AI translation is either a dead end or it would only allow us to develop a small subset of translatable concepts and that there are other rules governing these non-human communication systems that we've yet to figure out. I'll just give a couple examples. It may be that animal species have different languages for different times of year. A little bit like, you know, if you're familiar with Indian classical music, ragas, you know, there's the morning raga, the evening raga. So you have to throw out all of your assumptions about language. Maybe they have different languages for different parts of the world if they're migratory. The Bering Strait may have a different language than the warm waters of Hawaii where you give birth. So we are just really at the beginning of trying to figure out whether what Asa's trying to do is achievable. My personal bet, and I'd love to hear your view, is that there will be an incomplete translation. We will be able to detect names. We'll be able to detect alarm calls and we'll be able to detect the labels that are given to sort of features of the environment that are linguistically invariant. But there are many, many more complex concepts that we're going to have to invent entirely new types of science to begin understanding. And those are going to combine field observations with AI. Absolutely. I just want to say that like all of our work is built off of decades of painstaking research done by biologists out in the field. And everything we do is hence in collaboration. But then just to add a couple thoughts to what you're saying is one, the way we're describing doing this kind of translation with rotation, that's 2017 AI tech. It's now sort of stone age. There are like many other techniques which I can talk about that like that becomes just one tool among many. But why should we expect, like I agree, like the umwelt of a sperm whale might be so completely different. It spends 80 percent of its life in complete darkness a kilometer deep in the ocean. Why should we expect there to be any overlap whatsoever? And I'll give two examples for why there might be overlap. And then I'll sort of talk about the parts that like why I think this goes even beyond language. The first example is the mirror test. It's like how do you know whether another being has self-awareness? One way you might discover that you would paint a dot on them where they are unaware of that dot. Then you put a mirror in front of them. And if they see the dot and they start to like try to get it off, that shows that they're connecting the image in the mirror with themselves, that they have a self-image. Now if they don't respond, that doesn't actually tell you anything. Researchers thought for the longest time that elephants couldn't pass the mirror test, but it turned out they're just using small mirrors. So, but a number of species do pass this kind of mirror test. And that means if they're communicating, they may well be communicating about a rich interiority, like a self-awareness, one of the most profound things that we have. Another example, as I have this in my presentations, an incredible video of lemurs biting centipedes to get high. So they're like taking hits off of centipedes. They get super cuddly. They enter these translational spaces. They're like, they're super cuddly. They enter these trans-like states. It's sort of like a proto-burning man. And like dolphins do the same thing. They will intentionally inflate puffer fish also to get high off of their venom and pass them around in the original puff-puff pass. Right? So gorillas and chimpanzees will spin. They'll hang on a vine and spin to get really dizzy. Transcendent states of consciousness appears to be a thing that we share and desire across many species. So that too is a very profound thing that if we communicate or if they communicate, they may well communicate about. So there are some like, I think, really interesting areas of overlap. This conversation we're having, you know, on one sense, it can feel like we're just, we're like speeding ahead and there's this one other hurdle we have to pass, which is like figuring out this translation. The AI is almost there. We're also using a lot of anthropomorphic language. And I feel like that is definitely, I understand why we would do that. It's not, we don't have better words. I mean, I want to clarify that scientists do not use that language. They use very technical terms. So for example, scientists would not use the term name. They would say individual vocal label or vocal signature. Equally, most of the scientists studying the communicative regimes of non-human species would use the term would use the term communication, not language, because language is sufficiently anthropocentrically defined in terms of, you know, complex combinatorial capacity, symbolic content, syntax, so on and so forth, that it is as yet to be proven that other species have quote unquote language. So I just want to clarify that although Aza and I, in a sort of a public communication of science way, are using these terms, the scientific community is pretty rigorous, perhaps incorrectly so, but nonetheless pretty rigorous about setting a boundary between humans and non-human species. But one of the things that this research may eventually do is create a sufficient weight of evidence that we do indeed say, ah yes, other species have language. We may need to change our definition of language in order to do so. Or ah yes, other species do convey symbolic meaning through language. And here's how. We're not there yet. So progressively, I think this science is going to lead us somewhere very, very interesting in terms of asking these fundamental questions on the basis of a huge amount of empirical evidence. I wonder, you know, we briefly mentioned the Umfeldt question about what does it mean to be a bat or a whale or any of these things that perceive the world completely differently. And I guess I just wonder if we could be communicating something to the animal that the animal would be potentially understanding in a different way and acting in a way that looks like what we expect the animal to act because that's how we are understanding things. But will we ever, is it even possible to imagine that we could actually communicate where we both know that we are understanding each other? I mean the same problem exists between any two humans. Yeah, the myth of communication is that it ever happened in the first place. Yeah, so I think the practical pragmatic scientific responses, playback experiments. And that is how these are tested. We assume this acoustic signal means this. We can play it back in the field or in lab controlled conditions. We see the response as what we predict. Elephant honeybee alarm call leads to very specific physical behaviors. The group of elephants coming together, dusting themselves. Now beyond that, I mean the act of communication is a profound mystery. The ability of any two beings on Earth to believe they can actually understand one another, it is actually quite magical. So science is one of the ways of approaching some of the great mysteries and communication is one of them. But the reason I think this captures so much public imagination, Aysen and I have talked about this in the past, is because this is also a great mystery which has been the subject of much reflection in various mythological and spiritual traditions. And so that is some of the richness of this work. It's also some of the controversy that it inspires in the scientific community. We're going to take a quick break from the conversation here. When we come back, we're going to ask whether we should be trying to translate animal communication at all. If you're thinking of Lays, you can thank your farmers for making your stay golden moments possible. Lays, stay golden. To learn more, head to goldengrowshere.com. This episode is brought to you by Nerds Candy. Calling all the gaming tech savvy nerds out there. The team carriers and keyboard warriors. The achievement hunters and part-time LARPers. The tech wizards and boss destroyers. When nerds come together, we live louder. Leveling up in our own way. So let's raise our nerds in unison. The sweet, tangy, crunchy candy that's perfect for sharing. Nerds, shake things up. Shop now at nerdscandy.com. The hamster sense is required. One issue we haven't touched on is sort of a should issue. And assuming we can use AI to analyze these language shapes, to figure out in some basic sense what animals may be saying to each other. Maybe we can communicate with them. Maybe we can listen to them. Knowing humans, I can imagine there would be some maybe not friendly ways to approach that situation. And I'm wondering what you think about the danger of being able to understand animals and being able to communicate with them. So I want to give, excuse me, like a really specific example of the new responsibility that we as a species are going to have to show up to in the very, very near future. So you guys I'm sure have encountered chat GPT. You can build chat bots like chat GPT in Chinese, even if you do not speak Chinese. And you've probably also seen like all of the deep fake stuff. Now it is possible with just three seconds of anyone's voice to continue to speak in their voice and say what they were saying. And what this means is that in, you know, within months to short number of years, we will be able to build essentially synthetic chat bots, synthetic whales, synthetic belugas, synthetic tool using crows that can speak in a way that, you know, they don't understand they're not speaking to one of their own. Sort of imagine you had the superpower and your superpower was like being able to walk up to somebody whose language you don't understand. You sort of cock your ear and you listen, you're like, okay, I see this pattern after this pattern, you start to babble with those patterns. And you don't know what you're saying. The other person's like, yeah, wow. It's Douglas Adams, Babel fish. It is except here's the plot twist, you'll be able to communicate before you understand. And so this is this is actually the case, we're actually starting our first experiments with zebra finch likely later this year, we're doing real time two way communication with captive population to see, can we do that start to cross this communications barrier by being able to speak before we understand and this is fascinating. It obviously lets us start to get to decode much faster. But humpback whale song goes viral, right? Like song sung off the coast of Australia can like go, you know, 1000 kilometers, they the the humpback whale song will be picked up by the world population within a season or two sometimes. And so if we're not careful, right, if we just create a whale that starts to sing, especially before we understand what it's what it means, we could be messing up with wisdom tradition, right, creating a kind of whale QAnon, we don't know. And that means before that happens, because that means like, it's a very crazy thing to think about. I didn't think we were gonna get here this quickly. The next, you know, 12 months, five years, certainly before 2030, we will have the capacity to real time two way communication, animal to AI, not necessarily animal to human. And we need to have a kind of Geneva Convention for cross species communication, a prime directive, sets of norms, ways that IRBs review, there are a whole bunch of things we need to set up. And I think you can talk about. Yeah, and I think there are even more nefarious uses precision hunting, precision fishing, of course, coaching, yeah, poaching, this will enable the acceleration of the kind of cat and mouse game between poachers and game keepers, no doubt. There also is the specter of being able to domesticate species that were formerly not domesticatable by humans. So we may be able to use this in certain contexts. And this is what my next book is about for biodiversity conservation goals. At the same time, it could allow bad actors, and keep in mind how big the multi billion dollar global illegal wildlife trade is, right, to further capitalize on their ability to ensnare animals that have so far been, you know, sort of out of reach. So the Geneva Convention long term for multi species dialogue, great. Prior to that, I think we've got a more immediate problem on our hands, given the biodiversity crisis, with respect to nefarious uses of these technologies. The only saving grace is that the AI may not be really as good as we think. So, you know, first of all, we're being very, very self centered here, as usual, we're humans, we're assuming other species actually want to talk to us. They may be like, boring, you know, or they may just assume that that these sounds, which are gibberish, you know, are to be avoided, rightfully so, or they may simply be able to detect it's not being made by another living member of their species and avoid. So my hope is that they're going to reveal, we're going to reveal ourselves to be slightly stupider than we think, they're going to reveal themselves to be smarter than we believe. And maybe that'll create a bit more breathing room. But no doubt, longer term, deep fake AI technology creates a whole bunch of risks. And do you think we should, given these risks, you think this is something that scientists should push forward? Yeah, I mean, I believe we should have a moratorium. A's and I don't agree on this point. I think there are certainly thresholds that if we cross, we should have a moratorium, we should we should stop. Absolutely. Here's the thing. We are mutilating the tree of life. And at some point, we are going to cut the branch upon which humanity depends. Right. So we are in the land of Hail Mary passes. The hope for, I think, working on showing and really, the point is not really to talk to animals, the point is really to understand and listen. And along the path to that, we are creating the technology that solves the fundamental problems we see across all of conservation, biology and anthology research, every biologist we talk to needs to do classification, denoising, detection of signals to understand biodiversity, to understand their behaviors. And so the tools we build as we head towards decoding are the fundamental tools that are accelerating conservation biology, which to the extent that conservation science accelerates conservation, like we're trying to broad scale do that. But then there are these moments when we get shifts in perspective, and that changes everything we talked about songs of the humpback whale, but also when human beings went to the moon, and when human beings were dosed with that overview effect and seeing us as a pale blue dot suspended in space, planet, spaceship Earth. Right. That's when the EPA came into existence. Noah was born, modern environmental movement started, Clean Air Axe was passed, and that was in the Nixon era. Right. And so the goal here is like there are moments in history, which superpower movements, there are no silver bullets, but maybe they're silver buckshot. And maybe if we know this is coming, we can arm every other conservation org out there, rights for nature, personhood for non-humans, Eowar's Half Earth, much bigger marine protected zones. When this becomes the thing that the entire world sees and becomes the top of politicians' priority list, suddenly I think we can accelerate and be a force multiplier for every other conservation and climate action out there. And that I think is the reason why it's worth pursuing. I wonder if, yeah, I mean, just before we finish, I wonder, could you just say a bit about why you think we should have a moratorium? I will, but I do also want to build on Asa's point. So the climate change and biodiversity crisis are intimately interrelated, and the fundamental challenge of the next 20 or 30 years, as we add a couple billion more humans to the planet, is a sort of Noah's Ark-like challenge. How many species will be around at the end of our lifetimes? And acoustics, regardless of whether we actually achieve interspecies communication, is a powerful tool in the conservationists' view of the world. In the conservationists' toolkit, because simply through the use of digital bioacoustics, you have a very low cost, very effective monitoring regime that is much less invasive than human monitoring. And this is now something that is being set up around the world. I can't go into the technical details for lack of time, but very simply, bio and ecoacoustic indices allow us to tell simply by listening the extent to which climate change is disrupting species, species migrations and movements, and species abundance or disappearance, etc. So we may never achieve interspecies communication, but what we can do, and we all should be doing if we're interested in environmental work, is supporting the inclusion of digital acoustics, bio and ecoacoustics, into conservation work as a low cost, minimally invasive, very powerful tool. So I hope that's a take-home message for all of you. The question of the moratorium, I think, is really one about something humans are not very good at, that is having the ability to create a space in between what we think we're capable of doing and reaching for that thing. So there is a, in the tech community, in the scientific community, there is a can-do ethos. I can do that, and so I want to do it. But there is a should-do question here that I think requires very, very careful consideration, and I see no, and I hear I disagree with you, compelling reason right now to continue the work that could be so damaging to other species, that could lead to precision hunting and poaching. Without getting a lot of the ethical frameworks in place, it would mean updating the Convention on International Trade in Endangered Species, updating a lot of international environmental regulatory frameworks, AI governance poses a more general problem, right? So my view is we need to get our house in order on that, and just like from time to time human genomics research has hit pause, and there are certain no-go areas, like cloning humans, I think we can come up with a set of no-go areas for AI science in this regard that would allow technical progress to still be made, but not be invoking the kind of risks that we barely have even begun to understand. And on that we actually, I think, agree, which is that as we show up to the new responsibility of the power, you have to ask how is that power bound to wisdom, and how knowing that this technology is coming, because regardless the ability to emulate any signal, that's being pushed by the market forces on the human domain, so we need to accelerate as fast as possible all of the ethics and legal updates. It's Pandora's box, right? Every new technology creates a new responsibility. I think there's a small enough set of researchers doing this that we could actually do a better job this time at sorting out the responsibility before we unleash the technology. I completely agree. Thanks so much to Aza Raskin and Karen Bacher and to the Aspen Institute. If you want to read more about the history here, I recommend Karen's book, The Sounds of Life. It's got the whale story we mentioned, but also some fascinating stories about everything from chatty turtles to bee communication. You can also watch a video of the full discussion we had at Aspenideas.org. This episode was produced by Bird Pinkerton. It was edited by Brian Resnick and Meredith Hodnut, who also manages our team. We had mixing from Christian Ayala, music from me, Serena Solon checked the facts and we're so happy to have her on the team, and Manding Nguyen is not afraid of spiders. If you enjoyed the show, it would bring us a lot of joy if you leave a review or just send us thoughts directly. You can email us at unexplainable at vox.com. We read every email. Unexplainable is part of the Vox Media Podcast Network and we'll be back next week."}, "podcast_summary": "- TL;DR: Scientists are exploring the possibility of decoding and understanding animal communication using AI.\n- Researchers have discovered that there are more animal sounds than previously thought, and they are using AI to analyze and categorize these sounds.\n- AI has the potential to create shapes that represent languages, allowing for translation between different languages, including non-human communication systems.\n- There are risks involved in understanding and communicating with animals, such as precision hunting and poaching, which need to be addressed through ethical frameworks and regulations.\n- The study of animal communication has implications for conservation biology and can be used as a tool to monitor and understand biodiversity.\n- The conversation around animal communication raises philosophical and ethical questions about the nature of communication itself and our understanding of other species.", "podcast_guest": "Chance the Rapper", "podcast_highlights": "Chapter 1: The Discovery of Animal Sounds\nChapter 2: Decoding Animal Communication\nChapter 3: The Role of AI in Analyzing Animal Sounds\nChapter 4: Translating Between Languages with AI\nChapter 5: The Possibility of Communicating with Animals\nChapter 6: Risks and Responsibilities in Animal Communication"}